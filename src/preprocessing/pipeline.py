# src/preprocessing/pipeline.py

import os
import pandas as pd
import numpy as np
from tqdm import tqdm
from src.utils.config import GLOBAL_CONFIG
from src.utils.logger import get_logger
# [ä¿®æ”¹ç‚¹ 1] å¼•å…¥ save_csv
from src.utils.io import save_parquet, ensure_dir, read_parquet, save_csv
from src.data_source.datahub import DataHub
from src.preprocessing.features import FeatureGenerator
from src.preprocessing.labels import LabelGenerator
from src.preprocessing.neutralization import FeatureNeutralizer  # [æ–°å¢å¼•ç”¨]

logger = get_logger()

class PreprocessPipeline:
    def __init__(self):
        self.config = GLOBAL_CONFIG
        self.datahub = DataHub()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.feature_eng = FeatureGenerator(self.config)
        self.label_gen = LabelGenerator(self.config)
        self.neutralizer = FeatureNeutralizer(self.config)  # [æ–°å¢åˆå§‹åŒ–]
        
        # è·¯å¾„
        self.output_dir = self.config["paths"]["data_processed"]
        ensure_dir(self.output_dir)
        
        # [ä¿®æ”¹ç‚¹ 2] å®šä¹‰ç»Ÿè®¡æŠ¥å‘Šä¿å­˜è·¯å¾„
        self.report_path = os.path.join(self.output_dir, "data_filter_summary.csv")
        
        # æ‰¹å¤„ç†é…ç½®
        self.batch_cfg = self.config.get("preprocessing", {}).get("batch", {})
        
        # è¯»å–è¿‡æ»¤é…ç½®
        self.filter_cfg = self.config.get("preprocessing", {}).get("filter", {})

        # === è¿‡æ»¤ç»Ÿè®¡è®¡æ•°å™¨ ===
        self.filter_stats = {
            "total_rows_input": 0,    # åˆå§‹æ€»è¡Œæ•°
            "total_rows_output": 0,   # æœ€ç»ˆä¿ç•™è¡Œæ•°
            # å„ç¯èŠ‚ä¸¢å¼ƒè®¡æ•°
            "dropped_by_price": 0,          # ä»·æ ¼é™åˆ¶
            "dropped_by_turnover_rate": 0,  # æ¢æ‰‹ç‡
            "dropped_by_amount": 0,         # æˆäº¤é¢
            "dropped_by_mcap": 0,           # å¸‚å€¼
            "dropped_by_sector": 0,         # æ¿å— (ç§‘åˆ›/åˆ›ä¸š/åŒ—äº¤)
            "dropped_by_st": 0,             # ST è‚¡
            "dropped_by_nan": 0             # æœ€ç»ˆè®¡ç®—ç‰¹å¾åçš„ NaN
        }

    def _load_meta_data(self):
        """åŠ è½½å…ƒæ•°æ®ç”¨äº ST è¿‡æ»¤"""
        meta_path = os.path.join(self.config["paths"]["data_meta"], "all_stocks_meta.parquet")
        if os.path.exists(meta_path):
            return read_parquet(meta_path)[["symbol", "name"]]
        return pd.DataFrame(columns=["symbol", "name"])

    def _apply_strict_filter(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ä¸¥æ ¼è¿‡æ»¤é€»è¾‘ (Row-level Filtering) + [ç»Ÿè®¡åŠŸèƒ½]
        """
        if df.empty: return df
        
        # è®°å½•è¯¥è‚¡ç¥¨åˆå§‹è¡Œæ•°
        initial_rows = len(df)
        self.filter_stats["total_rows_input"] += initial_rows
        
        current_df = df
        
        # --- 1. ä»·æ ¼è¿‡æ»¤ (min_price, max_price) ---
        min_price = self.filter_cfg.get("min_price", 0.0)
        max_price = self.filter_cfg.get("max_price", 99999.0)
        if "close" in current_df.columns:
            prev_len = len(current_df)
            current_df = current_df[(current_df["close"] >= min_price) & (current_df["close"] <= max_price)].copy()
            self.filter_stats["dropped_by_price"] += (prev_len - len(current_df))

        if current_df.empty: return current_df

        # --- 2. æ¢æ‰‹ç‡è¿‡æ»¤ (Turnover Rate %) ---
        min_turnover_rate = self.filter_cfg.get("min_turnover_rate", 0.0)
        if "turnover" in current_df.columns and min_turnover_rate > 0:
            current_df["turnover"] = current_df["turnover"].fillna(0)
            prev_len = len(current_df)
            current_df = current_df[current_df["turnover"] >= min_turnover_rate].copy()
            self.filter_stats["dropped_by_turnover_rate"] += (prev_len - len(current_df))

        if current_df.empty: return current_df

        # --- 3. æˆäº¤é¢è¿‡æ»¤ (Amount) ---
        min_amount = self.filter_cfg.get("min_turnover", 0) 
        if "amount" in current_df.columns and min_amount > 0:
            current_df["amount"] = current_df["amount"].fillna(0)
            prev_len = len(current_df)
            current_df = current_df[current_df["amount"] >= min_amount].copy()
            self.filter_stats["dropped_by_amount"] += (prev_len - len(current_df))

        if current_df.empty: return current_df

        # --- 4. å¸‚å€¼è¿‡æ»¤ (åŠ¨æ€è®¡ç®—) ---
        min_mcap = self.filter_cfg.get("min_mcap", 0)
        max_mcap = self.filter_cfg.get("max_mcap", float("inf"))
        
        if (min_mcap > 0 or max_mcap < float("inf")) and "amount" in current_df.columns and "turnover" in current_df.columns:
            prev_len = len(current_df)
            valid_mask = current_df["turnover"] > 0.001
            
            est_mcap = pd.Series(np.nan, index=current_df.index)
            est_mcap.loc[valid_mask] = current_df.loc[valid_mask, "amount"] / (current_df.loc[valid_mask, "turnover"] * 0.01)
            
            keep_mask = pd.Series(True, index=current_df.index)
            if min_mcap > 0:
                has_mcap_but_small = (est_mcap < min_mcap)
                keep_mask = keep_mask & (~has_mcap_but_small)
            if max_mcap < float("inf"):
                has_mcap_but_large = (est_mcap > max_mcap)
                keep_mask = keep_mask & (~has_mcap_but_large)

            current_df = current_df[keep_mask].copy()
            self.filter_stats["dropped_by_mcap"] += (prev_len - len(current_df))

        if current_df.empty: return current_df

        # --- 5. æ¿å—è¿‡æ»¤ ---
        pool_cfg = self.config["data"]["stock_pool"]
        prev_len = len(current_df)
        if not pool_cfg.get("include_kcb", False):
            current_df = current_df[~current_df["symbol"].str.startswith("688")].copy()
        if not pool_cfg.get("include_cyb", False):
            current_df = current_df[~current_df["symbol"].str.startswith("300")].copy()
        if not pool_cfg.get("include_bj", False):
            current_df = current_df[~current_df["symbol"].str.match(r"^(8|4|92)")].copy() 
        self.filter_stats["dropped_by_sector"] += (prev_len - len(current_df))

        if current_df.empty: return current_df

        # --- 6. ST è¿‡æ»¤ ---
        if self.filter_cfg.get("exclude_st", True):
            prev_len = len(current_df)
            df_meta = self._load_meta_data()
            if not df_meta.empty:
                st_symbols = set(df_meta[df_meta["name"].str.contains("ST|é€€", na=False)]["symbol"])
                if st_symbols:
                    current_df = current_df[~current_df["symbol"].isin(st_symbols)].copy()
            self.filter_stats["dropped_by_st"] += (prev_len - len(current_df))

        return current_df
    
    def run(self):
        logger.info("=== å¼€å§‹æ‰§è¡Œç‰¹å¾å·¥ç¨‹æµæ°´çº¿ (å«ä¸¥æ ¼å‰ç½®è¿‡æ»¤) ===")
        
        stock_list = self.datahub.get_cleaned_stock_list()
        if not stock_list:
            logger.error("æœªæ‰¾åˆ°æ¸…æ´—åçš„è‚¡ç¥¨æ•°æ®")
            return
            
        logger.info(f"æ‰«æåˆ°æ¸…æ´—åè‚¡ç¥¨: {len(stock_list)} åª")
        processed_list = []
        
        for symbol in tqdm(stock_list, desc="Feature Engineering"):
            try:
                # A. è¯»å–
                df = self.datahub.load_cleaned_price(symbol)
                if df is None or df.empty:
                    continue
                df["symbol"] = symbol
                
                # B. ä¸¥æ ¼è¿‡æ»¤
                df = self._apply_strict_filter(df)
                if df.empty: continue

                # C. è®¡ç®—ç‰¹å¾
                df = self.feature_eng.run(df)
                
                # D. ç”Ÿæˆæ ‡ç­¾
                df = self.label_gen.run(df)
                
                # E. æ¸…æ´— NaN
                check_cols = [c for c in df.columns if c.startswith("feat_") or c in ["close", "volume"]]
                prev_len = len(df)
                df_clean = df.dropna(subset=check_cols).reset_index(drop=True)
                
                self.filter_stats["dropped_by_nan"] += (prev_len - len(df_clean))
                
                if df_clean.empty: continue
                
                # F. ä¿å­˜å•æ–‡ä»¶
                if self.batch_cfg.get("save_each", True):
                    save_path = os.path.join(self.output_dir, f"{symbol}.parquet")
                    save_parquet(df_clean, save_path)
                
                if self.batch_cfg.get("concat_all", True):
                    processed_list.append(df_clean)
                    self.filter_stats["total_rows_output"] += len(df_clean)
                    
            except Exception as e:
                logger.error(f"å¤„ç† {symbol} å¤±è´¥: {e}")
                
        # åˆå¹¶ä¿å­˜å¤§æ–‡ä»¶
        if self.batch_cfg.get("concat_all", True) and processed_list:
            logger.info("æ­£åœ¨åˆå¹¶å…¨é‡ç‰¹å¾çŸ©é˜µ...")
            full_df = pd.concat(processed_list, ignore_index=True)
            if "date" in full_df.columns:
                full_df = full_df.sort_values(by=["date", "symbol"])
            
            # åœ¨ä¿å­˜ all_stocks.parquet ä¹‹å‰æ‰§è¡Œä¸­æ€§åŒ–
            full_df = self.neutralizer.run(full_df)
            concat_file = self.batch_cfg.get("concat_file", "all_stocks.parquet")
            out_path = os.path.join(self.output_dir, concat_file)
            save_parquet(full_df, out_path)
            logger.info(f"å…¨é‡ç‰¹å¾æ–‡ä»¶å·²ä¿å­˜: {out_path}")
            
        # [ä¿®æ”¹ç‚¹ 3] ç”Ÿæˆå¹¶ä¿å­˜æŠ¥å‘Š
        self._save_filter_report()
        logger.info("ç‰¹å¾å·¥ç¨‹æµæ°´çº¿æ‰§è¡Œå®Œæ¯•ã€‚")

    def _save_filter_report(self):
        """ä¿å­˜å¹¶æ‰“å°æ•°æ®è¿‡æ»¤ç»Ÿè®¡æŠ¥å‘Š"""
        total_in = self.filter_stats["total_rows_input"]
        total_out = self.filter_stats["total_rows_output"]
        
        if total_in == 0:
            logger.warning("è¾“å…¥æ•°æ®é‡ä¸º 0ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Šã€‚")
            return

        dropped_total = total_in - total_out
        
        # 1. æ„é€ æŠ¥å‘Šæ•°æ® List
        report_data = []
        
        # (1) æ€»ä½“ç»Ÿè®¡
        report_data.append({
            "Category": "SUMMARY", 
            "Item": "Total Input Rows", 
            "Count": total_in, 
            "Ratio (%)": 100.0
        })
        report_data.append({
            "Category": "SUMMARY", 
            "Item": "Final Output Rows", 
            "Count": total_out, 
            "Ratio (%)": round((total_out / total_in) * 100, 2)
        })
        report_data.append({
            "Category": "SUMMARY", 
            "Item": "Total Dropped", 
            "Count": dropped_total, 
            "Ratio (%)": round((dropped_total / total_in) * 100, 2)
        })

        # (2) ç»†åˆ†åŸå› ç»Ÿè®¡
        drop_reasons = {k: v for k, v in self.filter_stats.items() if k.startswith("dropped_")}
        sorted_reasons = sorted(drop_reasons.items(), key=lambda item: item[1], reverse=True)
        
        for reason, count in sorted_reasons:
            if count > 0:
                report_data.append({
                    "Category": "FILTER_DETAIL",
                    "Item": reason.replace("dropped_by_", "").upper(),
                    "Count": count,
                    "Ratio (%)": round((count / total_in) * 100, 2)
                })

        # 2. è½¬æ¢ä¸º DataFrame å¹¶ä¿å­˜ CSV
        df_report = pd.DataFrame(report_data)
        save_csv(df_report, self.report_path)
        
        # 3. åŒæ—¶ä¹Ÿæ‰“å°åˆ°æ§åˆ¶å°ï¼Œæ–¹ä¾¿æŸ¥çœ‹
        logger.info("=" * 50)
        logger.info(f"ğŸ“Š è¿‡æ»¤ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜: {self.report_path}")
        logger.info("-" * 50)
        # ç®€å•æ‰“å°æœ€é‡è¦çš„å‡ è¡Œ
        print(df_report[["Item", "Count", "Ratio (%)"]].to_string(index=False))
        
        if sorted_reasons:
            max_reason = sorted_reasons[0][0].replace("dropped_by_", "").upper()
            logger.info("-" * 50)
            logger.info(f"ğŸš« æ ·æœ¬å‰Šå‡æœ€å¤§å› ç´ : ã€{max_reason}ã€‘")
        logger.info("=" * 50)