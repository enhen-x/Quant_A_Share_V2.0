# scripts/run_recommendation.py

import os
import sys
import pandas as pd
import datetime
import glob
import re

# è·¯å¾„é€‚é…
current_dir = os.path.dirname(os.path.abspath(__file__))
# ä»å½“å‰æ–‡ä»¶ä½ç½® (scripts/back_test) è¿”å›ä¸¤çº§åˆ°é¡¹ç›®æ ¹ç›®å½•
project_root = os.path.dirname(os.path.dirname(current_dir))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from src.utils.config import GLOBAL_CONFIG
from src.utils.logger import get_logger
from src.utils.io import read_parquet
from src.model.xgb_model import XGBModelWrapper
from src.strategy.signal import TopKSignalStrategy

logger = get_logger()

def get_latest_model_path():
    """
    æ™ºèƒ½å¯»æ‰¾ data/models ä¸‹æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶
    æ”¯æŒè¯†åˆ«æ™®é€šè®­ç»ƒç›®å½•å’Œ WF (æ»šåŠ¨è®­ç»ƒ) ç›®å½•
    """
    models_dir = GLOBAL_CONFIG["paths"]["models"]
    if not os.path.exists(models_dir):
        return None, None
    
    # 1. è·å–æ‰€æœ‰å­ç›®å½•
    subdirs = [d for d in os.listdir(models_dir) if os.path.isdir(os.path.join(models_dir, d))]
    if not subdirs:
        return None, None

    # 2. è¾…åŠ©å‡½æ•°ï¼šè§£æç›®å½•åä¸­çš„æ—¶é—´æˆ³
    def parse_timestamp(dir_name):
        # ç§»é™¤å‰ç¼€ (å¦‚ "WF_")
        clean_name = dir_name.replace("WF_", "")
        # å°è¯•åŒ¹é… YYYYMMDD_HHMMSS æ ¼å¼
        try:
            return datetime.datetime.strptime(clean_name, "%Y%m%d_%H%M%S")
        except ValueError:
            # å¦‚æœæ ¼å¼ä¸å¯¹ï¼Œè¿”å›ä¸€ä¸ªæå°æ—¶é—´ï¼Œæ’åœ¨æœ€å
            return datetime.datetime.min

    # 3. æŒ‰æ—¶é—´å€’åºæ’åˆ— (æœ€æ–°çš„åœ¨å‰)
    subdirs.sort(key=parse_timestamp, reverse=True)
    latest_version = subdirs[0]
    version_dir = os.path.join(models_dir, latest_version)
    
    logger.info(f"é”å®šæœ€æ–°æ¨¡å‹ç‰ˆæœ¬ç›®å½•: {latest_version}")

    # 4. åœ¨ç›®å½•ä¸­å¯»æ‰¾æœ€ä½³æ¨¡å‹æ–‡ä»¶
    # æƒ…å†µ A: å•æ¬¡è®­ç»ƒçš„æ ‡å‡†æ¨¡å‹
    if os.path.exists(os.path.join(version_dir, "model.json")):
        return latest_version, os.path.join(version_dir, "model.json")
    
    # æƒ…å†µ B: æ»šåŠ¨è®­ç»ƒçš„å¹´åº¦æ¨¡å‹ (model_2024.json, model_2025.json ...)
    # æˆ‘ä»¬éœ€è¦æ‰¾åˆ°å¹´ä»½æœ€å¤§çš„é‚£ä¸ªï¼Œå› ä¸ºå®ƒåŒ…å«äº†æœ€æ–°çš„å¸‚åœºè§„å¾‹
    wf_models = glob.glob(os.path.join(version_dir, "model_*.json"))
    if wf_models:
        def extract_year(path):
            fname = os.path.basename(path)
            # æå–æ•°å­—éƒ¨åˆ†
            match = re.search(r"model_(\d+)\.json", fname)
            return int(match.group(1)) if match else 0
        
        # æ‰¾å¹´ä»½æœ€å¤§çš„
        best_model_path = max(wf_models, key=extract_year)
        best_year = extract_year(best_model_path)
        logger.info(f"æ£€æµ‹åˆ°æ»šåŠ¨è®­ç»ƒæ¨¡å‹é›†ï¼Œå·²è‡ªåŠ¨é€‰æ‹©æœ€æ–°å¹´ä»½: model_{best_year}.json")
        return latest_version, best_model_path

    return None, None

def load_latest_data():
    """åŠ è½½ç‰¹å¾æ•°æ®ï¼Œå¹¶æå–å‡ºã€æœ€è¿‘ N ä¸ªäº¤æ˜“æ—¥ã€‘çš„æ•°æ®ï¼Œç”¨äºé¢„æµ‹å’Œå¹³æ»‘ã€‚"""
    data_path = os.path.join(GLOBAL_CONFIG["paths"]["data_processed"], "all_stocks.parquet")
    if not os.path.exists(data_path):
        logger.error(f"ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨: {data_path}ï¼Œè¯·å…ˆè¿è¡Œ rebuild_features.py")
        return None, None, None

    # è¯»å–æ•°æ® (å®ç›˜å¯ä¼˜åŒ–ä¸ºåªè¯»æœ€åçš„åˆ†åŒº)
    df = read_parquet(data_path)
    df["date"] = pd.to_datetime(df["date"])
    
    # æå–ç‰¹å¾åˆ— (feat_ å¼€å¤´)
    feat_cols = [c for c in df.columns if c.startswith("feat_")]
    
    # è·å–æœ€æ–°çš„ N ä¸ªäº¤æ˜“æ—¥çš„æ•°æ® (N=3, ä¸ signal.py ä¸­çš„ SMOOTH_WINDOW åŒ¹é…)
    N_DAYS = 3 
    
    # 1. è·å–å”¯ä¸€çš„æ—¥æœŸå¹¶æ’åº
    unique_dates = sorted(df["date"].unique(), reverse=True)
    
    if len(unique_dates) < N_DAYS:
        logger.warning(f"æ€»äº¤æ˜“æ—¥ ({len(unique_dates)}) å°‘äºå¹³æ»‘çª—å£ ({N_DAYS}å¤©)ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®ã€‚")
        target_dates = unique_dates
    else:
        # å–æœ€è¿‘çš„ N ä¸ªäº¤æ˜“æ—¥
        target_dates = unique_dates[:N_DAYS]
    
    df_slice = df[df["date"].isin(target_dates)].copy()
    
    if df_slice.empty:
        logger.error("æ•°æ®åˆ‡ç‰‡ä¸ºç©ºï¼Œæ— æ³•æ¨èã€‚")
        return None, None, None
    
    latest_date = unique_dates[0]
    logger.info(f"æ•°æ®é›†ä¸­æœ€æ–°æ—¥æœŸä¸º: {latest_date.strftime('%Y-%m-%d')}ï¼Œå°†åŠ è½½å‰ {len(target_dates)} ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®ã€‚")

    # è¿”å›åˆ‡ç‰‡æ•°æ®ã€ç‰¹å¾åˆ—è¡¨ã€æœ€æ–°æ—¥æœŸ
    return df_slice, feat_cols, latest_date

def main():
    logger.info("=== å¯åŠ¨æ¯æ—¥æ¨èç³»ç»Ÿ (Daily Recommendation) ===")

    # 1. æ™ºèƒ½åŠ è½½æ¨¡å‹
    version, model_path = get_latest_model_path()
    if not model_path:
        logger.error("æœªæ‰¾åˆ°å¯ç”¨æ¨¡å‹æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ run_walkforward.py æˆ– train_model.py")
        return
    
    logger.info(f"åŠ è½½æ¨¡å‹æ–‡ä»¶: {model_path}")
    model = XGBModelWrapper()
    model.load(model_path)
    
    # 2. åŠ è½½æœ€æ–°è¡Œæƒ…æ•°æ®ï¼ˆæœ€è¿‘ N å¤©ï¼‰
    df_slice, feat_cols, latest_date = load_latest_data()
    if df_slice is None or df_slice.empty:
        logger.error("æ— æ•°æ®åˆ‡ç‰‡ï¼Œæ— æ³•æ¨èã€‚")
        return

    # 3. æ‰§è¡Œé¢„æµ‹
    logger.info(f"æ­£åœ¨å¯¹ {len(df_slice)} è¡Œæ•°æ® ({df_slice['symbol'].nunique()} åªè‚¡ç¥¨) è¿›è¡Œæ‰“åˆ†...")
    
    # 3.1 ç‰¹å¾å¯¹é½
    # ä¼˜å…ˆä½¿ç”¨æ¨¡å‹è®°å½•çš„ç‰¹å¾å (å¦‚æœæœ‰)
    final_features = feat_cols
    if hasattr(model.model, "feature_names") and model.model.feature_names:
        model_features = model.model.feature_names
        logger.info(f"ä½¿ç”¨æ¨¡å‹å†…ç½®ç‰¹å¾åˆ—è¡¨: {len(model_features)} ä¸ª")
        
        # æ£€æŸ¥ç¼ºå¤±ç‰¹å¾
        missing = [f for f in model_features if f not in df_slice.columns]
        if missing:
            logger.error(f"ä¸¥é‡é”™è¯¯ï¼šæ•°æ®ä¸­ç¼ºå°‘æ¨¡å‹æ‰€éœ€çš„ç‰¹å¾: {missing}")
            logger.error("è¿™é€šå¸¸æ˜¯ç”±äºç‰¹å¾å·¥ç¨‹é…ç½® (rebuild_features.py) ä¸æ¨¡å‹è®­ç»ƒæ—¶çš„é…ç½®ä¸ä¸€è‡´å¯¼è‡´çš„ã€‚")
            return
            
        final_features = model_features
    else:
        logger.warning(f"æ¨¡å‹æœªè®°å½•ç‰¹å¾åï¼Œå°†ä½¿ç”¨æ‰€æœ‰ {len(final_features)} ä¸ª 'feat_' å¼€å¤´çš„åˆ—ã€‚å¯èƒ½ä¼šå¯¼è‡´ mismatch é”™è¯¯ã€‚")

    # 3.2 é¢„æµ‹åˆ†æ•°
    try:
        # ç¡®ä¿åˆ—é¡ºåºä¸æ¨¡å‹ä¸€è‡´
        X_pred = df_slice[final_features]
        pred_scores = model.predict(X_pred)
    except Exception as e:
        logger.error(f"é¢„æµ‹å¤±è´¥: {e}")
        return
    
    # æ„é€ åŒ…å«å†å²é¢„æµ‹çš„ DataFrame (ç”¨äºç­–ç•¥è®¡ç®—å¹³æ»‘åˆ†)
    pred_df = df_slice[["date", "symbol"]].copy()
    pred_df["pred_score"] = pred_scores
    
    # =======================================================
    # 4. ç­–ç•¥ç­›é€‰ (è¯»å–æ¨èä¸“ç”¨ Top-K é…ç½®)
    # =======================================================
    
    strat_cfg = GLOBAL_CONFIG["strategy"]
    rec_k = strat_cfg.get("recommend_top_k", strat_cfg.get("top_k", 5))
    
    logger.info(f"ç”Ÿæˆæ¨èåˆ—è¡¨é•¿åº¦: {rec_k} (å«å¤‡é€‰)")
    
    # å®ä¾‹åŒ–ç­–ç•¥æ—¶ä¼ å…¥ top_k
    strategy = TopKSignalStrategy(top_k=rec_k)
    
    # **å…³é”®ï¼šä¼ é€’åŒ…å«å†å²æ•°æ®çš„ pred_dfï¼Œä»¥ä¾¿ strategy.generate è®¡ç®—å¹³æ»‘å¾—åˆ†**
    # æ³¨æ„ï¼šéœ€ç¡®ä¿ src/strategy/signal.py å·²ä¿®æ”¹ä¸ºè¿”å›åŒ…å« pos_ratio çš„åˆ—
    recommend_df = strategy.generate(pred_df)
    
    # ç­›é€‰å‡ºæœ€æ–°çš„ä¿¡å·ï¼ˆå³ä»Šå¤©ï¼‰
    recommend_df_latest = recommend_df[recommend_df["date"] == latest_date].copy()
    
    # 5. è¾“å‡ºç»“æœ
    if recommend_df_latest.empty:
        logger.warning("ç­–ç•¥ç­›é€‰åæ— è‚¡ç¥¨å…¥é€‰ (å¯èƒ½éƒ½è¢«é£æ§å‰”é™¤æˆ–åˆ†æ•°ä¸è¶³)ã€‚")
        logger.info("Top 5 åŸå§‹é¢„æµ‹å¾—åˆ† (æœªç»è¿‡æ»¤):")
        print(pred_df[pred_df["date"] == latest_date].sort_values("pred_score", ascending=False).head(5))
        return

    # === [æ–°å¢] è·å–é£æ§ä»“ä½ç³»æ•° ===
    current_pos_ratio = 1.0
    if "pos_ratio" in recommend_df_latest.columns:
        # è·å–å½“å¤©çš„é£æ§ç³»æ•° (æ‰€æœ‰è‚¡ç¥¨åŒä¸€å¤©ç³»æ•°ç›¸åŒ)
        current_pos_ratio = recommend_df_latest["pos_ratio"].iloc[0]

    # è¡¥å……è‚¡ç¥¨åç§°ä»¥ä¾¿é˜…è¯»
    meta_path = os.path.join(GLOBAL_CONFIG["paths"]["data_meta"], "all_stocks_meta.parquet")
    if os.path.exists(meta_path):
        df_meta = read_parquet(meta_path)
        recommend_df_latest = pd.merge(recommend_df_latest, df_meta[["symbol", "name"]], on="symbol", how="left")
    
    # è¡¥å……åŸå§‹é¢„æµ‹åˆ†
    recommend_df_latest = pd.merge(recommend_df_latest, 
                                   pred_df[["date", "symbol", "pred_score"]], 
                                   on=["date", "symbol"], how="left")
    
    # æ ¼å¼åŒ–è¾“å‡º
    print("\n" + "="*70)
    print(f"ğŸŒŸ {latest_date.strftime('%Y-%m-%d')} æ¯æ—¥ç²¾é€‰æ¨è (Top {len(recommend_df_latest)}) ğŸŒŸ")
    
    # === [æ–°å¢] æ˜¾å¼æ‰“å°é£æ§çŠ¶æ€ ===
    print("-" * 70)
    print(f"ğŸ›¡ï¸  é£æ§ç³»ç»Ÿå»ºè®®æ€»ä»“ä½: {current_pos_ratio * 100:.0f}%")
    if current_pos_ratio < 1.0:
        if current_pos_ratio == 0.0:
            print("âš ï¸  [æé«˜é£é™©] å¤§ç›˜å¤„äºç†Šå¸‚é˜¶æ®µï¼Œç­–ç•¥å»ºè®®ç©ºä»“è§‚æœ›ï¼(åˆ—è¡¨ä¸­è‚¡ç¥¨ä»…ä¾›è·Ÿè¸ªç ”ç©¶)")
        else:
            print(f"âš ï¸  [é£é™©æç¤º] å¤§ç›˜å¤„äºéœ‡è¡/å›è°ƒé˜¶æ®µï¼Œå»ºè®®é™ä½ä»“ä½è‡³ {current_pos_ratio * 100:.0f}%")
    else:
        print("âœ…  [ç§¯æä¿¡å·] å¸‚åœºè¶‹åŠ¿è‰¯å¥½ï¼Œå»ºè®®æ­£å¸¸ä»“ä½æ“ä½œã€‚")
    print("-" * 70)
    
    # [ä¿®æ”¹] è¾“å‡ºåˆ—ä¸­åŠ å…¥ pos_ratio
    cols = ["symbol", "name", "pred_score", "pos_ratio", "weight"]
    print_cols = [c for c in cols if c in recommend_df_latest.columns]
    
    print_df = recommend_df_latest[print_cols].sort_values("pred_score", ascending=False).reset_index(drop=True)
    
    # å°è¯•ä½¿ç”¨ tabulate ç¾åŒ–è¾“å‡º
    try:
        # floatfmt æ§åˆ¶å°æ•°ä½æ•°ï¼Œè®© pred_score å’Œ weight æ˜¾ç¤ºæ›´æ¸…æ™°
        print(print_df.to_markdown(index=True, floatfmt=".4f"))
    except:
        print(print_df)
    
    # ä¿å­˜ç»“æœ
    out_dir = os.path.join(GLOBAL_CONFIG["paths"]["reports"], "daily_picks")
    if not os.path.exists(out_dir):
        os.makedirs(out_dir)
    # ä½¿ç”¨ latest_date ä½œä¸ºæ–‡ä»¶åæ—¥æœŸ
    out_file = os.path.join(out_dir, f"picks_{version}_{latest_date.strftime('%Y%m%d')}.csv")
    print_df.to_csv(out_file, index=False, encoding="utf-8-sig")
    print(f"\n[æ–‡ä»¶] æ¨èåˆ—è¡¨å·²ä¿å­˜è‡³: {out_file}")
    print("="*70)

if __name__ == "__main__":
    main()