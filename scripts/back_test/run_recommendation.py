# scripts/run_recommendation.py

import os
import sys
import pandas as pd
import datetime
import glob
import re

# è·¯å¾„é€‚é…
current_dir = os.path.dirname(os.path.abspath(__file__))
# ä»å½“å‰æ–‡ä»¶ä½ç½® (scripts/back_test) è¿”å›ä¸¤çº§åˆ°é¡¹ç›®æ ¹ç›®å½•
project_root = os.path.dirname(os.path.dirname(current_dir))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from src.utils.config import GLOBAL_CONFIG
from src.utils.logger import get_logger
from src.utils.io import read_parquet
from src.strategy.signal import TopKSignalStrategy

logger = get_logger()

def get_latest_model_path():
    """
    æ™ºèƒ½å¯»æ‰¾ data/models ä¸‹æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶
    æ”¯æŒè¯†åˆ«æ™®é€šè®­ç»ƒç›®å½•å’Œ WF (æ»šåŠ¨è®­ç»ƒ) ç›®å½•
    è¿”å›: (version, model_info_dict)
    model_info_dict æ ¼å¼:
      - å•æ¨¡å‹: {"type": "single", "path": "...", "format": "xgb/lgb"}
      - åŒå¤´: {"type": "dual_head", "reg_path": "...", "cls_path": "...", "format": "lgb"}
    """
    models_dir = GLOBAL_CONFIG["paths"]["models"]
    if not os.path.exists(models_dir):
        return None, None
    
    # 1. è·å–æ‰€æœ‰å­ç›®å½•
    subdirs = [d for d in os.listdir(models_dir) if os.path.isdir(os.path.join(models_dir, d))]
    if not subdirs:
        return None, None

    # 2. è¾…åŠ©å‡½æ•°ï¼šè§£æç›®å½•åä¸­çš„æ—¶é—´æˆ³
    def parse_timestamp(dir_name):
        clean_name = dir_name.replace("WF_", "")
        try:
            return datetime.datetime.strptime(clean_name, "%Y%m%d_%H%M%S")
        except ValueError:
            return datetime.datetime.min

    # 3. æŒ‰æ—¶é—´å€’åºæ’åˆ— (æœ€æ–°çš„åœ¨å‰)
    subdirs.sort(key=parse_timestamp, reverse=True)
    latest_version = subdirs[0]
    version_dir = os.path.join(models_dir, latest_version)
    
    logger.info(f"é”å®šæœ€æ–°æ¨¡å‹ç‰ˆæœ¬ç›®å½•: {latest_version}")

    # 4. æ£€æµ‹æ¨¡å‹ç±»å‹
    model_info = {}
    
    # æƒ…å†µ A: åŒå¤´æ¨¡å‹ (LightGBM joblib æ ¼å¼)
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ model_reg*.joblib å’Œ model_cls*.joblib
    reg_models = glob.glob(os.path.join(version_dir, "model_reg*.joblib"))
    cls_models = glob.glob(os.path.join(version_dir, "model_cls*.joblib"))
    
    if reg_models and cls_models:
        # åŒå¤´æ¨¡å‹ï¼Œæ‰¾å¹´ä»½æœ€å¤§çš„
        def extract_year(path):
            fname = os.path.basename(path)
            match = re.search(r"model_(?:reg|cls)_(\d+)\.joblib", fname)
            return int(match.group(1)) if match else 0
        
        best_reg = max(reg_models, key=extract_year)
        best_cls = max(cls_models, key=extract_year)
        best_year = extract_year(best_reg)
        
        logger.info(f"æ£€æµ‹åˆ°åŒå¤´æ¨¡å‹ï¼Œå·²è‡ªåŠ¨é€‰æ‹©æœ€æ–°å¹´ä»½: {best_year}")
        model_info = {
            "type": "dual_head",
            "reg_path": best_reg,
            "cls_path": best_cls,
            "format": "lgb"
        }
        return latest_version, model_info
    
    # æƒ…å†µ B: å•æ¨¡å‹ - LightGBM joblib (model_reg.joblib å•ç‹¬å­˜åœ¨)
    single_lgb = os.path.join(version_dir, "model_reg.joblib")
    if os.path.exists(single_lgb):
        model_info = {"type": "single", "path": single_lgb, "format": "lgb"}
        return latest_version, model_info
    
    # æƒ…å†µ C: å•æ¨¡å‹ - XGBoost json
    if os.path.exists(os.path.join(version_dir, "model.json")):
        model_info = {"type": "single", "path": os.path.join(version_dir, "model.json"), "format": "xgb"}
        return latest_version, model_info
    
    # æƒ…å†µ D: æ»šåŠ¨è®­ç»ƒçš„ XGBoost å¹´åº¦æ¨¡å‹ (model_2024.json ...)
    wf_xgb_models = glob.glob(os.path.join(version_dir, "model_*.json"))
    if wf_xgb_models:
        def extract_year_xgb(path):
            fname = os.path.basename(path)
            match = re.search(r"model_(\d+)\.json", fname)
            return int(match.group(1)) if match else 0
        
        best_model_path = max(wf_xgb_models, key=extract_year_xgb)
        best_year = extract_year_xgb(best_model_path)
        logger.info(f"æ£€æµ‹åˆ°æ»šåŠ¨è®­ç»ƒ XGBoost æ¨¡å‹ï¼Œå·²è‡ªåŠ¨é€‰æ‹©æœ€æ–°å¹´ä»½: model_{best_year}.json")
        model_info = {"type": "single", "path": best_model_path, "format": "xgb"}
        return latest_version, model_info

    return None, None

def load_model(model_info):
    """
    æ ¹æ® model_info åŠ è½½æ¨¡å‹
    è¿”å›: model æˆ– (reg_model, cls_model)
    """
    if model_info["type"] == "single":
        if model_info["format"] == "xgb":
            from src.model.xgb_model import XGBModelWrapper
            model = XGBModelWrapper()
            model.load(model_info["path"])
            return model, None
        else:  # lgb
            from src.model.lgb_model import LGBModelWrapper
            model = LGBModelWrapper(task_type="regression")
            model.load(model_info["path"])
            return model, None
    else:  # dual_head
        from src.model.lgb_model import LGBModelWrapper
        reg_model = LGBModelWrapper(task_type="regression")
        reg_model.load(model_info["reg_path"])
        cls_model = LGBModelWrapper(task_type="classification")
        cls_model.load(model_info["cls_path"])
        return reg_model, cls_model

def fuse_predictions(pred_reg, pred_cls, dual_head_cfg):
    """èåˆåŒå¤´æ¨¡å‹é¢„æµ‹ç»“æœ"""
    import numpy as np
    
    fusion_cfg = dual_head_cfg.get("fusion", {})
    normalize = fusion_cfg.get("normalize", True)
    reg_weight = dual_head_cfg.get("regression", {}).get("weight", 0.6)
    cls_weight = dual_head_cfg.get("classification", {}).get("weight", 0.4)
    
    if normalize:
        def min_max_normalize(arr):
            arr = np.array(arr)
            min_val, max_val = arr.min(), arr.max()
            if max_val - min_val < 1e-9:
                return np.zeros_like(arr)
            return (arr - min_val) / (max_val - min_val)
        pred_reg = min_max_normalize(pred_reg)
        pred_cls = min_max_normalize(pred_cls)
    
    return reg_weight * pred_reg + cls_weight * pred_cls

def load_latest_data():
    """åŠ è½½ç‰¹å¾æ•°æ®ï¼Œå¹¶æå–å‡ºã€æœ€è¿‘ N ä¸ªäº¤æ˜“æ—¥ã€‘çš„æ•°æ®ï¼Œç”¨äºé¢„æµ‹å’Œå¹³æ»‘ã€‚"""
    data_path = os.path.join(GLOBAL_CONFIG["paths"]["data_processed"], "all_stocks.parquet")
    if not os.path.exists(data_path):
        logger.error(f"ç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨: {data_path}ï¼Œè¯·å…ˆè¿è¡Œ rebuild_features.py")
        return None, None, None

    df = read_parquet(data_path)
    df["date"] = pd.to_datetime(df["date"])
    
    feat_cols = [c for c in df.columns if c.startswith("feat_")]
    
    N_DAYS = 3 
    unique_dates = sorted(df["date"].unique(), reverse=True)
    
    if len(unique_dates) < N_DAYS:
        logger.warning(f"æ€»äº¤æ˜“æ—¥ ({len(unique_dates)}) å°‘äºå¹³æ»‘çª—å£ ({N_DAYS}å¤©)ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®ã€‚")
        target_dates = unique_dates
    else:
        target_dates = unique_dates[:N_DAYS]
    
    df_slice = df[df["date"].isin(target_dates)].copy()
    
    if df_slice.empty:
        logger.error("æ•°æ®åˆ‡ç‰‡ä¸ºç©ºï¼Œæ— æ³•æ¨èã€‚")
        return None, None, None
    
    latest_date = unique_dates[0]
    logger.info(f"æ•°æ®é›†ä¸­æœ€æ–°æ—¥æœŸä¸º: {latest_date.strftime('%Y-%m-%d')}ï¼Œå°†åŠ è½½å‰ {len(target_dates)} ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®ã€‚")

    return df_slice, feat_cols, latest_date

def main():
    logger.info("=== å¯åŠ¨æ¯æ—¥æ¨èç³»ç»Ÿ (Daily Recommendation) ===")

    # è¯»å–åŒå¤´æ¨¡å‹é…ç½®
    dual_head_cfg = GLOBAL_CONFIG["model"].get("dual_head", {})
    dual_head_enabled = dual_head_cfg.get("enable", False)
    logger.info(f"åŒå¤´æ¨¡å‹é…ç½®: {'å¯ç”¨' if dual_head_enabled else 'ç¦ç”¨'}")

    # 1. æ™ºèƒ½åŠ è½½æ¨¡å‹
    version, model_info = get_latest_model_path()
    if not model_info:
        logger.error("æœªæ‰¾åˆ°å¯ç”¨æ¨¡å‹æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ run_walkforward.py æˆ– train_model.py")
        return
    
    logger.info(f"æ¨¡å‹ç±»å‹: {model_info['type']}, æ ¼å¼: {model_info['format']}")
    
    reg_model, cls_model = load_model(model_info)
    is_dual_head = model_info["type"] == "dual_head"
    
    # 2. åŠ è½½æœ€æ–°è¡Œæƒ…æ•°æ®ï¼ˆæœ€è¿‘ N å¤©ï¼‰
    df_slice, feat_cols, latest_date = load_latest_data()
    if df_slice is None or df_slice.empty:
        logger.error("æ— æ•°æ®åˆ‡ç‰‡ï¼Œæ— æ³•æ¨èã€‚")
        return

    # 3. æ‰§è¡Œé¢„æµ‹
    logger.info(f"æ­£åœ¨å¯¹ {len(df_slice)} è¡Œæ•°æ® ({df_slice['symbol'].nunique()} åªè‚¡ç¥¨) è¿›è¡Œæ‰“åˆ†...")
    
    # 3.1 ç‰¹å¾å¯¹é½
    final_features = feat_cols
    # ä½¿ç”¨æ¨¡å‹è®°å½•çš„ç‰¹å¾å
    if reg_model and hasattr(reg_model, 'feature_names') and reg_model.feature_names:
        model_features = reg_model.feature_names
        logger.info(f"ä½¿ç”¨æ¨¡å‹å†…ç½®ç‰¹å¾åˆ—è¡¨: {len(model_features)} ä¸ª")
        
        missing = [f for f in model_features if f not in df_slice.columns]
        if missing:
            logger.error(f"ä¸¥é‡é”™è¯¯ï¼šæ•°æ®ä¸­ç¼ºå°‘æ¨¡å‹æ‰€éœ€çš„ç‰¹å¾: {missing}")
            return
            
        final_features = model_features
    else:
        logger.warning(f"æ¨¡å‹æœªè®°å½•ç‰¹å¾åï¼Œå°†ä½¿ç”¨æ‰€æœ‰ {len(final_features)} ä¸ª 'feat_' å¼€å¤´çš„åˆ—ã€‚")

    # 3.2 é¢„æµ‹åˆ†æ•°
    try:
        X_pred = df_slice[final_features]
        
        if is_dual_head:
            pred_reg = reg_model.predict(X_pred)
            pred_cls = cls_model.predict(X_pred)
            pred_scores = fuse_predictions(pred_reg, pred_cls, dual_head_cfg)
            logger.info(f"åŒå¤´èåˆé¢„æµ‹å®Œæˆ (æƒé‡: reg={dual_head_cfg.get('regression', {}).get('weight', 0.6)}, cls={dual_head_cfg.get('classification', {}).get('weight', 0.4)})")
        else:
            pred_scores = reg_model.predict(X_pred)
            
    except Exception as e:
        logger.error(f"é¢„æµ‹å¤±è´¥: {e}")
        return
    
    # æ„é€ åŒ…å«å†å²é¢„æµ‹çš„ DataFrame (ç”¨äºç­–ç•¥è®¡ç®—å¹³æ»‘åˆ†)
    pred_df = df_slice[["date", "symbol"]].copy()
    pred_df["pred_score"] = pred_scores
    
    # =======================================================
    # 4. ç­–ç•¥ç­›é€‰ (è¯»å–æ¨èä¸“ç”¨ Top-K é…ç½®)
    # =======================================================
    
    strat_cfg = GLOBAL_CONFIG["strategy"]
    rec_k = strat_cfg.get("recommend_top_k", strat_cfg.get("top_k", 5))
    
    logger.info(f"ç”Ÿæˆæ¨èåˆ—è¡¨é•¿åº¦: {rec_k} (å«å¤‡é€‰)")
    
    strategy = TopKSignalStrategy(top_k=rec_k)
    recommend_df = strategy.generate(pred_df)
    recommend_df_latest = recommend_df[recommend_df["date"] == latest_date].copy()
    
    # 5. è¾“å‡ºç»“æœ
    if recommend_df_latest.empty:
        logger.warning("ç­–ç•¥ç­›é€‰åæ— è‚¡ç¥¨å…¥é€‰ (å¯èƒ½éƒ½è¢«é£æ§å‰”é™¤æˆ–åˆ†æ•°ä¸è¶³)ã€‚")
        logger.info("Top 5 åŸå§‹é¢„æµ‹å¾—åˆ† (æœªç»è¿‡æ»¤):")
        print(pred_df[pred_df["date"] == latest_date].sort_values("pred_score", ascending=False).head(5))
        return

    current_pos_ratio = 1.0
    if "pos_ratio" in recommend_df_latest.columns:
        current_pos_ratio = recommend_df_latest["pos_ratio"].iloc[0]

    meta_path = os.path.join(GLOBAL_CONFIG["paths"]["data_meta"], "all_stocks_meta.parquet")
    if os.path.exists(meta_path):
        df_meta = read_parquet(meta_path)
        recommend_df_latest = pd.merge(recommend_df_latest, df_meta[["symbol", "name"]], on="symbol", how="left")
    
    recommend_df_latest = pd.merge(recommend_df_latest, 
                                   pred_df[["date", "symbol", "pred_score"]], 
                                   on=["date", "symbol"], how="left")
    
    # æ ¼å¼åŒ–è¾“å‡º
    print("\n" + "="*70)
    print(f"ğŸŒŸ {latest_date.strftime('%Y-%m-%d')} æ¯æ—¥ç²¾é€‰æ¨è (Top {len(recommend_df_latest)}) ğŸŒŸ")
    
    if is_dual_head:
        print(f"ğŸ“Š ä½¿ç”¨åŒå¤´æ¨¡å‹ (å›å½’+åˆ†ç±»èåˆ)")
    
    print("-" * 70)
    print(f"ğŸ›¡ï¸  é£æ§ç³»ç»Ÿå»ºè®®æ€»ä»“ä½: {current_pos_ratio * 100:.0f}%")
    if current_pos_ratio < 1.0:
        if current_pos_ratio == 0.0:
            print("âš ï¸  [æé«˜é£é™©] å¤§ç›˜å¤„äºç†Šå¸‚é˜¶æ®µï¼Œç­–ç•¥å»ºè®®ç©ºä»“è§‚æœ›ï¼(åˆ—è¡¨ä¸­è‚¡ç¥¨ä»…ä¾›è·Ÿè¸ªç ”ç©¶)")
        else:
            print(f"âš ï¸  [é£é™©æç¤º] å¤§ç›˜å¤„äºéœ‡è¡/å›è°ƒé˜¶æ®µï¼Œå»ºè®®é™ä½ä»“ä½è‡³ {current_pos_ratio * 100:.0f}%")
    else:
        print("âœ…  [ç§¯æä¿¡å·] å¸‚åœºè¶‹åŠ¿è‰¯å¥½ï¼Œå»ºè®®æ­£å¸¸ä»“ä½æ“ä½œã€‚")
    print("-" * 70)
    
    cols = ["symbol", "name", "pred_score", "pos_ratio", "weight"]
    print_cols = [c for c in cols if c in recommend_df_latest.columns]
    
    print_df = recommend_df_latest[print_cols].sort_values("pred_score", ascending=False).reset_index(drop=True)
    
    try:
        print(print_df.to_markdown(index=True, floatfmt=".4f"))
    except:
        print(print_df)
    
    # ä¿å­˜ç»“æœ
    out_dir = os.path.join(GLOBAL_CONFIG["paths"]["reports"], "daily_picks")
    if not os.path.exists(out_dir):
        os.makedirs(out_dir)
    out_file = os.path.join(out_dir, f"picks_{version}_{latest_date.strftime('%Y%m%d')}.csv")
    print_df.to_csv(out_file, index=False, encoding="utf-8-sig")
    print(f"\n[æ–‡ä»¶] æ¨èåˆ—è¡¨å·²ä¿å­˜è‡³: {out_file}")
    print("="*70)

if __name__ == "__main__":
    main()
