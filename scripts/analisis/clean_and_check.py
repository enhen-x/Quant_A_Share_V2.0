# scripts/clean_and_check.py

import os
import sys
import pandas as pd
import numpy as np
from tqdm import tqdm

# è·¯å¾„é€‚é…
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from src.utils.config import GLOBAL_CONFIG
from src.utils.io import read_parquet, save_parquet, ensure_dir, save_csv
from src.utils.logger import get_logger
from src.data_source.datahub import DataHub

logger = get_logger()

class DataCleaner:
    def __init__(self):
        self.raw_dir = GLOBAL_CONFIG["paths"]["data_raw"]
        # è¾“å‡ºç›®å½•ï¼šæ¸…æ´—åçš„æ•°æ®
        self.cleaned_dir = GLOBAL_CONFIG["paths"].get("data_cleaned", 
                                                      os.path.join(GLOBAL_CONFIG["paths"]["data_root"], "raw_cleaned"))
        # è¯¦ç»†æŠ¥å‘Šï¼ˆæ¯åªè‚¡ç¥¨ä¸€è¡Œï¼‰
        self.detail_report_path = os.path.join(self.cleaned_dir, "data_quality_report.csv")
        # [æ–°å¢] æ±‡æ€»æŠ¥å‘Šï¼ˆå…¨å±€ç»Ÿè®¡ï¼‰
        self.summary_report_path = os.path.join(self.cleaned_dir, "data_cleaning_summary.csv")
        
        ensure_dir(self.cleaned_dir)
        
        # === 1. è·å–æ¸…æ´—/ç­›é€‰é˜ˆå€¼ ===
        quality_cfg = GLOBAL_CONFIG.get("preprocessing", {}).get("quality", {})
        
        self.limit_suspension = quality_cfg.get("max_suspension_rate", 0.1) 
        self.limit_turnover = quality_cfg.get("min_avg_turnover", 1.0)
        
        logger.info(f"æ¸…æ´—é˜ˆå€¼è®¾å®š: æœ€å¤§åœç‰Œç‡={self.limit_suspension:.1%}, æœ€ä½æ—¥å‡æ¢æ‰‹={self.limit_turnover}%")

        # === 2. åŠ è½½äº¤æ˜“æ—¥å† ===
        self.datahub = DataHub()
        logger.info("æ­£åœ¨è¯»å–æœ¬åœ°äº¤æ˜“æ—¥å†ä»¥è®¡ç®—ç¼ºå¤±ç‡...")
        
        self.calendar_df = self.datahub.load_local_trade_calendar()
        
        if self.calendar_df.empty:
            logger.error("äº¤æ˜“æ—¥å†åŠ è½½å¤±è´¥ï¼Œè¯·å…ˆè¿è¡Œ scripts/init_stock_pool.py")
            raise FileNotFoundError("Trade calendar is empty")
            
        self.trade_dates = set(pd.to_datetime(self.calendar_df["date"]).dt.date)

    def check_and_clean_single(self, file_path: str) -> dict:
        """
        å¤„ç†å•åªè‚¡ç¥¨
        """
        file_name = os.path.basename(file_path)
        symbol = file_name.replace(".parquet", "")
        
        try:
            df = read_parquet(file_path)
        except Exception as e:
            logger.error(f"è¯»å–å¤±è´¥ {file_name}: {e}")
            return {"symbol": symbol, "status": "ERROR_READ"}

        if "date" not in df.columns:
            return {"symbol": symbol, "status": "ERROR_NO_DATE"}
        
        df["date"] = pd.to_datetime(df["date"])
        df = df.sort_values("date").reset_index(drop=True)
        
        initial_count = len(df)
        
        # åˆå§‹åŒ–ç»Ÿè®¡å­—å…¸
        stats = {
            "symbol": symbol,
            "status": "OK",             
            "reason": "",               
            "total_rows": initial_count,
            "start_date": df["date"].min() if not df.empty else None,
            "end_date": df["date"].max() if not df.empty else None,
            "n_duplicates": 0,
            "n_zero_price": 0,
            "n_suspension": 0,
            "suspension_ratio": 0.0,
            "avg_turnover": 0.0,
            "n_missing_days": 0,
            "clean_rows": 0   # è¡Œçº§æ¸…æ´—åçš„è¡Œæ•°ï¼ˆä¸è®ºè¯¥è‚¡ç¥¨æœ€ç»ˆæ˜¯å¦è¢« Rejectï¼‰
        }

        if df.empty:
            stats["status"] = "REJECT_EMPTY"
            return stats

        # ==========================
        # Step A: è¡Œçº§æ¸…æ´— (Row-level Cleaning)
        # ==========================
        
        # 1. å»é‡
        if df["date"].duplicated().any():
            stats["n_duplicates"] = df["date"].duplicated().sum()
            df = df.drop_duplicates(subset=["date"], keep="last")

        # 2. ä»·æ ¼å¼‚å¸¸å¤„ç† (Close <= 0 æˆ– NaN)
        price_cols = ["open", "high", "low", "close", "volume"]
        valid_cols = [c for c in price_cols if c in df.columns]
        
        # å‰”é™¤ NaN
        nan_mask = df[valid_cols].isnull().any(axis=1)
        df = df[~nan_mask]
        
        # å‰”é™¤ 0 ä»·æ ¼
        if "close" in df.columns:
            zero_price_mask = (df["close"] <= 1e-4)
            stats["n_zero_price"] = zero_price_mask.sum()
            df = df[~zero_price_mask]

        if df.empty:
            stats["status"] = "REJECT_EMPTY_AFTER_CLEAN"
            return stats

        stats["clean_rows"] = len(df)

        # ==========================
        # Step B: æŒ‡æ ‡è®¡ç®— (Metrics)
        # ==========================
        # 1. åœç‰Œç»Ÿè®¡
        if "volume" in df.columns:
            suspension_mask = (df["volume"] < 1e-6)
            stats["n_suspension"] = suspension_mask.sum()
            stats["suspension_ratio"] = stats["n_suspension"] / len(df)
        
        # 2. æ¢æ‰‹ç‡ç»Ÿè®¡
        if "turnover" in df.columns:
            stats["avg_turnover"] = df["turnover"].mean()
        
        # 3. æ—¥æœŸç¼ºå¤±
        s_date = df["date"].min().date()
        e_date = df["date"].max().date()
        expected_dates = {d for d in self.trade_dates if s_date <= d <= e_date}
        actual_dates = set(df["date"].dt.date)
        missing_dates = expected_dates - actual_dates
        stats["n_missing_days"] = len(missing_dates)
        
        denom = len(expected_dates)
        missing_ratio = len(missing_dates) / denom if denom > 0 else 0.0

        # ==========================
        # Step C: æ ‡çš„çº§ç­›é€‰ (Stock-level Filter)
        # ==========================
        
        # è§„åˆ™ 1: åœç‰Œç‡è¿‡é«˜
        if stats["suspension_ratio"] > self.limit_suspension:
            stats["status"] = "REJECT"
            stats["reason"] = "HIGH_SUSPENSION"
            return stats 

        # è§„åˆ™ 2: æµåŠ¨æ€§æ¯ç«­(æ•°æ®æ¸…æ´—é˜¶æ®µå·²æ³¨é‡Šæ‰)
        # if "turnover" in df.columns and stats["avg_turnover"] < self.limit_turnover:
        #     stats["status"] = "REJECT"
        #     stats["reason"] = "LOW_LIQUIDITY"
        #     return stats
        
        if "turnover" in df.columns and stats["avg_turnover"] < self.limit_turnover:
             # ä»…åšè®°å½•ï¼Œä¸æ‹’ç»
             pass

        # è§„åˆ™ 3: æ•°æ®ä¸¥é‡ç¼ºå¤±
        if missing_ratio > 0.5:
            stats["status"] = "REJECT"
            stats["reason"] = "HIGH_MISSING"
            return stats

        # ==========================
        # Step D: ä¿å­˜æœ‰æ•ˆæ•°æ®
        # ==========================
        save_path = os.path.join(self.cleaned_dir, f"{symbol}.parquet")
        save_parquet(df, save_path)
        
        return stats

    def run(self):
        logger.info(f"=== å¼€å§‹æ•°æ®æ¸…æ´—ä¸è´¨æ£€ (v2.0 å¢å¼ºç‰ˆ) ===")
        logger.info(f"æºæ•°æ®: {self.raw_dir}")
        logger.info(f"è¾“å‡ºç›®æ ‡: {self.cleaned_dir}")
        
        if not os.path.exists(self.raw_dir):
            logger.warning("åŸå§‹æ•°æ®ç›®å½•ä¸å­˜åœ¨")
            return

        files = [f for f in os.listdir(self.raw_dir) if f.endswith(".parquet") and f[0].isdigit()]
        logger.info(f"å¾…å¤„ç†æ–‡ä»¶æ•°: {len(files)}")
        
        results = []
        
        for f in tqdm(files, desc="Cleaning"):
            stats = self.check_and_clean_single(os.path.join(self.raw_dir, f))
            if stats:
                results.append(stats)
            
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š & æ±‡æ€»æŠ¥å‘Š
        if results:
            # 1. ä¿å­˜è¯¦ç»†æŠ¥å‘Š
            df_report = pd.DataFrame(results)
            df_report = df_report.sort_values(by=["status", "suspension_ratio"], ascending=[False, False])
            save_csv(df_report, self.detail_report_path)
            
            # 2. ç”Ÿæˆå¹¶ä¿å­˜æ±‡æ€»æŠ¥å‘Š
            self._save_summary_report(results)
            
            # 3. ç®€å•æ—¥å¿—
            rejected_count = len(df_report[df_report["status"] != "OK"])
            logger.info("-" * 40)
            logger.info(f"æ¸…æ´—å®Œæˆï¼")
            logger.info(f"  - æ€»å¤„ç†è‚¡ç¥¨: {len(files)}")
            logger.info(f"  - æœ‰æ•ˆä¿ç•™ (OK): {len(files) - rejected_count}")
            logger.info(f"  - å‰”é™¤è‚¡ç¥¨ (REJECT): {rejected_count}")
            logger.info(f"  - è¯¦ç»†æŠ¥å‘Šä½ç½®: {self.detail_report_path}")
            logger.info("-" * 40)

    def _save_summary_report(self, results: list):
        """
        è®¡ç®—å…¨å±€è¡Œçº§æŸå¤±å¹¶ç”ŸæˆæŠ¥å‘Š
        """
        # 1. åŸºç¡€èšåˆ
        # è¾“å…¥æ€»è¡Œæ•°
        total_input_rows = sum(r.get("total_rows", 0) for r in results)
        
        # æœ€ç»ˆè¾“å‡ºè¡Œæ•° (ä»…ç»Ÿè®¡ Status=OK çš„ Clean Rows)
        total_output_rows = sum(r.get("clean_rows", 0) for r in results if r["status"] == "OK")
        
        # 2. è®¡ç®—å„ç¯èŠ‚ä¸¢å¼ƒçš„è¡Œæ•°
        
        # A. è¡Œçº§æ¸…æ´—ä¸¢å¼ƒ (Duplicates / Zero Price)
        # è¿™äº›æ˜¯åœ¨æ‰€æœ‰è‚¡ç¥¨ä¸­éƒ½ä¼šå‘ç”Ÿçš„ï¼Œä¸è®ºè¯¥è‚¡ç¥¨æœ€åæ˜¯å¦è¢«å‰”é™¤
        dropped_by_duplicates = sum(r.get("n_duplicates", 0) for r in results)
        dropped_by_zero_price = sum(r.get("n_zero_price", 0) for r in results)
        
        # B. æ ‡çš„çº§å‰”é™¤é€ æˆçš„æŸå¤± (Stock Rejection)
        # å¦‚æœä¸€åªè‚¡ç¥¨è¢«å‰”é™¤ï¼Œå®ƒå‰©ä¸‹çš„æ‰€æœ‰è¡Œ (clean_rows) éƒ½è¢«è§†ä¸ºæŸå¤±
        dropped_by_suspension = sum(r["clean_rows"] for r in results if r["reason"] == "HIGH_SUSPENSION")
        dropped_by_liquidity = sum(r["clean_rows"] for r in results if r["reason"] == "LOW_LIQUIDITY")
        dropped_by_missing = sum(r["clean_rows"] for r in results if r["reason"] == "HIGH_MISSING")
        dropped_by_other = sum(r["clean_rows"] for r in results if r["status"] != "OK" 
                               and r["reason"] not in ["HIGH_SUSPENSION", "LOW_LIQUIDITY", "HIGH_MISSING"])

        dropped_total = total_input_rows - total_output_rows
        
        # 3. æ„é€ æŠ¥å‘Šåˆ—è¡¨
        report_data = []
        
        # æ€»ä½“
        report_data.append({
            "Category": "SUMMARY", "Item": "Total Input Rows", 
            "Count": total_input_rows, "Ratio (%)": 100.0
        })
        report_data.append({
            "Category": "SUMMARY", "Item": "Final Output Rows", 
            "Count": total_output_rows, 
            "Ratio (%)": round((total_output_rows / total_input_rows * 100), 2) if total_input_rows else 0
        })
        report_data.append({
            "Category": "SUMMARY", "Item": "Total Dropped", 
            "Count": dropped_total, 
            "Ratio (%)": round((dropped_total / total_input_rows * 100), 2) if total_input_rows else 0
        })
        
        # ç»†èŠ‚
        details = [
            ("ROW_CLEANING", "Dropped (Duplicates)", dropped_by_duplicates),
            ("ROW_CLEANING", "Dropped (Zero/NaN Price)", dropped_by_zero_price),
            ("STOCK_REJECT", "Dropped (High Suspension)", dropped_by_suspension),
            ("STOCK_REJECT", "Dropped (Low Liquidity)", dropped_by_liquidity),
            ("STOCK_REJECT", "Dropped (High Missing Data)", dropped_by_missing),
            ("STOCK_REJECT", "Dropped (Other Reasons)", dropped_by_other),
        ]
        
        # æŒ‰ä¸¢å¼ƒæ•°é‡æ’åº
        details.sort(key=lambda x: x[2], reverse=True)
        
        for cat, item, count in details:
            if count > 0:
                report_data.append({
                    "Category": cat,
                    "Item": item,
                    "Count": count,
                    "Ratio (%)": round((count / total_input_rows * 100), 2) if total_input_rows else 0
                })
                
        # 4. ä¿å­˜ä¸æ‰“å°
        df_summary = pd.DataFrame(report_data)
        save_csv(df_summary, self.summary_report_path)
        
        logger.info("=" * 50)
        logger.info(f"ğŸ“Š æ•°æ®æ¸…æ´—ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜: {self.summary_report_path}")
        logger.info("-" * 50)
        if not df_summary.empty:
            print(df_summary[["Item", "Count", "Ratio (%)"]].to_string(index=False))
            
            # æ‰¾å‡ºæœ€å¤§æ€æ‰‹
            df_reasons = df_summary[df_summary["Category"].isin(["ROW_CLEANING", "STOCK_REJECT"])]
            if not df_reasons.empty:
                max_row = df_reasons.loc[df_reasons["Count"].idxmax()]
                logger.info("-" * 50)
                logger.info(f"ğŸš« æ ·æœ¬å‰Šå‡æœ€å¤§å› ç´ : ã€{max_row['Item']}ã€‘")
        logger.info("=" * 50)

if __name__ == "__main__":
    cleaner = DataCleaner()
    cleaner.run()