# scripts/analisis/check_monte_carlo.py
# ============================================================================
# è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿåˆ†æ (Monte Carlo Simulation Analysis)
# ============================================================================
#
# ã€åŠŸèƒ½ã€‘
# å¯¹åŒå¤´æ¨¡å‹çš„é¢„æµ‹ç»“æœè¿›è¡Œè’™ç‰¹å¡æ´›æ¨¡æ‹Ÿï¼Œè¯„ä¼°ç­–ç•¥çš„ç¨³å¥æ€§å’Œç½®ä¿¡åŒºé—´ã€‚
# åŒ…å«4ç§æ¨¡æ‹Ÿæ–¹æ³•ï¼š
#   1. Bootstrap é‡é‡‡æ · - è¯„ä¼°æ”¶ç›Šåˆ†å¸ƒç½®ä¿¡åŒºé—´
#   2. æƒé‡æ‰°åŠ¨ - è¯„ä¼°èåˆæƒé‡æ•æ„Ÿæ€§
#   3. å™ªéŸ³æ³¨å…¥ - è¯„ä¼°æ¨¡å‹æŠ—å¹²æ‰°èƒ½åŠ›
#   4. æ—¶é—´çª—å£é‡‡æ · - è¯„ä¼°ç­–ç•¥æ—¶é—´ç¨³å®šæ€§
#
# ã€ä½¿ç”¨æ–¹æ³•ã€‘
# python scripts/analisis/check_monte_carlo.py
# ============================================================================

import os
import sys
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings("ignore")

# Matplotlib å­—ä½“é…ç½®ï¼ˆå¿…é¡»åœ¨ import pyplot ä¹‹å‰è®¾ç½®ï¼‰
import matplotlib
matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS', 'sans-serif']
matplotlib.rcParams['axes.unicode_minus'] = False

import matplotlib.pyplot as plt
from matplotlib.ticker import ScalarFormatter

# è·¯å¾„é€‚é…
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from src.utils.config import GLOBAL_CONFIG
from src.utils.logger import get_logger
from src.utils.io import read_parquet, ensure_dir
from src.strategy.signal import TopKSignalStrategy
from src.backtest.backtester import VectorBacktester

logger = get_logger()


# ============================================================================
# è’™ç‰¹å¡æ´›åˆ†æå™¨
# ============================================================================

class MonteCarloAnalyzer:
    """
    è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿåˆ†æå™¨
    
    å¯¹åŒå¤´æ¨¡å‹é¢„æµ‹ç»“æœè¿›è¡Œå¤šç§æ¨¡æ‹Ÿåˆ†æï¼Œè¯„ä¼°ç­–ç•¥ç¨³å¥æ€§ã€‚
    """
    
    def __init__(self, n_simulations: int = 500, random_seed: int = 42):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            n_simulations: æ¨¡æ‹Ÿæ¬¡æ•°
            random_seed: éšæœºç§å­ï¼ˆä¿è¯å¯å¤ç°ï¼‰
        """
        self.n_simulations = n_simulations
        self.random_seed = random_seed
        np.random.seed(random_seed)
        
        self.config = GLOBAL_CONFIG
        self.dual_head_cfg = self.config["model"].get("dual_head", {})
        
        # è·å–åŸå§‹èåˆæƒé‡
        self.base_return_weight = self.dual_head_cfg.get("return_head", {}).get(
            "weight",
            self.dual_head_cfg.get("regression", {}).get("weight", 0.6),
        )
        self.base_risk_weight = self.dual_head_cfg.get("risk_head", {}).get(
            "weight",
            self.dual_head_cfg.get("classification", {}).get("weight", 0.4),
        )
        self.fusion_cfg = self.dual_head_cfg.get("fusion", {})
        
        # å›æµ‹å™¨å’Œç­–ç•¥
        self.backtester = VectorBacktester()
        self.strategy = TopKSignalStrategy()
        
        # å¦‚æœæœªå¼€å¯ä»“ä½ç®¡ç†ï¼Œå¼ºåˆ¶æ»¡ä»“æµ‹è¯•
        if not self.config["strategy"].get("position_control", {}).get("enable", False):
            self.strategy.min_score = -999.0
        
        # è¾“å‡ºç›®å½•
        self.report_dir = os.path.join(self.config["paths"]["reports"], "monte_carlo")
        ensure_dir(self.report_dir)
    
    def load_predictions(self) -> Optional[pd.DataFrame]:
        """åŠ è½½æœ€æ–°çš„é¢„æµ‹æ–‡ä»¶"""
        models_dir = self.config["paths"]["models"]
        if not os.path.exists(models_dir):
            return None
        
        subdirs = [d for d in os.listdir(models_dir) if os.path.isdir(os.path.join(models_dir, d))]
        if not subdirs:
            return None
        
        subdirs.sort(reverse=True)
        latest_dir = subdirs[0]
        pred_path = os.path.join(models_dir, latest_dir, "predictions.parquet")
        
        if os.path.exists(pred_path):
            logger.info(f"ä½¿ç”¨é¢„æµ‹æ–‡ä»¶: {pred_path}")
            df = read_parquet(pred_path)
            df["date"] = pd.to_datetime(df["date"])
            return self._ensure_pred_score(df)
        return None
    
    def _get_dual_head_cols(self, pred_df: pd.DataFrame) -> Optional[Tuple[str, str]]:
        if "pred_return" in pred_df.columns and "pred_risk" in pred_df.columns:
            return "pred_return", "pred_risk"
        if "pred_reg" in pred_df.columns and "pred_cls" in pred_df.columns:
            return "pred_reg", "pred_cls"
        return None

    @staticmethod
    def _min_max_normalize(arr: np.ndarray) -> np.ndarray:
        arr = np.array(arr)
        min_val, max_val = arr.min(), arr.max()
        if max_val - min_val > 1e-8:
            return (arr - min_val) / (max_val - min_val)
        return np.zeros_like(arr)

    def _fuse_predictions(
        self,
        pred_return: np.ndarray,
        pred_risk: np.ndarray,
        method: Optional[str] = None,
        return_weight: Optional[float] = None,
        risk_weight: Optional[float] = None,
        risk_aversion: Optional[float] = None,
    ) -> np.ndarray:
        method = method or self.fusion_cfg.get("method", "rank_ratio")
        normalize = self.fusion_cfg.get("normalize", True)
        return_weight = return_weight if return_weight is not None else self.base_return_weight
        risk_weight = risk_weight if risk_weight is not None else self.base_risk_weight
        risk_aversion = risk_aversion if risk_aversion is not None else self.fusion_cfg.get("risk_aversion", 2.0)

        if pred_return is None or pred_risk is None:
            logger.warning("pred_return or pred_risk is missing; fallback to available predictions.")
            return pred_return if pred_return is not None else pred_risk

        if method == "sharpe_like":
            epsilon = 1e-6
            return pred_return / (pred_risk + epsilon)
        if method == "rank_ratio":
            from scipy.stats import rankdata
            rank_return = rankdata(pred_return)
            rank_risk = rankdata(pred_risk)
            return rank_return / (rank_risk + 1)
        if method == "utility":
            if normalize:
                pred_return = self._min_max_normalize(pred_return)
                pred_risk = self._min_max_normalize(pred_risk)
            return pred_return - risk_aversion * (pred_risk ** 2)
        if method == "weighted_average":
            if normalize:
                pred_return = self._min_max_normalize(pred_return)
                pred_risk = self._min_max_normalize(pred_risk)
            return return_weight * pred_return - risk_weight * pred_risk

        logger.warning("Unknown fusion method: %s. Fallback to rank_ratio.", method)
        from scipy.stats import rankdata
        rank_return = rankdata(pred_return)
        rank_risk = rankdata(pred_risk)
        return rank_return / (rank_risk + 1)

    def _ensure_pred_score(self, pred_df: pd.DataFrame) -> pd.DataFrame:
        if "pred_score" in pred_df.columns and pred_df["pred_score"].notna().any():
            return pred_df
        dual_cols = self._get_dual_head_cols(pred_df)
        if not dual_cols:
            return pred_df
        pred_df = pred_df.copy()
        return_col, risk_col = dual_cols
        pred_df["pred_score"] = self._fuse_predictions(
            pred_df[return_col].values,
            pred_df[risk_col].values,
        )
        return pred_df
    
    def _run_single_backtest(self, pred_df: pd.DataFrame, silent: bool = True) -> Optional[Dict]:
        """
        æ‰§è¡Œå•æ¬¡å›æµ‹
        
        Returns:
            åŒ…å«ç»©æ•ˆæŒ‡æ ‡çš„å­—å…¸ï¼Œå¤±è´¥è¿”å› None
        """
        try:
            signal_df = self.strategy.generate(pred_df)
            if signal_df.empty:
                return None
            
            # ä½¿ç”¨ä¸´æ—¶ç›®å½•é¿å…è¦†ç›–
            temp_dir = os.path.join(self.report_dir, "_temp")
            metrics = self.backtester.run(signal_df, output_dir=temp_dir)
            
            return {
                "annual_return": metrics["annual_return"],
                "sharpe": metrics["sharpe"],
                "max_drawdown": metrics["max_drawdown"],
                "total_return": metrics.get("total_return", 0),
                "volatility": metrics.get("volatility", 0),
                "equity_curve": metrics.get("equity_curve")
            }
        except Exception as e:
            if not silent:
                logger.warning(f"å›æµ‹å¤±è´¥: {e}")
            return None
    
    # ========================================================================
    # æ¨¡æ‹Ÿæ–¹æ³• 1: Bootstrap é‡é‡‡æ ·
    # ========================================================================
    def run_bootstrap_simulation(self, pred_df: pd.DataFrame) -> List[Dict]:
        """
        Bootstrap é‡é‡‡æ ·æ¨¡æ‹Ÿ
        
        å¯¹æ¯æ—¥çš„è‚¡ç¥¨ä¿¡å·è¿›è¡Œæœ‰æ”¾å›æŠ½æ ·ï¼Œé‡æ–°è®¡ç®—æ”¶ç›Šã€‚
        
        Returns:
            æ¨¡æ‹Ÿç»“æœåˆ—è¡¨
        """
        logger.info(f">>> æ‰§è¡Œ Bootstrap é‡é‡‡æ ·æ¨¡æ‹Ÿ ({self.n_simulations} æ¬¡)...")
        results = []
        
        # æŒ‰æ—¥æœŸåˆ†ç»„
        dates = pred_df["date"].unique()
        
        for i in range(self.n_simulations):
            # éšæœºæŠ½æ · 80% çš„æ—¥æœŸï¼ˆæœ‰æ”¾å›ï¼‰
            sample_dates = np.random.choice(dates, size=int(len(dates) * 0.8), replace=True)
            sample_df = pred_df[pred_df["date"].isin(sample_dates)].copy()
            
            if len(sample_df) < 100:
                continue
            
            metrics = self._run_single_backtest(sample_df)
            if metrics:
                metrics["simulation_id"] = i
                metrics["method"] = "bootstrap"
                results.append(metrics)
            
            if (i + 1) % 100 == 0:
                logger.info(f"  Bootstrap è¿›åº¦: {i + 1}/{self.n_simulations}")
        
        logger.info(f"  Bootstrap å®Œæˆ: {len(results)} æ¬¡æœ‰æ•ˆæ¨¡æ‹Ÿ")
        return results
    
    # ========================================================================
    # æ¨¡æ‹Ÿæ–¹æ³• 2: æƒé‡æ‰°åŠ¨
    # ========================================================================
    def run_weight_perturbation(self, pred_df: pd.DataFrame) -> List[Dict]:
        """
        æƒé‡æ‰°åŠ¨æ¨¡æ‹Ÿ
        
        éšæœºæ‰°åŠ¨å›å½’/åˆ†ç±»èåˆæƒé‡ï¼Œè¯„ä¼°æƒé‡æ•æ„Ÿæ€§ã€‚
        
        Returns:
            æ¨¡æ‹Ÿç»“æœåˆ—è¡¨
        """
        logger.info(f">>> æ‰§è¡Œæƒé‡æ‰°åŠ¨æ¨¡æ‹Ÿ ({self.n_simulations} æ¬¡)...")
        results = []

        dual_cols = self._get_dual_head_cols(pred_df)
        if not dual_cols:
            logger.warning("  Missing dual-head prediction columns, skip weight perturbation.")
            return results

        fusion_method = self.fusion_cfg.get("method", "rank_ratio")
        if fusion_method != "weighted_average":
            logger.warning("  Fusion method=%s. Weight perturbation only supports weighted_average.", fusion_method)
            return results

        return_col, risk_col = dual_cols

        for i in range(self.n_simulations):
            return_weight = np.random.uniform(0.2, 0.8)
            risk_weight = 1.0 - return_weight

            perturbed_df = pred_df.copy()
            perturbed_df["pred_score"] = self._fuse_predictions(
                perturbed_df[return_col].values,
                perturbed_df[risk_col].values,
                method="weighted_average",
                return_weight=return_weight,
                risk_weight=risk_weight,
            )

            metrics = self._run_single_backtest(perturbed_df)
            if metrics:
                metrics["simulation_id"] = i
                metrics["method"] = "weight_perturbation"
                metrics["return_weight"] = return_weight
                metrics["risk_weight"] = risk_weight
                results.append(metrics)

            if (i + 1) % 100 == 0:
                logger.info(f"  æƒé‡æ‰°åŠ¨è¿›åº¦: {i + 1}/{self.n_simulations}")

        logger.info(f"  æƒé‡æ‰°åŠ¨å®Œæˆ: {len(results)} æ¬¡æœ‰æ•ˆæ¨¡æ‹Ÿ")
        return results
    
    # ========================================================================
    # æ¨¡æ‹Ÿæ–¹æ³• 3: å™ªéŸ³æ³¨å…¥
    # ========================================================================
    def run_noise_injection(self, pred_df: pd.DataFrame) -> List[Dict]:
        """
        å™ªéŸ³æ³¨å…¥æ¨¡æ‹Ÿ
        
        å‘é¢„æµ‹åˆ†æ•°æ·»åŠ éšæœºå™ªéŸ³ï¼Œè¯„ä¼°æ¨¡å‹æŠ—å¹²æ‰°èƒ½åŠ›ã€‚
        
        Returns:
            æ¨¡æ‹Ÿç»“æœåˆ—è¡¨
        """
        logger.info(f">>> æ‰§è¡Œå™ªéŸ³æ³¨å…¥æ¨¡æ‹Ÿ ({self.n_simulations} æ¬¡)...")
        results = []
        
        # å™ªéŸ³æ¯”ä¾‹èŒƒå›´
        noise_levels = np.linspace(0.0, 0.3, 20)  # 0% ~ 30%
        repeats_per_level = max(1, self.n_simulations // len(noise_levels))
        
        for noise_ratio in noise_levels:
            for j in range(repeats_per_level):
                noisy_df = pred_df.copy()
                
                # æ·»åŠ å™ªéŸ³
                noise = noisy_df["pred_score"].std() * noise_ratio * np.random.randn(len(noisy_df))
                noisy_df["pred_score"] = noisy_df["pred_score"] + noise
                
                metrics = self._run_single_backtest(noisy_df)
                if metrics:
                    metrics["simulation_id"] = len(results)
                    metrics["method"] = "noise_injection"
                    metrics["noise_ratio"] = noise_ratio
                    results.append(metrics)
        
        logger.info(f"  å™ªéŸ³æ³¨å…¥å®Œæˆ: {len(results)} æ¬¡æœ‰æ•ˆæ¨¡æ‹Ÿ")
        return results
    
    # ========================================================================
    # æ¨¡æ‹Ÿæ–¹æ³• 4: æ—¶é—´çª—å£é‡‡æ ·
    # ========================================================================
    def run_time_window_sampling(self, pred_df: pd.DataFrame) -> List[Dict]:
        """
        æ—¶é—´çª—å£é‡‡æ ·æ¨¡æ‹Ÿ
        
        éšæœºé‡‡æ ·ä¸åŒæ—¶é—´åŒºé—´è¿›è¡Œå›æµ‹ï¼Œè¯„ä¼°ç­–ç•¥æ—¶é—´ç¨³å®šæ€§ã€‚
        
        Returns:
            æ¨¡æ‹Ÿç»“æœåˆ—è¡¨
        """
        logger.info(f">>> æ‰§è¡Œæ—¶é—´çª—å£é‡‡æ ·æ¨¡æ‹Ÿ ({self.n_simulations} æ¬¡)...")
        results = []
        
        dates = sorted(pred_df["date"].unique())
        total_days = len(dates)
        min_window = max(60, total_days // 4)  # æœ€å°‘ 60 å¤©æˆ–æ€»å¤©æ•°çš„ 1/4
        
        for i in range(self.n_simulations):
            # éšæœºé€‰æ‹©çª—å£å¤§å°å’Œèµ·å§‹ä½ç½®
            window_size = np.random.randint(min_window, total_days)
            start_idx = np.random.randint(0, total_days - window_size)
            
            sample_dates = dates[start_idx:start_idx + window_size]
            sample_df = pred_df[pred_df["date"].isin(sample_dates)].copy()
            
            if len(sample_df) < 100:
                continue
            
            metrics = self._run_single_backtest(sample_df)
            if metrics:
                metrics["simulation_id"] = i
                metrics["method"] = "time_window"
                metrics["start_date"] = str(sample_dates[0])[:10]
                metrics["end_date"] = str(sample_dates[-1])[:10]
                metrics["window_days"] = window_size
                results.append(metrics)
            
            if (i + 1) % 100 == 0:
                logger.info(f"  æ—¶é—´çª—å£é‡‡æ ·è¿›åº¦: {i + 1}/{self.n_simulations}")
        
        logger.info(f"  æ—¶é—´çª—å£é‡‡æ ·å®Œæˆ: {len(results)} æ¬¡æœ‰æ•ˆæ¨¡æ‹Ÿ")
        return results
    
    # ========================================================================
    # æ±‡æ€»å’Œå¯è§†åŒ–
    # ========================================================================
    def aggregate_results(self, all_results: List[Dict]) -> pd.DataFrame:
        """æ±‡æ€»æ‰€æœ‰æ¨¡æ‹Ÿç»“æœ"""
        if not all_results:
            return pd.DataFrame()
        
        df = pd.DataFrame(all_results)
        # ç§»é™¤ equity_curve åˆ—ï¼ˆå¤ªå¤§ï¼‰
        if "equity_curve" in df.columns:
            df = df.drop(columns=["equity_curve"])
        return df
    
    def compute_statistics(self, results_df: pd.DataFrame) -> Dict:
        """
        è®¡ç®—ç»Ÿè®¡æ±‡æ€»
        
        Returns:
            åŒ…å«å„é¡¹æŒ‡æ ‡ç»Ÿè®¡çš„å­—å…¸
        """
        if results_df.empty:
            return {}
        
        stats = {
            "total_simulations": len(results_df),
            "annual_return": {
                "mean": results_df["annual_return"].mean(),
                "median": results_df["annual_return"].median(),
                "std": results_df["annual_return"].std(),
                "p5": results_df["annual_return"].quantile(0.05),
                "p25": results_df["annual_return"].quantile(0.25),
                "p75": results_df["annual_return"].quantile(0.75),
                "p95": results_df["annual_return"].quantile(0.95),
                "min": results_df["annual_return"].min(),
                "max": results_df["annual_return"].max(),
            },
            "sharpe": {
                "mean": results_df["sharpe"].mean(),
                "median": results_df["sharpe"].median(),
                "std": results_df["sharpe"].std(),
                "p5": results_df["sharpe"].quantile(0.05),
                "p95": results_df["sharpe"].quantile(0.95),
            },
            "max_drawdown": {
                "mean": results_df["max_drawdown"].mean(),
                "median": results_df["max_drawdown"].median(),
                "std": results_df["max_drawdown"].std(),
                "p5": results_df["max_drawdown"].quantile(0.05),
                "p90": results_df["max_drawdown"].quantile(0.90),
                "p95": results_df["max_drawdown"].quantile(0.95),
            }
        }
        
        return stats
    
    def plot_return_distribution(self, results_df: pd.DataFrame, stats: Dict):
        """ç»˜åˆ¶æ”¶ç›Šåˆ†å¸ƒå›¾"""
        if results_df.empty:
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        
        # 1. å¹´åŒ–æ”¶ç›Šåˆ†å¸ƒ
        ax1 = axes[0, 0]
        returns = results_df["annual_return"] * 100  # è½¬ä¸ºç™¾åˆ†æ¯”
        ax1.hist(returns, bins=50, edgecolor='black', alpha=0.7, color='#3498db')
        ax1.axvline(stats["annual_return"]["median"] * 100, color='red', linestyle='--', 
                   linewidth=2, label=f'ä¸­ä½æ•°: {stats["annual_return"]["median"]*100:.1f}%')
        ax1.axvline(stats["annual_return"]["p5"] * 100, color='orange', linestyle=':', 
                   linewidth=2, label=f'5%åˆ†ä½: {stats["annual_return"]["p5"]*100:.1f}%')
        ax1.axvline(stats["annual_return"]["p95"] * 100, color='green', linestyle=':', 
                   linewidth=2, label=f'95%åˆ†ä½: {stats["annual_return"]["p95"]*100:.1f}%')
        ax1.set_xlabel("å¹´åŒ–æ”¶ç›Šç‡ (%)")
        ax1.set_ylabel("é¢‘æ¬¡")
        ax1.set_title("å¹´åŒ–æ”¶ç›Šåˆ†å¸ƒ")
        ax1.legend(loc='upper right')
        ax1.grid(True, alpha=0.3)
        
        # 2. å¤æ™®æ¯”ç‡åˆ†å¸ƒ
        ax2 = axes[0, 1]
        sharpes = results_df["sharpe"]
        ax2.hist(sharpes, bins=50, edgecolor='black', alpha=0.7, color='#2ecc71')
        ax2.axvline(stats["sharpe"]["median"], color='red', linestyle='--', 
                   linewidth=2, label=f'ä¸­ä½æ•°: {stats["sharpe"]["median"]:.2f}')
        ax2.set_xlabel("å¤æ™®æ¯”ç‡")
        ax2.set_ylabel("é¢‘æ¬¡")
        ax2.set_title("å¤æ™®æ¯”ç‡åˆ†å¸ƒ")
        ax2.legend(loc='upper right')
        ax2.grid(True, alpha=0.3)
        
        # 3. æœ€å¤§å›æ’¤åˆ†å¸ƒ
        ax3 = axes[1, 0]
        drawdowns = results_df["max_drawdown"] * 100  # è½¬ä¸ºç™¾åˆ†æ¯”
        ax3.hist(drawdowns, bins=50, edgecolor='black', alpha=0.7, color='#e74c3c')
        ax3.axvline(stats["max_drawdown"]["median"] * 100, color='blue', linestyle='--', 
                   linewidth=2, label=f'ä¸­ä½æ•°: {stats["max_drawdown"]["median"]*100:.1f}%')
        ax3.axvline(stats["max_drawdown"]["p90"] * 100, color='purple', linestyle=':',
                   linewidth=2, label=f'90%åˆ†ä½: {stats["max_drawdown"]["p90"]*100:.1f}%')
        ax3.set_xlabel("æœ€å¤§å›æ’¤ (%)")
        ax3.set_ylabel("é¢‘æ¬¡")
        ax3.set_title("æœ€å¤§å›æ’¤åˆ†å¸ƒ")
        ax3.legend(loc='upper right')
        ax3.grid(True, alpha=0.3)
        
        # 4. æ”¶ç›Š-é£é™©æ•£ç‚¹å›¾
        ax4 = axes[1, 1]
        scatter = ax4.scatter(
            results_df["max_drawdown"] * 100, 
            results_df["annual_return"] * 100,
            c=results_df["sharpe"], 
            cmap='RdYlGn', 
            alpha=0.6,
            s=30
        )
        ax4.set_xlabel("æœ€å¤§å›æ’¤ (%)")
        ax4.set_ylabel("å¹´åŒ–æ”¶ç›Šç‡ (%)")
        ax4.set_title("æ”¶ç›Š-é£é™©æ•£ç‚¹å›¾ (é¢œè‰²=å¤æ™®æ¯”ç‡)")
        plt.colorbar(scatter, ax=ax4, label="å¤æ™®æ¯”ç‡")
        ax4.grid(True, alpha=0.3)
        
        plt.suptitle("è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿç»“æœåˆ†æ", fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        chart_path = os.path.join(self.report_dir, "monte_carlo_distribution.png")
        plt.savefig(chart_path, dpi=150)
        plt.close()
        logger.info(f"åˆ†å¸ƒå›¾å·²ä¿å­˜: {chart_path}")

    def plot_drawdown_distribution(self, results_df: pd.DataFrame, stats: Dict):
        """è¾“å‡ºå›æ’¤åˆ†å¸ƒå›¾ä¸ç»Ÿè®¡"""
        if results_df.empty:
            return

        drawdown_abs = (-results_df["max_drawdown"]).clip(lower=0) * 100

        fig, ax = plt.subplots(figsize=(8, 5))
        ax.hist(drawdown_abs, bins=40, edgecolor='black', alpha=0.75, color='#c0392b')
        ax.axvline((-stats["max_drawdown"]["median"]) * 100, color='blue', linestyle='--',
                   linewidth=2, label=f'ä¸­ä½æ•°: {-stats["max_drawdown"]["median"]*100:.1f}%')
        ax.axvline((-stats["max_drawdown"]["p90"]) * 100, color='purple', linestyle=':',
                   linewidth=2, label=f'90%åˆ†ä½: {-stats["max_drawdown"]["p90"]*100:.1f}%')
        ax.axvline((-stats["max_drawdown"]["p95"]) * 100, color='orange', linestyle=':',
                   linewidth=2, label=f'95%åˆ†ä½: {-stats["max_drawdown"]["p95"]*100:.1f}%')
        ax.set_xlabel("æœ€å¤§å›æ’¤å¹…åº¦(%)")
        ax.set_ylabel("é¢‘æ¬¡")
        ax.set_title("æœ€å¤§å›æ’¤åˆ†å¸ƒ(æ­£å€¼)")
        ax.legend(loc='upper right')
        ax.grid(True, alpha=0.3)

        chart_path = os.path.join(self.report_dir, "drawdown_distribution.png")
        plt.tight_layout()
        plt.savefig(chart_path, dpi=150)
        plt.close()
        logger.info(f"å›æ’¤åˆ†å¸ƒå›¾å·²ä¿å­˜: {chart_path}")

        dd_stats = {
            "count": int(len(drawdown_abs)),
            "mean": float(drawdown_abs.mean()),
            "median": float(drawdown_abs.median()),
            "std": float(drawdown_abs.std()),
            "p5": float(drawdown_abs.quantile(0.05)),
            "p25": float(drawdown_abs.quantile(0.25)),
            "p75": float(drawdown_abs.quantile(0.75)),
            "p90": float(drawdown_abs.quantile(0.90)),
            "p95": float(drawdown_abs.quantile(0.95)),
            "min": float(drawdown_abs.min()),
            "max": float(drawdown_abs.max()),
        }
        dd_path = os.path.join(self.report_dir, "drawdown_stats.csv")
        pd.DataFrame([dd_stats]).to_csv(dd_path, index=False, encoding="utf-8-sig")
        logger.info(f"å›æ’¤ç»Ÿè®¡å·²ä¿å­˜: {dd_path}")

    def analyze_time_clustered_drawdown(
        self,
        results_df: pd.DataFrame,
        extreme_quantile: float = 0.90,
    ):
        """
        åŸºäº time_window æ¨¡æ‹Ÿç»“æœï¼Œåˆ†æå›æ’¤çš„æ—¶é—´èšé›†æ€§ï¼Œå¹¶å‰”é™¤æç«¯è¡Œæƒ…åçš„â€œæ—¥å¸¸å›æ’¤â€ã€‚
        extreme_quantile: ç”¨äºå®šä¹‰æç«¯å›æ’¤é˜ˆå€¼ï¼ˆå¦‚ 0.90=å‰”é™¤æœ€å·®10%ï¼‰
        """
        time_df = results_df[results_df["method"] == "time_window"].copy()
        if time_df.empty:
            logger.warning("time_window ç»“æœä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œå›æ’¤æ—¶é—´èšé›†åˆ†æã€‚")
            return

        # åªä¿ç•™æœ‰æ—¥æœŸçš„æ ·æœ¬
        time_df = time_df.dropna(subset=["start_date", "end_date"])
        if time_df.empty:
            logger.warning("time_window ç»“æœç¼ºå°‘æ—¥æœŸï¼Œæ— æ³•è¿›è¡Œå›æ’¤æ—¶é—´èšé›†åˆ†æã€‚")
            return

        # å›æ’¤å¹…åº¦ï¼ˆæ­£æ•°ï¼‰
        time_df["drawdown_abs"] = (-time_df["max_drawdown"]).clip(lower=0) * 100
        time_df["start_year"] = pd.to_datetime(time_df["start_date"]).dt.year

        # 1) æŒ‰å¹´ä»½ç»Ÿè®¡å›æ’¤åˆ†å¸ƒ
        year_stats = (
            time_df.groupby("start_year")["drawdown_abs"]
            .agg(["count", "mean", "median", "std", "min", "max"])
            .reset_index()
        )
        year_path = os.path.join(self.report_dir, "drawdown_by_year.csv")
        year_stats.to_csv(year_path, index=False, encoding="utf-8-sig")
        logger.info(f"æŒ‰å¹´ä»½å›æ’¤ç»Ÿè®¡å·²ä¿å­˜: {year_path}")

        # å¹´ä»½ç®±çº¿å›¾ï¼ˆå±•ç¤ºå›æ’¤æ—¶é—´èšé›†æ€§ï¼‰
        fig, ax = plt.subplots(figsize=(10, 5))
        years = sorted(time_df["start_year"].unique())
        data = [time_df[time_df["start_year"] == y]["drawdown_abs"].values for y in years]
        ax.boxplot(data, labels=years, showfliers=False)
        ax.set_xlabel("èµ·å§‹å¹´ä»½")
        ax.set_ylabel("æœ€å¤§å›æ’¤å¹…åº¦(%)")
        ax.set_title("å›æ’¤åœ¨æ—¶é—´ä¸Šçš„èšé›†æ€§(æŒ‰å¹´ä»½)")
        ax.grid(True, alpha=0.3)
        plt.tight_layout()
        year_plot = os.path.join(self.report_dir, "drawdown_by_year.png")
        plt.savefig(year_plot, dpi=150)
        plt.close()
        logger.info(f"æŒ‰å¹´ä»½å›æ’¤åˆ†å¸ƒå›¾å·²ä¿å­˜: {year_plot}")

        # 2) æ’é™¤æç«¯è¡Œæƒ…ï¼šå‰”é™¤æœ€å·® X% å›æ’¤æ ·æœ¬
        threshold = time_df["drawdown_abs"].quantile(extreme_quantile)
        normal_df = time_df[time_df["drawdown_abs"] <= threshold].copy()

        normal_stats = {
            "count": int(len(normal_df)),
            "extreme_quantile": float(extreme_quantile),
            "extreme_threshold": float(threshold),
            "mean": float(normal_df["drawdown_abs"].mean()),
            "median": float(normal_df["drawdown_abs"].median()),
            "std": float(normal_df["drawdown_abs"].std()),
            "p5": float(normal_df["drawdown_abs"].quantile(0.05)),
            "p25": float(normal_df["drawdown_abs"].quantile(0.25)),
            "p75": float(normal_df["drawdown_abs"].quantile(0.75)),
            "p90": float(normal_df["drawdown_abs"].quantile(0.90)),
            "p95": float(normal_df["drawdown_abs"].quantile(0.95)),
            "min": float(normal_df["drawdown_abs"].min()),
            "max": float(normal_df["drawdown_abs"].max()),
        }
        normal_path = os.path.join(self.report_dir, "drawdown_normal_excluding_extremes.csv")
        pd.DataFrame([normal_stats]).to_csv(normal_path, index=False, encoding="utf-8-sig")
        logger.info(f"å‰”é™¤æç«¯è¡Œæƒ…åçš„å›æ’¤ç»Ÿè®¡å·²ä¿å­˜: {normal_path}")

        # è¾“å‡ºè¢«å‰”é™¤çš„æç«¯çª—å£åˆ—è¡¨ï¼ˆä¾¿äºå¤ç›˜ï¼‰
        extreme_df = time_df[time_df["drawdown_abs"] > threshold].copy()
        extreme_df = extreme_df.sort_values("drawdown_abs", ascending=False)
        extreme_out = os.path.join(self.report_dir, "drawdown_extreme_windows.csv")
        extreme_df.to_csv(extreme_out, index=False, encoding="utf-8-sig")
        logger.info(f"æç«¯å›æ’¤çª—å£å·²ä¿å­˜: {extreme_out}")
    
    def plot_noise_sensitivity(self, results_df: pd.DataFrame):
        """ç»˜åˆ¶å™ªéŸ³æ•æ„Ÿæ€§å›¾"""
        noise_results = results_df[results_df["method"] == "noise_injection"]
        if noise_results.empty:
            return
        
        # æŒ‰å™ªéŸ³æ¯”ä¾‹åˆ†ç»„
        grouped = noise_results.groupby("noise_ratio").agg({
            "annual_return": ["mean", "std"],
            "sharpe": ["mean", "std"]
        }).reset_index()
        
        fig, axes = plt.subplots(1, 2, figsize=(12, 5))
        
        # 1. å¹´åŒ–æ”¶ç›Š vs å™ªéŸ³æ¯”ä¾‹
        ax1 = axes[0]
        noise_levels = grouped["noise_ratio"] * 100
        returns_mean = grouped[("annual_return", "mean")] * 100
        returns_std = grouped[("annual_return", "std")] * 100
        
        ax1.plot(noise_levels, returns_mean, 'b-o', linewidth=2, markersize=6, label='å‡å€¼')
        ax1.fill_between(noise_levels, returns_mean - returns_std, returns_mean + returns_std, 
                        alpha=0.3, color='blue')
        ax1.set_xlabel("å™ªéŸ³æ¯”ä¾‹ (%)")
        ax1.set_ylabel("å¹´åŒ–æ”¶ç›Šç‡ (%)")
        ax1.set_title("å¹´åŒ–æ”¶ç›Š vs å™ªéŸ³å¼ºåº¦")
        ax1.grid(True, alpha=0.3)
        ax1.legend()
        
        # 2. å¤æ™®æ¯”ç‡ vs å™ªéŸ³æ¯”ä¾‹
        ax2 = axes[1]
        sharpe_mean = grouped[("sharpe", "mean")]
        sharpe_std = grouped[("sharpe", "std")]
        
        ax2.plot(noise_levels, sharpe_mean, 'g-o', linewidth=2, markersize=6, label='å‡å€¼')
        ax2.fill_between(noise_levels, sharpe_mean - sharpe_std, sharpe_mean + sharpe_std, 
                        alpha=0.3, color='green')
        ax2.set_xlabel("å™ªéŸ³æ¯”ä¾‹ (%)")
        ax2.set_ylabel("å¤æ™®æ¯”ç‡")
        ax2.set_title("å¤æ™®æ¯”ç‡ vs å™ªéŸ³å¼ºåº¦")
        ax2.grid(True, alpha=0.3)
        ax2.legend()
        
        plt.suptitle("å™ªéŸ³æ•æ„Ÿæ€§åˆ†æ", fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        chart_path = os.path.join(self.report_dir, "noise_sensitivity.png")
        plt.savefig(chart_path, dpi=150)
        plt.close()
        logger.info(f"å™ªéŸ³æ•æ„Ÿæ€§å›¾å·²ä¿å­˜: {chart_path}")
    
    def plot_weight_sensitivity(self, results_df: pd.DataFrame):
        """ç»˜åˆ¶æƒé‡æ•æ„Ÿæ€§å›¾"""
        weight_results = results_df[results_df["method"] == "weight_perturbation"]
        if weight_results.empty or "return_weight" not in weight_results.columns:
            return
        
        fig, ax = plt.subplots(figsize=(10, 6))
        
        scatter = ax.scatter(
            weight_results["return_weight"], 
            weight_results["annual_return"] * 100,
            c=weight_results["sharpe"], 
            cmap='RdYlGn', 
            alpha=0.6,
            s=50
        )
        
        # æ ‡è®°æœ€ä½³ç‚¹
        best_idx = weight_results["sharpe"].idxmax()
        best_row = weight_results.loc[best_idx]
        ax.scatter(best_row["return_weight"], best_row["annual_return"] * 100, 
                  s=200, c='red', marker='*', edgecolors='black', linewidths=2,
                  label=f'æœ€ä½³: w_return={best_row["return_weight"]:.2f}, å¤æ™®={best_row["sharpe"]:.2f}')
        
        # æ ‡è®°åŸå§‹æƒé‡
        ax.axvline(self.base_return_weight, color='orange', linestyle='--', 
                  linewidth=2, label=f'é…ç½®æƒé‡: w_return={self.base_return_weight:.2f}')
        
        ax.set_xlabel("Return weight (w_return)")
        ax.set_ylabel("å¹´åŒ–æ”¶ç›Šç‡ (%)")
        ax.set_title("èåˆæƒé‡æ•æ„Ÿæ€§åˆ†æ")
        plt.colorbar(scatter, ax=ax, label="å¤æ™®æ¯”ç‡")
        ax.legend(loc='upper right')
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        chart_path = os.path.join(self.report_dir, "weight_sensitivity.png")
        plt.savefig(chart_path, dpi=150)
        plt.close()
        logger.info(f"æƒé‡æ•æ„Ÿæ€§å›¾å·²ä¿å­˜: {chart_path}")
    
    def generate_report(self, results_df: pd.DataFrame, stats: Dict):
        """ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š"""
        if results_df.empty:
            return
        
        report_lines = []
        report_lines.append("=" * 60)
        report_lines.append("ğŸ“Š è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿåˆ†ææŠ¥å‘Š (Monte Carlo Simulation Report)")
        report_lines.append("=" * 60)
        report_lines.append("")
        
        report_lines.append(f"æ¨¡æ‹Ÿæ€»æ¬¡æ•°: {stats['total_simulations']}")
        report_lines.append("")
        
        report_lines.append("-" * 60)
        report_lines.append("ã€å¹´åŒ–æ”¶ç›Šç‡ç»Ÿè®¡ã€‘")
        report_lines.append("-" * 60)
        ar = stats["annual_return"]
        report_lines.append(f"  å‡å€¼:     {ar['mean']*100:>8.2f}%")
        report_lines.append(f"  ä¸­ä½æ•°:   {ar['median']*100:>8.2f}%")
        report_lines.append(f"  æ ‡å‡†å·®:   {ar['std']*100:>8.2f}%")
        report_lines.append(f"  5%åˆ†ä½:   {ar['p5']*100:>8.2f}%")
        report_lines.append(f"  25%åˆ†ä½:  {ar['p25']*100:>8.2f}%")
        report_lines.append(f"  75%åˆ†ä½:  {ar['p75']*100:>8.2f}%")
        report_lines.append(f"  95%åˆ†ä½:  {ar['p95']*100:>8.2f}%")
        report_lines.append(f"  æœ€å°å€¼:   {ar['min']*100:>8.2f}%")
        report_lines.append(f"  æœ€å¤§å€¼:   {ar['max']*100:>8.2f}%")
        report_lines.append("")
        
        report_lines.append("-" * 60)
        report_lines.append("ã€å¤æ™®æ¯”ç‡ç»Ÿè®¡ã€‘")
        report_lines.append("-" * 60)
        sr = stats["sharpe"]
        report_lines.append(f"  å‡å€¼:     {sr['mean']:>8.2f}")
        report_lines.append(f"  ä¸­ä½æ•°:   {sr['median']:>8.2f}")
        report_lines.append(f"  æ ‡å‡†å·®:   {sr['std']:>8.2f}")
        report_lines.append(f"  5%åˆ†ä½:   {sr['p5']:>8.2f}")
        report_lines.append(f"  95%åˆ†ä½:  {sr['p95']:>8.2f}")
        report_lines.append("")
        
        report_lines.append("-" * 60)
        report_lines.append("ã€æœ€å¤§å›æ’¤ç»Ÿè®¡ã€‘")
        report_lines.append("-" * 60)
        md = stats["max_drawdown"]
        report_lines.append(f"  å‡å€¼:     {md['mean']*100:>8.2f}%")
        report_lines.append(f"  ä¸­ä½æ•°:   {md['median']*100:>8.2f}%")
        report_lines.append(f"  æ ‡å‡†å·®:   {md['std']*100:>8.2f}%")
        report_lines.append(f"  5%åˆ†ä½:   {md['p5']*100:>8.2f}%")
        report_lines.append(f"  90%??:  {md['p90']*100:>8.2f}%")
        report_lines.append(f"  95%åˆ†ä½:  {md['p95']*100:>8.2f}%")
        report_lines.append("")
        
        report_lines.append("-" * 60)
        report_lines.append("ã€ç½®ä¿¡åŒºé—´è§£è¯»ã€‘")
        report_lines.append("-" * 60)
        report_lines.append(f"  â€¢ 90% ç½®ä¿¡åŒºé—´ä¸‹ï¼Œå¹´åŒ–æ”¶ç›Šé¢„æœŸåœ¨ {ar['p5']*100:.1f}% ~ {ar['p95']*100:.1f}% ä¹‹é—´")
        report_lines.append(f"  â€¢ 90% ç½®ä¿¡åŒºé—´ä¸‹ï¼Œå¤æ™®æ¯”ç‡é¢„æœŸåœ¨ {sr['p5']:.2f} ~ {sr['p95']:.2f} ä¹‹é—´")
        report_lines.append(f"  â€¢ 90% ç½®ä¿¡åŒºé—´ä¸‹ï¼Œæœ€å¤§å›æ’¤é¢„æœŸåœ¨ {md['p5']*100:.1f}% ~ {md['p95']*100:.1f}% ä¹‹é—´")
        
        # ç¨³å¥æ€§è¯„ä¼°
        report_lines.append("")
        report_lines.append("-" * 60)
        report_lines.append("ã€ç¨³å¥æ€§è¯„ä¼°ã€‘")
        report_lines.append("-" * 60)
        
        # æ”¶ç›Šæ³¢åŠ¨ç³»æ•°
        cv = abs(ar['std'] / ar['mean']) if ar['mean'] != 0 else float('inf')
        if cv < 0.3:
            stability = "âœ… é«˜åº¦ç¨³å¥ (å˜å¼‚ç³»æ•° < 0.3)"
        elif cv < 0.6:
            stability = "âš ï¸ ä¸­ç­‰ç¨³å¥ (å˜å¼‚ç³»æ•° 0.3 ~ 0.6)"
        else:
            stability = "âŒ æ³¢åŠ¨è¾ƒå¤§ (å˜å¼‚ç³»æ•° > 0.6)"
        report_lines.append(f"  æ”¶ç›Šç¨³å®šæ€§: {stability}")
        report_lines.append(f"  å˜å¼‚ç³»æ•°: {cv:.2f}")
        
        # æœ€å·®æƒ…å†µåˆ†æ
        if ar['p5'] > 0:
            report_lines.append(f"  æœ€å·® 5% æƒ…å†µä»ç›ˆåˆ©: âœ… æ˜¯ ({ar['p5']*100:.1f}%)")
        else:
            report_lines.append(f"  æœ€å·® 5% æƒ…å†µä»ç›ˆåˆ©: âŒ å¦ ({ar['p5']*100:.1f}%)")
        
        report_lines.append("=" * 60)
        
        # æ‰“å°æŠ¥å‘Š
        report_text = "\n".join(report_lines)
        print("\n" + report_text)
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = os.path.join(self.report_dir, "monte_carlo_report.txt")
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(report_text)
        logger.info(f"æŠ¥å‘Šå·²ä¿å­˜: {report_path}")


# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================

def main():
    logger.info("=" * 60)
    logger.info("=== è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿåˆ†æ (Monte Carlo Simulation) ===")
    logger.info("=" * 60)
    
    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = MonteCarloAnalyzer(n_simulations = 50, random_seed=42)
    
    # 1. åŠ è½½é¢„æµ‹æ•°æ®
    pred_df = analyzer.load_predictions()
    if pred_df is None:
        logger.error("æœªæ‰¾åˆ°é¢„æµ‹æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ run_walkforward.py")
        return
    
    logger.info(f"é¢„æµ‹æ•°æ®: {len(pred_df)} è¡Œ, æ—¥æœŸèŒƒå›´: {pred_df['date'].min()} ~ {pred_df['date'].max()}")
    
    # æ£€æŸ¥åŒå¤´æ¨¡å‹åˆ—
    dual_cols = analyzer._get_dual_head_cols(pred_df)
    has_dual_head = dual_cols is not None
    logger.info(f"åŒå¤´æ¨¡å‹é¢„æµ‹åˆ—: {'å­˜åœ¨' if has_dual_head else 'ä¸å­˜åœ¨'}")
    
    # 2. æ‰§è¡Œå„ç§æ¨¡æ‹Ÿ
    all_results = []
    
    # 2.1 Bootstrap é‡é‡‡æ ·
    bootstrap_results = analyzer.run_bootstrap_simulation(pred_df)
    all_results.extend(bootstrap_results)
    
    # 2.2 æƒé‡æ‰°åŠ¨ï¼ˆä»…åŒå¤´æ¨¡å‹ï¼‰
    if has_dual_head:
        weight_results = analyzer.run_weight_perturbation(pred_df)
        all_results.extend(weight_results)
    
    # 2.3 å™ªéŸ³æ³¨å…¥
    noise_results = analyzer.run_noise_injection(pred_df)
    all_results.extend(noise_results)
    
    # 2.4 æ—¶é—´çª—å£é‡‡æ ·
    time_results = analyzer.run_time_window_sampling(pred_df)
    all_results.extend(time_results)
    
    # 3. æ±‡æ€»ç»“æœ
    results_df = analyzer.aggregate_results(all_results)
    
    if results_df.empty:
        logger.error("æ‰€æœ‰æ¨¡æ‹Ÿå‡å¤±è´¥ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š")
        return
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    results_path = os.path.join(analyzer.report_dir, "monte_carlo_results.csv")
    results_df.to_csv(results_path, index=False, encoding="utf-8-sig")
    logger.info(f"è¯¦ç»†ç»“æœå·²ä¿å­˜: {results_path}")
    
    # 4. è®¡ç®—ç»Ÿè®¡æ±‡æ€»
    stats = analyzer.compute_statistics(results_df)
    
    # 5. ç”Ÿæˆå¯è§†åŒ–
    analyzer.plot_return_distribution(results_df, stats)
    analyzer.plot_drawdown_distribution(results_df, stats)
    analyzer.analyze_time_clustered_drawdown(results_df, extreme_quantile=0.90)
    analyzer.plot_noise_sensitivity(results_df)
    if has_dual_head:
        analyzer.plot_weight_sensitivity(results_df)
    
    # 6. ç”ŸæˆæŠ¥å‘Š
    analyzer.generate_report(results_df, stats)
    
    logger.info(f"\nè’™ç‰¹å¡æ´›åˆ†æå®Œæˆï¼æŠ¥å‘Šå·²ä¿å­˜è‡³: {analyzer.report_dir}")


if __name__ == "__main__":
    main()
