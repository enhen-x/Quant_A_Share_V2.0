# scripts/analisis/check_monte_carlo.py
# ============================================================================
# è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿåˆ†æ (Monte Carlo Simulation Analysis)
# ============================================================================
#
# ã€åŠŸèƒ½ã€‘
# å¯¹åŒå¤´æ¨¡å‹çš„é¢„æµ‹ç»“æœè¿›è¡Œè’™ç‰¹å¡æ´›æ¨¡æ‹Ÿï¼Œè¯„ä¼°ç­–ç•¥çš„ç¨³å¥æ€§å’Œç½®ä¿¡åŒºé—´ã€‚
# åŒ…å«4ç§æ¨¡æ‹Ÿæ–¹æ³•ï¼š
#   1. Bootstrap é‡é‡‡æ · - è¯„ä¼°æ”¶ç›Šåˆ†å¸ƒç½®ä¿¡åŒºé—´
#   2. æƒé‡æ‰°åŠ¨ - è¯„ä¼°èåˆæƒé‡æ•æ„Ÿæ€§
#   3. å™ªéŸ³æ³¨å…¥ - è¯„ä¼°æ¨¡å‹æŠ—å¹²æ‰°èƒ½åŠ›
#   4. æ—¶é—´çª—å£é‡‡æ · - è¯„ä¼°ç­–ç•¥æ—¶é—´ç¨³å®šæ€§
#
# ã€ä½¿ç”¨æ–¹æ³•ã€‘
# python scripts/analisis/check_monte_carlo.py
# ============================================================================

import os
import sys
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings("ignore")

# Matplotlib å­—ä½“é…ç½®ï¼ˆå¿…é¡»åœ¨ import pyplot ä¹‹å‰è®¾ç½®ï¼‰
import matplotlib
matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS', 'sans-serif']
matplotlib.rcParams['axes.unicode_minus'] = False

import matplotlib.pyplot as plt
from matplotlib.ticker import ScalarFormatter

# è·¯å¾„é€‚é…
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from src.utils.config import GLOBAL_CONFIG
from src.utils.logger import get_logger
from src.utils.io import read_parquet, ensure_dir
from src.strategy.signal import TopKSignalStrategy
from src.backtest.backtester import VectorBacktester

logger = get_logger()


# ============================================================================
# è’™ç‰¹å¡æ´›åˆ†æå™¨
# ============================================================================

class MonteCarloAnalyzer:
    """
    è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿåˆ†æå™¨
    
    å¯¹åŒå¤´æ¨¡å‹é¢„æµ‹ç»“æœè¿›è¡Œå¤šç§æ¨¡æ‹Ÿåˆ†æï¼Œè¯„ä¼°ç­–ç•¥ç¨³å¥æ€§ã€‚
    """
    
    def __init__(self, n_simulations: int = 500, random_seed: int = 42):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            n_simulations: æ¨¡æ‹Ÿæ¬¡æ•°
            random_seed: éšæœºç§å­ï¼ˆä¿è¯å¯å¤ç°ï¼‰
        """
        self.n_simulations = n_simulations
        self.random_seed = random_seed
        np.random.seed(random_seed)
        
        self.config = GLOBAL_CONFIG
        self.dual_head_cfg = self.config["model"].get("dual_head", {})
        
        # è·å–åŸå§‹èåˆæƒé‡
        self.base_reg_weight = self.dual_head_cfg.get("regression", {}).get("weight", 0.6)
        self.base_cls_weight = self.dual_head_cfg.get("classification", {}).get("weight", 0.4)
        
        # å›æµ‹å™¨å’Œç­–ç•¥
        self.backtester = VectorBacktester()
        self.strategy = TopKSignalStrategy()
        
        # å¦‚æœæœªå¼€å¯ä»“ä½ç®¡ç†ï¼Œå¼ºåˆ¶æ»¡ä»“æµ‹è¯•
        if not self.config["strategy"].get("position_control", {}).get("enable", False):
            self.strategy.min_score = -999.0
        
        # è¾“å‡ºç›®å½•
        self.report_dir = os.path.join(self.config["paths"]["reports"], "monte_carlo")
        ensure_dir(self.report_dir)
    
    def load_predictions(self) -> Optional[pd.DataFrame]:
        """åŠ è½½æœ€æ–°çš„é¢„æµ‹æ–‡ä»¶"""
        models_dir = self.config["paths"]["models"]
        if not os.path.exists(models_dir):
            return None
        
        subdirs = [d for d in os.listdir(models_dir) if os.path.isdir(os.path.join(models_dir, d))]
        if not subdirs:
            return None
        
        subdirs.sort(reverse=True)
        latest_dir = subdirs[0]
        pred_path = os.path.join(models_dir, latest_dir, "predictions.parquet")
        
        if os.path.exists(pred_path):
            logger.info(f"ä½¿ç”¨é¢„æµ‹æ–‡ä»¶: {pred_path}")
            df = read_parquet(pred_path)
            df["date"] = pd.to_datetime(df["date"])
            return df
        return None
    
    def _run_single_backtest(self, pred_df: pd.DataFrame, silent: bool = True) -> Optional[Dict]:
        """
        æ‰§è¡Œå•æ¬¡å›æµ‹
        
        Returns:
            åŒ…å«ç»©æ•ˆæŒ‡æ ‡çš„å­—å…¸ï¼Œå¤±è´¥è¿”å› None
        """
        try:
            signal_df = self.strategy.generate(pred_df)
            if signal_df.empty:
                return None
            
            # ä½¿ç”¨ä¸´æ—¶ç›®å½•é¿å…è¦†ç›–
            temp_dir = os.path.join(self.report_dir, "_temp")
            metrics = self.backtester.run(signal_df, output_dir=temp_dir)
            
            return {
                "annual_return": metrics["annual_return"],
                "sharpe": metrics["sharpe"],
                "max_drawdown": metrics["max_drawdown"],
                "total_return": metrics.get("total_return", 0),
                "volatility": metrics.get("volatility", 0),
                "equity_curve": metrics.get("equity_curve")
            }
        except Exception as e:
            if not silent:
                logger.warning(f"å›æµ‹å¤±è´¥: {e}")
            return None
    
    # ========================================================================
    # æ¨¡æ‹Ÿæ–¹æ³• 1: Bootstrap é‡é‡‡æ ·
    # ========================================================================
    def run_bootstrap_simulation(self, pred_df: pd.DataFrame) -> List[Dict]:
        """
        Bootstrap é‡é‡‡æ ·æ¨¡æ‹Ÿ
        
        å¯¹æ¯æ—¥çš„è‚¡ç¥¨ä¿¡å·è¿›è¡Œæœ‰æ”¾å›æŠ½æ ·ï¼Œé‡æ–°è®¡ç®—æ”¶ç›Šã€‚
        
        Returns:
            æ¨¡æ‹Ÿç»“æœåˆ—è¡¨
        """
        logger.info(f">>> æ‰§è¡Œ Bootstrap é‡é‡‡æ ·æ¨¡æ‹Ÿ ({self.n_simulations} æ¬¡)...")
        results = []
        
        # æŒ‰æ—¥æœŸåˆ†ç»„
        dates = pred_df["date"].unique()
        
        for i in range(self.n_simulations):
            # éšæœºæŠ½æ · 80% çš„æ—¥æœŸï¼ˆæœ‰æ”¾å›ï¼‰
            sample_dates = np.random.choice(dates, size=int(len(dates) * 0.8), replace=True)
            sample_df = pred_df[pred_df["date"].isin(sample_dates)].copy()
            
            if len(sample_df) < 100:
                continue
            
            metrics = self._run_single_backtest(sample_df)
            if metrics:
                metrics["simulation_id"] = i
                metrics["method"] = "bootstrap"
                results.append(metrics)
            
            if (i + 1) % 100 == 0:
                logger.info(f"  Bootstrap è¿›åº¦: {i + 1}/{self.n_simulations}")
        
        logger.info(f"  Bootstrap å®Œæˆ: {len(results)} æ¬¡æœ‰æ•ˆæ¨¡æ‹Ÿ")
        return results
    
    # ========================================================================
    # æ¨¡æ‹Ÿæ–¹æ³• 2: æƒé‡æ‰°åŠ¨
    # ========================================================================
    def run_weight_perturbation(self, pred_df: pd.DataFrame) -> List[Dict]:
        """
        æƒé‡æ‰°åŠ¨æ¨¡æ‹Ÿ
        
        éšæœºæ‰°åŠ¨å›å½’/åˆ†ç±»èåˆæƒé‡ï¼Œè¯„ä¼°æƒé‡æ•æ„Ÿæ€§ã€‚
        
        Returns:
            æ¨¡æ‹Ÿç»“æœåˆ—è¡¨
        """
        logger.info(f">>> æ‰§è¡Œæƒé‡æ‰°åŠ¨æ¨¡æ‹Ÿ ({self.n_simulations} æ¬¡)...")
        results = []
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ pred_reg å’Œ pred_cls åˆ—
        has_dual_head = "pred_reg" in pred_df.columns and "pred_cls" in pred_df.columns
        
        if not has_dual_head:
            logger.warning("  é¢„æµ‹æ–‡ä»¶ä¸åŒ…å«åŒå¤´æ¨¡å‹è¾“å‡º (pred_reg, pred_cls)ï¼Œè·³è¿‡æƒé‡æ‰°åŠ¨æ¨¡æ‹Ÿ")
            return results
        
        for i in range(self.n_simulations):
            # éšæœºç”Ÿæˆæƒé‡ (ä¿è¯å’Œä¸º1)
            reg_weight = np.random.uniform(0.2, 0.8)
            cls_weight = 1.0 - reg_weight
            
            # é‡æ–°è®¡ç®—èåˆåˆ†æ•°
            perturbed_df = pred_df.copy()
            
            # å½’ä¸€åŒ–
            def min_max_normalize(arr):
                arr = np.array(arr)
                min_val, max_val = arr.min(), arr.max()
                if max_val - min_val < 1e-9:
                    return np.zeros_like(arr)
                return (arr - min_val) / (max_val - min_val)
            
            pred_reg_norm = min_max_normalize(perturbed_df["pred_reg"].values)
            pred_cls_norm = min_max_normalize(perturbed_df["pred_cls"].values)
            
            perturbed_df["pred_score"] = reg_weight * pred_reg_norm + cls_weight * pred_cls_norm
            
            metrics = self._run_single_backtest(perturbed_df)
            if metrics:
                metrics["simulation_id"] = i
                metrics["method"] = "weight_perturbation"
                metrics["reg_weight"] = reg_weight
                metrics["cls_weight"] = cls_weight
                results.append(metrics)
            
            if (i + 1) % 100 == 0:
                logger.info(f"  æƒé‡æ‰°åŠ¨è¿›åº¦: {i + 1}/{self.n_simulations}")
        
        logger.info(f"  æƒé‡æ‰°åŠ¨å®Œæˆ: {len(results)} æ¬¡æœ‰æ•ˆæ¨¡æ‹Ÿ")
        return results
    
    # ========================================================================
    # æ¨¡æ‹Ÿæ–¹æ³• 3: å™ªéŸ³æ³¨å…¥
    # ========================================================================
    def run_noise_injection(self, pred_df: pd.DataFrame) -> List[Dict]:
        """
        å™ªéŸ³æ³¨å…¥æ¨¡æ‹Ÿ
        
        å‘é¢„æµ‹åˆ†æ•°æ·»åŠ éšæœºå™ªéŸ³ï¼Œè¯„ä¼°æ¨¡å‹æŠ—å¹²æ‰°èƒ½åŠ›ã€‚
        
        Returns:
            æ¨¡æ‹Ÿç»“æœåˆ—è¡¨
        """
        logger.info(f">>> æ‰§è¡Œå™ªéŸ³æ³¨å…¥æ¨¡æ‹Ÿ ({self.n_simulations} æ¬¡)...")
        results = []
        
        # å™ªéŸ³æ¯”ä¾‹èŒƒå›´
        noise_levels = np.linspace(0.0, 0.3, 20)  # 0% ~ 30%
        repeats_per_level = max(1, self.n_simulations // len(noise_levels))
        
        for noise_ratio in noise_levels:
            for j in range(repeats_per_level):
                noisy_df = pred_df.copy()
                
                # æ·»åŠ å™ªéŸ³
                noise = noisy_df["pred_score"].std() * noise_ratio * np.random.randn(len(noisy_df))
                noisy_df["pred_score"] = noisy_df["pred_score"] + noise
                
                metrics = self._run_single_backtest(noisy_df)
                if metrics:
                    metrics["simulation_id"] = len(results)
                    metrics["method"] = "noise_injection"
                    metrics["noise_ratio"] = noise_ratio
                    results.append(metrics)
        
        logger.info(f"  å™ªéŸ³æ³¨å…¥å®Œæˆ: {len(results)} æ¬¡æœ‰æ•ˆæ¨¡æ‹Ÿ")
        return results
    
    # ========================================================================
    # æ¨¡æ‹Ÿæ–¹æ³• 4: æ—¶é—´çª—å£é‡‡æ ·
    # ========================================================================
    def run_time_window_sampling(self, pred_df: pd.DataFrame) -> List[Dict]:
        """
        æ—¶é—´çª—å£é‡‡æ ·æ¨¡æ‹Ÿ
        
        éšæœºé‡‡æ ·ä¸åŒæ—¶é—´åŒºé—´è¿›è¡Œå›æµ‹ï¼Œè¯„ä¼°ç­–ç•¥æ—¶é—´ç¨³å®šæ€§ã€‚
        
        Returns:
            æ¨¡æ‹Ÿç»“æœåˆ—è¡¨
        """
        logger.info(f">>> æ‰§è¡Œæ—¶é—´çª—å£é‡‡æ ·æ¨¡æ‹Ÿ ({self.n_simulations} æ¬¡)...")
        results = []
        
        dates = sorted(pred_df["date"].unique())
        total_days = len(dates)
        min_window = max(60, total_days // 4)  # æœ€å°‘ 60 å¤©æˆ–æ€»å¤©æ•°çš„ 1/4
        
        for i in range(self.n_simulations):
            # éšæœºé€‰æ‹©çª—å£å¤§å°å’Œèµ·å§‹ä½ç½®
            window_size = np.random.randint(min_window, total_days)
            start_idx = np.random.randint(0, total_days - window_size)
            
            sample_dates = dates[start_idx:start_idx + window_size]
            sample_df = pred_df[pred_df["date"].isin(sample_dates)].copy()
            
            if len(sample_df) < 100:
                continue
            
            metrics = self._run_single_backtest(sample_df)
            if metrics:
                metrics["simulation_id"] = i
                metrics["method"] = "time_window"
                metrics["start_date"] = str(sample_dates[0])[:10]
                metrics["end_date"] = str(sample_dates[-1])[:10]
                metrics["window_days"] = window_size
                results.append(metrics)
            
            if (i + 1) % 100 == 0:
                logger.info(f"  æ—¶é—´çª—å£é‡‡æ ·è¿›åº¦: {i + 1}/{self.n_simulations}")
        
        logger.info(f"  æ—¶é—´çª—å£é‡‡æ ·å®Œæˆ: {len(results)} æ¬¡æœ‰æ•ˆæ¨¡æ‹Ÿ")
        return results
    
    # ========================================================================
    # æ±‡æ€»å’Œå¯è§†åŒ–
    # ========================================================================
    def aggregate_results(self, all_results: List[Dict]) -> pd.DataFrame:
        """æ±‡æ€»æ‰€æœ‰æ¨¡æ‹Ÿç»“æœ"""
        if not all_results:
            return pd.DataFrame()
        
        df = pd.DataFrame(all_results)
        # ç§»é™¤ equity_curve åˆ—ï¼ˆå¤ªå¤§ï¼‰
        if "equity_curve" in df.columns:
            df = df.drop(columns=["equity_curve"])
        return df
    
    def compute_statistics(self, results_df: pd.DataFrame) -> Dict:
        """
        è®¡ç®—ç»Ÿè®¡æ±‡æ€»
        
        Returns:
            åŒ…å«å„é¡¹æŒ‡æ ‡ç»Ÿè®¡çš„å­—å…¸
        """
        if results_df.empty:
            return {}
        
        stats = {
            "total_simulations": len(results_df),
            "annual_return": {
                "mean": results_df["annual_return"].mean(),
                "median": results_df["annual_return"].median(),
                "std": results_df["annual_return"].std(),
                "p5": results_df["annual_return"].quantile(0.05),
                "p25": results_df["annual_return"].quantile(0.25),
                "p75": results_df["annual_return"].quantile(0.75),
                "p95": results_df["annual_return"].quantile(0.95),
                "min": results_df["annual_return"].min(),
                "max": results_df["annual_return"].max(),
            },
            "sharpe": {
                "mean": results_df["sharpe"].mean(),
                "median": results_df["sharpe"].median(),
                "std": results_df["sharpe"].std(),
                "p5": results_df["sharpe"].quantile(0.05),
                "p95": results_df["sharpe"].quantile(0.95),
            },
            "max_drawdown": {
                "mean": results_df["max_drawdown"].mean(),
                "median": results_df["max_drawdown"].median(),
                "std": results_df["max_drawdown"].std(),
                "p5": results_df["max_drawdown"].quantile(0.05),
                "p95": results_df["max_drawdown"].quantile(0.95),
            }
        }
        
        return stats
    
    def plot_return_distribution(self, results_df: pd.DataFrame, stats: Dict):
        """ç»˜åˆ¶æ”¶ç›Šåˆ†å¸ƒå›¾"""
        if results_df.empty:
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        
        # 1. å¹´åŒ–æ”¶ç›Šåˆ†å¸ƒ
        ax1 = axes[0, 0]
        returns = results_df["annual_return"] * 100  # è½¬ä¸ºç™¾åˆ†æ¯”
        ax1.hist(returns, bins=50, edgecolor='black', alpha=0.7, color='#3498db')
        ax1.axvline(stats["annual_return"]["median"] * 100, color='red', linestyle='--', 
                   linewidth=2, label=f'ä¸­ä½æ•°: {stats["annual_return"]["median"]*100:.1f}%')
        ax1.axvline(stats["annual_return"]["p5"] * 100, color='orange', linestyle=':', 
                   linewidth=2, label=f'5%åˆ†ä½: {stats["annual_return"]["p5"]*100:.1f}%')
        ax1.axvline(stats["annual_return"]["p95"] * 100, color='green', linestyle=':', 
                   linewidth=2, label=f'95%åˆ†ä½: {stats["annual_return"]["p95"]*100:.1f}%')
        ax1.set_xlabel("å¹´åŒ–æ”¶ç›Šç‡ (%)")
        ax1.set_ylabel("é¢‘æ¬¡")
        ax1.set_title("å¹´åŒ–æ”¶ç›Šåˆ†å¸ƒ")
        ax1.legend(loc='upper right')
        ax1.grid(True, alpha=0.3)
        
        # 2. å¤æ™®æ¯”ç‡åˆ†å¸ƒ
        ax2 = axes[0, 1]
        sharpes = results_df["sharpe"]
        ax2.hist(sharpes, bins=50, edgecolor='black', alpha=0.7, color='#2ecc71')
        ax2.axvline(stats["sharpe"]["median"], color='red', linestyle='--', 
                   linewidth=2, label=f'ä¸­ä½æ•°: {stats["sharpe"]["median"]:.2f}')
        ax2.set_xlabel("å¤æ™®æ¯”ç‡")
        ax2.set_ylabel("é¢‘æ¬¡")
        ax2.set_title("å¤æ™®æ¯”ç‡åˆ†å¸ƒ")
        ax2.legend(loc='upper right')
        ax2.grid(True, alpha=0.3)
        
        # 3. æœ€å¤§å›æ’¤åˆ†å¸ƒ
        ax3 = axes[1, 0]
        drawdowns = results_df["max_drawdown"] * 100  # è½¬ä¸ºç™¾åˆ†æ¯”
        ax3.hist(drawdowns, bins=50, edgecolor='black', alpha=0.7, color='#e74c3c')
        ax3.axvline(stats["max_drawdown"]["median"] * 100, color='blue', linestyle='--', 
                   linewidth=2, label=f'ä¸­ä½æ•°: {stats["max_drawdown"]["median"]*100:.1f}%')
        ax3.set_xlabel("æœ€å¤§å›æ’¤ (%)")
        ax3.set_ylabel("é¢‘æ¬¡")
        ax3.set_title("æœ€å¤§å›æ’¤åˆ†å¸ƒ")
        ax3.legend(loc='upper right')
        ax3.grid(True, alpha=0.3)
        
        # 4. æ”¶ç›Š-é£é™©æ•£ç‚¹å›¾
        ax4 = axes[1, 1]
        scatter = ax4.scatter(
            results_df["max_drawdown"] * 100, 
            results_df["annual_return"] * 100,
            c=results_df["sharpe"], 
            cmap='RdYlGn', 
            alpha=0.6,
            s=30
        )
        ax4.set_xlabel("æœ€å¤§å›æ’¤ (%)")
        ax4.set_ylabel("å¹´åŒ–æ”¶ç›Šç‡ (%)")
        ax4.set_title("æ”¶ç›Š-é£é™©æ•£ç‚¹å›¾ (é¢œè‰²=å¤æ™®æ¯”ç‡)")
        plt.colorbar(scatter, ax=ax4, label="å¤æ™®æ¯”ç‡")
        ax4.grid(True, alpha=0.3)
        
        plt.suptitle("è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿç»“æœåˆ†æ", fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        chart_path = os.path.join(self.report_dir, "monte_carlo_distribution.png")
        plt.savefig(chart_path, dpi=150)
        plt.close()
        logger.info(f"åˆ†å¸ƒå›¾å·²ä¿å­˜: {chart_path}")
    
    def plot_noise_sensitivity(self, results_df: pd.DataFrame):
        """ç»˜åˆ¶å™ªéŸ³æ•æ„Ÿæ€§å›¾"""
        noise_results = results_df[results_df["method"] == "noise_injection"]
        if noise_results.empty:
            return
        
        # æŒ‰å™ªéŸ³æ¯”ä¾‹åˆ†ç»„
        grouped = noise_results.groupby("noise_ratio").agg({
            "annual_return": ["mean", "std"],
            "sharpe": ["mean", "std"]
        }).reset_index()
        
        fig, axes = plt.subplots(1, 2, figsize=(12, 5))
        
        # 1. å¹´åŒ–æ”¶ç›Š vs å™ªéŸ³æ¯”ä¾‹
        ax1 = axes[0]
        noise_levels = grouped["noise_ratio"] * 100
        returns_mean = grouped[("annual_return", "mean")] * 100
        returns_std = grouped[("annual_return", "std")] * 100
        
        ax1.plot(noise_levels, returns_mean, 'b-o', linewidth=2, markersize=6, label='å‡å€¼')
        ax1.fill_between(noise_levels, returns_mean - returns_std, returns_mean + returns_std, 
                        alpha=0.3, color='blue')
        ax1.set_xlabel("å™ªéŸ³æ¯”ä¾‹ (%)")
        ax1.set_ylabel("å¹´åŒ–æ”¶ç›Šç‡ (%)")
        ax1.set_title("å¹´åŒ–æ”¶ç›Š vs å™ªéŸ³å¼ºåº¦")
        ax1.grid(True, alpha=0.3)
        ax1.legend()
        
        # 2. å¤æ™®æ¯”ç‡ vs å™ªéŸ³æ¯”ä¾‹
        ax2 = axes[1]
        sharpe_mean = grouped[("sharpe", "mean")]
        sharpe_std = grouped[("sharpe", "std")]
        
        ax2.plot(noise_levels, sharpe_mean, 'g-o', linewidth=2, markersize=6, label='å‡å€¼')
        ax2.fill_between(noise_levels, sharpe_mean - sharpe_std, sharpe_mean + sharpe_std, 
                        alpha=0.3, color='green')
        ax2.set_xlabel("å™ªéŸ³æ¯”ä¾‹ (%)")
        ax2.set_ylabel("å¤æ™®æ¯”ç‡")
        ax2.set_title("å¤æ™®æ¯”ç‡ vs å™ªéŸ³å¼ºåº¦")
        ax2.grid(True, alpha=0.3)
        ax2.legend()
        
        plt.suptitle("å™ªéŸ³æ•æ„Ÿæ€§åˆ†æ", fontsize=14, fontweight='bold')
        plt.tight_layout()
        
        chart_path = os.path.join(self.report_dir, "noise_sensitivity.png")
        plt.savefig(chart_path, dpi=150)
        plt.close()
        logger.info(f"å™ªéŸ³æ•æ„Ÿæ€§å›¾å·²ä¿å­˜: {chart_path}")
    
    def plot_weight_sensitivity(self, results_df: pd.DataFrame):
        """ç»˜åˆ¶æƒé‡æ•æ„Ÿæ€§å›¾"""
        weight_results = results_df[results_df["method"] == "weight_perturbation"]
        if weight_results.empty or "reg_weight" not in weight_results.columns:
            return
        
        fig, ax = plt.subplots(figsize=(10, 6))
        
        scatter = ax.scatter(
            weight_results["reg_weight"], 
            weight_results["annual_return"] * 100,
            c=weight_results["sharpe"], 
            cmap='RdYlGn', 
            alpha=0.6,
            s=50
        )
        
        # æ ‡è®°æœ€ä½³ç‚¹
        best_idx = weight_results["sharpe"].idxmax()
        best_row = weight_results.loc[best_idx]
        ax.scatter(best_row["reg_weight"], best_row["annual_return"] * 100, 
                  s=200, c='red', marker='*', edgecolors='black', linewidths=2,
                  label=f'æœ€ä½³: Î±={best_row["reg_weight"]:.2f}, å¤æ™®={best_row["sharpe"]:.2f}')
        
        # æ ‡è®°åŸå§‹æƒé‡
        ax.axvline(self.base_reg_weight, color='orange', linestyle='--', 
                  linewidth=2, label=f'é…ç½®æƒé‡: Î±={self.base_reg_weight:.2f}')
        
        ax.set_xlabel("å›å½’æƒé‡ (Î±)")
        ax.set_ylabel("å¹´åŒ–æ”¶ç›Šç‡ (%)")
        ax.set_title("èåˆæƒé‡æ•æ„Ÿæ€§åˆ†æ")
        plt.colorbar(scatter, ax=ax, label="å¤æ™®æ¯”ç‡")
        ax.legend(loc='upper right')
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        chart_path = os.path.join(self.report_dir, "weight_sensitivity.png")
        plt.savefig(chart_path, dpi=150)
        plt.close()
        logger.info(f"æƒé‡æ•æ„Ÿæ€§å›¾å·²ä¿å­˜: {chart_path}")
    
    def generate_report(self, results_df: pd.DataFrame, stats: Dict):
        """ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š"""
        if results_df.empty:
            return
        
        report_lines = []
        report_lines.append("=" * 60)
        report_lines.append("ğŸ“Š è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿåˆ†ææŠ¥å‘Š (Monte Carlo Simulation Report)")
        report_lines.append("=" * 60)
        report_lines.append("")
        
        report_lines.append(f"æ¨¡æ‹Ÿæ€»æ¬¡æ•°: {stats['total_simulations']}")
        report_lines.append("")
        
        report_lines.append("-" * 60)
        report_lines.append("ã€å¹´åŒ–æ”¶ç›Šç‡ç»Ÿè®¡ã€‘")
        report_lines.append("-" * 60)
        ar = stats["annual_return"]
        report_lines.append(f"  å‡å€¼:     {ar['mean']*100:>8.2f}%")
        report_lines.append(f"  ä¸­ä½æ•°:   {ar['median']*100:>8.2f}%")
        report_lines.append(f"  æ ‡å‡†å·®:   {ar['std']*100:>8.2f}%")
        report_lines.append(f"  5%åˆ†ä½:   {ar['p5']*100:>8.2f}%")
        report_lines.append(f"  25%åˆ†ä½:  {ar['p25']*100:>8.2f}%")
        report_lines.append(f"  75%åˆ†ä½:  {ar['p75']*100:>8.2f}%")
        report_lines.append(f"  95%åˆ†ä½:  {ar['p95']*100:>8.2f}%")
        report_lines.append(f"  æœ€å°å€¼:   {ar['min']*100:>8.2f}%")
        report_lines.append(f"  æœ€å¤§å€¼:   {ar['max']*100:>8.2f}%")
        report_lines.append("")
        
        report_lines.append("-" * 60)
        report_lines.append("ã€å¤æ™®æ¯”ç‡ç»Ÿè®¡ã€‘")
        report_lines.append("-" * 60)
        sr = stats["sharpe"]
        report_lines.append(f"  å‡å€¼:     {sr['mean']:>8.2f}")
        report_lines.append(f"  ä¸­ä½æ•°:   {sr['median']:>8.2f}")
        report_lines.append(f"  æ ‡å‡†å·®:   {sr['std']:>8.2f}")
        report_lines.append(f"  5%åˆ†ä½:   {sr['p5']:>8.2f}")
        report_lines.append(f"  95%åˆ†ä½:  {sr['p95']:>8.2f}")
        report_lines.append("")
        
        report_lines.append("-" * 60)
        report_lines.append("ã€æœ€å¤§å›æ’¤ç»Ÿè®¡ã€‘")
        report_lines.append("-" * 60)
        md = stats["max_drawdown"]
        report_lines.append(f"  å‡å€¼:     {md['mean']*100:>8.2f}%")
        report_lines.append(f"  ä¸­ä½æ•°:   {md['median']*100:>8.2f}%")
        report_lines.append(f"  æ ‡å‡†å·®:   {md['std']*100:>8.2f}%")
        report_lines.append(f"  5%åˆ†ä½:   {md['p5']*100:>8.2f}%")
        report_lines.append(f"  95%åˆ†ä½:  {md['p95']*100:>8.2f}%")
        report_lines.append("")
        
        report_lines.append("-" * 60)
        report_lines.append("ã€ç½®ä¿¡åŒºé—´è§£è¯»ã€‘")
        report_lines.append("-" * 60)
        report_lines.append(f"  â€¢ 90% ç½®ä¿¡åŒºé—´ä¸‹ï¼Œå¹´åŒ–æ”¶ç›Šé¢„æœŸåœ¨ {ar['p5']*100:.1f}% ~ {ar['p95']*100:.1f}% ä¹‹é—´")
        report_lines.append(f"  â€¢ 90% ç½®ä¿¡åŒºé—´ä¸‹ï¼Œå¤æ™®æ¯”ç‡é¢„æœŸåœ¨ {sr['p5']:.2f} ~ {sr['p95']:.2f} ä¹‹é—´")
        report_lines.append(f"  â€¢ 90% ç½®ä¿¡åŒºé—´ä¸‹ï¼Œæœ€å¤§å›æ’¤é¢„æœŸåœ¨ {md['p5']*100:.1f}% ~ {md['p95']*100:.1f}% ä¹‹é—´")
        
        # ç¨³å¥æ€§è¯„ä¼°
        report_lines.append("")
        report_lines.append("-" * 60)
        report_lines.append("ã€ç¨³å¥æ€§è¯„ä¼°ã€‘")
        report_lines.append("-" * 60)
        
        # æ”¶ç›Šæ³¢åŠ¨ç³»æ•°
        cv = abs(ar['std'] / ar['mean']) if ar['mean'] != 0 else float('inf')
        if cv < 0.3:
            stability = "âœ… é«˜åº¦ç¨³å¥ (å˜å¼‚ç³»æ•° < 0.3)"
        elif cv < 0.6:
            stability = "âš ï¸ ä¸­ç­‰ç¨³å¥ (å˜å¼‚ç³»æ•° 0.3 ~ 0.6)"
        else:
            stability = "âŒ æ³¢åŠ¨è¾ƒå¤§ (å˜å¼‚ç³»æ•° > 0.6)"
        report_lines.append(f"  æ”¶ç›Šç¨³å®šæ€§: {stability}")
        report_lines.append(f"  å˜å¼‚ç³»æ•°: {cv:.2f}")
        
        # æœ€å·®æƒ…å†µåˆ†æ
        if ar['p5'] > 0:
            report_lines.append(f"  æœ€å·® 5% æƒ…å†µä»ç›ˆåˆ©: âœ… æ˜¯ ({ar['p5']*100:.1f}%)")
        else:
            report_lines.append(f"  æœ€å·® 5% æƒ…å†µä»ç›ˆåˆ©: âŒ å¦ ({ar['p5']*100:.1f}%)")
        
        report_lines.append("=" * 60)
        
        # æ‰“å°æŠ¥å‘Š
        report_text = "\n".join(report_lines)
        print("\n" + report_text)
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = os.path.join(self.report_dir, "monte_carlo_report.txt")
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(report_text)
        logger.info(f"æŠ¥å‘Šå·²ä¿å­˜: {report_path}")


# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================

def main():
    logger.info("=" * 60)
    logger.info("=== è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿåˆ†æ (Monte Carlo Simulation) ===")
    logger.info("=" * 60)
    
    # åˆå§‹åŒ–åˆ†æå™¨
    analyzer = MonteCarloAnalyzer(n_simulations = 50, random_seed=42)
    
    # 1. åŠ è½½é¢„æµ‹æ•°æ®
    pred_df = analyzer.load_predictions()
    if pred_df is None:
        logger.error("æœªæ‰¾åˆ°é¢„æµ‹æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ run_walkforward.py")
        return
    
    logger.info(f"é¢„æµ‹æ•°æ®: {len(pred_df)} è¡Œ, æ—¥æœŸèŒƒå›´: {pred_df['date'].min()} ~ {pred_df['date'].max()}")
    
    # æ£€æŸ¥åŒå¤´æ¨¡å‹åˆ—
    has_dual_head = "pred_reg" in pred_df.columns and "pred_cls" in pred_df.columns
    logger.info(f"åŒå¤´æ¨¡å‹é¢„æµ‹åˆ—: {'å­˜åœ¨' if has_dual_head else 'ä¸å­˜åœ¨'}")
    
    # 2. æ‰§è¡Œå„ç§æ¨¡æ‹Ÿ
    all_results = []
    
    # 2.1 Bootstrap é‡é‡‡æ ·
    bootstrap_results = analyzer.run_bootstrap_simulation(pred_df)
    all_results.extend(bootstrap_results)
    
    # 2.2 æƒé‡æ‰°åŠ¨ï¼ˆä»…åŒå¤´æ¨¡å‹ï¼‰
    if has_dual_head:
        weight_results = analyzer.run_weight_perturbation(pred_df)
        all_results.extend(weight_results)
    
    # 2.3 å™ªéŸ³æ³¨å…¥
    noise_results = analyzer.run_noise_injection(pred_df)
    all_results.extend(noise_results)
    
    # 2.4 æ—¶é—´çª—å£é‡‡æ ·
    time_results = analyzer.run_time_window_sampling(pred_df)
    all_results.extend(time_results)
    
    # 3. æ±‡æ€»ç»“æœ
    results_df = analyzer.aggregate_results(all_results)
    
    if results_df.empty:
        logger.error("æ‰€æœ‰æ¨¡æ‹Ÿå‡å¤±è´¥ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š")
        return
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    results_path = os.path.join(analyzer.report_dir, "monte_carlo_results.csv")
    results_df.to_csv(results_path, index=False, encoding="utf-8-sig")
    logger.info(f"è¯¦ç»†ç»“æœå·²ä¿å­˜: {results_path}")
    
    # 4. è®¡ç®—ç»Ÿè®¡æ±‡æ€»
    stats = analyzer.compute_statistics(results_df)
    
    # 5. ç”Ÿæˆå¯è§†åŒ–
    analyzer.plot_return_distribution(results_df, stats)
    analyzer.plot_noise_sensitivity(results_df)
    if has_dual_head:
        analyzer.plot_weight_sensitivity(results_df)
    
    # 6. ç”ŸæˆæŠ¥å‘Š
    analyzer.generate_report(results_df, stats)
    
    logger.info(f"\nè’™ç‰¹å¡æ´›åˆ†æå®Œæˆï¼æŠ¥å‘Šå·²ä¿å­˜è‡³: {analyzer.report_dir}")


if __name__ == "__main__":
    main()
