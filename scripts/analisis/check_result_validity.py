# scripts/analisis/check_result_validity.py
# ============================================================================
# å›æµ‹ç»“æœæœ‰æ•ˆæ€§éªŒè¯ (Result Validity Checker)
# ============================================================================
#
# ã€åŠŸèƒ½ã€‘
# æ£€æŸ¥å›æµ‹ç»“æœæ˜¯å¦å­˜åœ¨æ½œåœ¨é—®é¢˜ï¼š
#   1. å¸‚å€¼/æµåŠ¨æ€§åå·® - æ˜¯å¦åªé€‰å°ç›˜è‚¡
#   2. æˆæœ¬æ•æ„Ÿæ€§ - é«˜æˆæœ¬ä¸‹æ˜¯å¦è¿˜ç›ˆåˆ©
#   3. åˆ†å¹´åº¦æ”¶ç›Š - æ”¶ç›Šæ˜¯å¦é›†ä¸­åœ¨æŸäº›å¹´ä»½
#   4. ç‰¹å¾æ³„éœ²æ£€æŸ¥ - æ˜¯å¦å­˜åœ¨æœªæ¥å‡½æ•°
#
# ã€ä½¿ç”¨æ–¹æ³•ã€‘
# python scripts/analisis/check_result_validity.py
# ============================================================================

import os
import sys
import pandas as pd
import numpy as np
from collections import defaultdict

# Matplotlib å­—ä½“é…ç½®
import matplotlib
matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS', 'sans-serif']
matplotlib.rcParams['axes.unicode_minus'] = False

import matplotlib.pyplot as plt

# è·¯å¾„é€‚é…
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from src.utils.config import GLOBAL_CONFIG
from src.utils.logger import get_logger
from src.utils.io import read_parquet, ensure_dir
from src.strategy.signal import TopKSignalStrategy
from src.backtest.backtester import VectorBacktester

logger = get_logger()


def get_latest_predictions():
    """è·å–æœ€æ–°çš„é¢„æµ‹æ–‡ä»¶"""
    models_dir = GLOBAL_CONFIG["paths"]["models"]
    if not os.path.exists(models_dir):
        return None, None
    
    subdirs = [d for d in os.listdir(models_dir) if os.path.isdir(os.path.join(models_dir, d))]
    if not subdirs:
        return None, None
    
    subdirs.sort(reverse=True)
    latest_dir = subdirs[0]
    pred_path = os.path.join(models_dir, latest_dir, "predictions.parquet")
    
    if os.path.exists(pred_path):
        logger.info(f"ä½¿ç”¨é¢„æµ‹æ–‡ä»¶: {pred_path}")
        return read_parquet(pred_path), latest_dir
    return None, None


def load_stock_data():
    """åŠ è½½è‚¡ç¥¨è¡Œæƒ…æ•°æ®"""
    data_path = os.path.join(GLOBAL_CONFIG["paths"]["data_processed"], "all_stocks.parquet")
    if os.path.exists(data_path):
        return read_parquet(data_path)
    return None


def analyze_market_cap_distribution(pred_df, signal_df, stock_df, report_dir):
    """åˆ†æé€‰è‚¡ç»„åˆçš„å¸‚å€¼åˆ†å¸ƒ"""
    print("\n" + "=" * 60)
    print("ğŸ“Š å¸‚å€¼åˆ†å¸ƒåˆ†æ (Market Cap Distribution)")
    print("=" * 60)
    
    # è·å–è¢«é€‰ä¸­çš„è‚¡ç¥¨
    selected_stocks = signal_df[["date", "symbol"]].copy()
    selected_stocks["date"] = pd.to_datetime(selected_stocks["date"])
    
    # åˆå¹¶å¸‚å€¼æ•°æ®
    if "mcap" in stock_df.columns or "feat_mcap" in stock_df.columns:
        mcap_col = "mcap" if "mcap" in stock_df.columns else "feat_mcap"
        stock_df["date"] = pd.to_datetime(stock_df["date"])
        
        merged = selected_stocks.merge(
            stock_df[["date", "symbol", mcap_col]], 
            on=["date", "symbol"], 
            how="left"
        )
        
        # å¸‚å€¼åˆ†ä½æ•°
        mcap_values = merged[mcap_col].dropna() / 1e8  # è½¬æ¢ä¸ºäº¿å…ƒ
        
        print(f"\n  é€‰è‚¡ç»„åˆå¸‚å€¼ç»Ÿè®¡ (å•ä½: äº¿å…ƒ):")
        print(f"  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
        print(f"  â”‚ æœ€å°å¸‚å€¼:    {mcap_values.min():>10.2f} äº¿ â”‚")
        print(f"  â”‚ 25%åˆ†ä½:     {mcap_values.quantile(0.25):>10.2f} äº¿ â”‚")
        print(f"  â”‚ ä¸­ä½æ•°:      {mcap_values.median():>10.2f} äº¿ â”‚")
        print(f"  â”‚ å¹³å‡å€¼:      {mcap_values.mean():>10.2f} äº¿ â”‚")
        print(f"  â”‚ 75%åˆ†ä½:     {mcap_values.quantile(0.75):>10.2f} äº¿ â”‚")
        print(f"  â”‚ æœ€å¤§å¸‚å€¼:    {mcap_values.max():>10.2f} äº¿ â”‚")
        print(f"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
        
        # å¸‚å€¼åˆ†å±‚ç»Ÿè®¡
        small_cap = (mcap_values < 50).sum()
        mid_cap = ((mcap_values >= 50) & (mcap_values < 200)).sum()
        large_cap = (mcap_values >= 200).sum()
        total = len(mcap_values)
        
        print(f"\n  å¸‚å€¼åˆ†å±‚:")
        print(f"    å°ç›˜è‚¡ (<50äº¿):   {small_cap:>5} æ¬¡  ({small_cap/total*100:.1f}%)")
        print(f"    ä¸­ç›˜è‚¡ (50-200äº¿): {mid_cap:>5} æ¬¡  ({mid_cap/total*100:.1f}%)")
        print(f"    å¤§ç›˜è‚¡ (>200äº¿):  {large_cap:>5} æ¬¡  ({large_cap/total*100:.1f}%)")
        
        # è­¦å‘Š
        if small_cap / total > 0.7:
            print(f"\n  âš ï¸  [è­¦å‘Š] é€‰è‚¡ç»„åˆè¿‡åº¦åå‘å°ç›˜è‚¡ ({small_cap/total*100:.1f}%)")
            print(f"      å°ç›˜è‚¡æµåŠ¨æ€§å·®ï¼Œå®ç›˜å¯èƒ½æ— æ³•æŒ‰é¢„æœŸä»·æ ¼æˆäº¤")
        
        # ç»˜å›¾
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.hist(mcap_values, bins=50, edgecolor='black', alpha=0.7, color='#3498db')
        ax.axvline(mcap_values.median(), color='red', linestyle='--', linewidth=2, 
                   label=f'ä¸­ä½æ•°: {mcap_values.median():.1f}äº¿')
        ax.set_xlabel("æµé€šå¸‚å€¼ (äº¿å…ƒ)")
        ax.set_ylabel("é¢‘æ¬¡")
        ax.set_title("é€‰è‚¡ç»„åˆå¸‚å€¼åˆ†å¸ƒ")
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        chart_path = os.path.join(report_dir, "market_cap_distribution.png")
        plt.savefig(chart_path, dpi=150)
        plt.close()
        logger.info(f"å¸‚å€¼åˆ†å¸ƒå›¾å·²ä¿å­˜: {chart_path}")
        
        return mcap_values.median()
    else:
        print("  [è·³è¿‡] æ•°æ®ä¸­æœªæ‰¾åˆ°å¸‚å€¼åˆ—")
        return None


def analyze_cost_sensitivity(pred_df, report_dir):
    """åˆ†æä¸åŒæˆæœ¬ä¸‹çš„å›æµ‹è¡¨ç°"""
    print("\n" + "=" * 60)
    print("ğŸ’° æˆæœ¬æ•æ„Ÿæ€§åˆ†æ (Cost Sensitivity)")
    print("=" * 60)
    
    pred_df = pred_df.copy()
    pred_df["date"] = pd.to_datetime(pred_df["date"])
    
    strategy = TopKSignalStrategy()
    if not GLOBAL_CONFIG["strategy"].get("position_control", {}).get("enable", False):
        strategy.min_score = -999.0
    
    signal_df = strategy.generate(pred_df)
    
    cost_rates = [0.001, 0.002, 0.003, 0.005, 0.008, 0.01]
    results = []
    
    backtester = VectorBacktester()
    
    for cost in cost_rates:
        out_path = os.path.join(report_dir, f"cost_{int(cost*1000)}bps")
        metrics = backtester.run(signal_df, output_dir=out_path, cost_rate=cost)
        
        results.append({
            "cost_bps": cost * 10000,
            "annual_return": metrics["annual_return"],
            "sharpe": metrics["sharpe"],
            "max_drawdown": metrics["max_drawdown"]
        })
        print(f"  æˆæœ¬ {cost*100:.1f}%: å¹´åŒ–={metrics['annual_return']*100:.1f}%, å¤æ™®={metrics['sharpe']:.2f}")
    
    df_results = pd.DataFrame(results)
    
    # ç»˜å›¾
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    ax1 = axes[0]
    ax1.plot(df_results["cost_bps"], df_results["annual_return"] * 100, 'b-o', linewidth=2, markersize=8)
    ax1.axhline(0, color='red', linestyle='--', alpha=0.5)
    ax1.set_xlabel("äº¤æ˜“æˆæœ¬ (åŸºç‚¹ bps)")
    ax1.set_ylabel("å¹´åŒ–æ”¶ç›Šç‡ (%)")
    ax1.set_title("å¹´åŒ–æ”¶ç›Š vs äº¤æ˜“æˆæœ¬")
    ax1.grid(True, alpha=0.3)
    
    ax2 = axes[1]
    ax2.plot(df_results["cost_bps"], df_results["sharpe"], 'g-o', linewidth=2, markersize=8)
    ax2.axhline(1, color='red', linestyle='--', alpha=0.5, label="å¤æ™®=1 é—¨æ§›")
    ax2.set_xlabel("äº¤æ˜“æˆæœ¬ (åŸºç‚¹ bps)")
    ax2.set_ylabel("å¤æ™®æ¯”ç‡")
    ax2.set_title("å¤æ™®æ¯”ç‡ vs äº¤æ˜“æˆæœ¬")
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.suptitle("æˆæœ¬æ•æ„Ÿæ€§åˆ†æ", fontsize=14, fontweight='bold')
    plt.tight_layout()
    
    chart_path = os.path.join(report_dir, "cost_sensitivity.png")
    plt.savefig(chart_path, dpi=150)
    plt.close()
    logger.info(f"æˆæœ¬æ•æ„Ÿæ€§å›¾å·²ä¿å­˜: {chart_path}")
    
    # æ‰¾åˆ°ç›ˆäºå¹³è¡¡ç‚¹
    for r in results:
        if r["annual_return"] <= 0:
            print(f"\n  âš ï¸  ç›ˆäºå¹³è¡¡ç‚¹: æˆæœ¬çº¦ {r['cost_bps']:.0f} åŸºç‚¹ ({r['cost_bps']/100:.2f}%)")
            break
    else:
        print(f"\n  âœ…  å³ä½¿æˆæœ¬è¾¾åˆ° 100 åŸºç‚¹ (1%)ï¼Œç­–ç•¥ä»ç„¶ç›ˆåˆ©")
    
    return df_results


def analyze_yearly_returns(pred_df, report_dir):
    """åˆ†å¹´åº¦æ”¶ç›Šåˆ†æ"""
    print("\n" + "=" * 60)
    print("ğŸ“… åˆ†å¹´åº¦æ”¶ç›Šåˆ†æ (Yearly Returns)")
    print("=" * 60)
    
    pred_df = pred_df.copy()
    pred_df["date"] = pd.to_datetime(pred_df["date"])
    pred_df["year"] = pred_df["date"].dt.year
    
    years = sorted(pred_df["year"].unique())
    
    strategy = TopKSignalStrategy()
    if not GLOBAL_CONFIG["strategy"].get("position_control", {}).get("enable", False):
        strategy.min_score = -999.0
    
    backtester = VectorBacktester()
    results = []
    
    for year in years:
        year_df = pred_df[pred_df["year"] == year].copy()
        if len(year_df) < 50:
            continue
        
        signal_df = strategy.generate(year_df)
        if signal_df.empty:
            continue
        
        out_path = os.path.join(report_dir, f"year_{year}")
        try:
            metrics = backtester.run(signal_df, output_dir=out_path)
            results.append({
                "year": year,
                "annual_return": metrics["annual_return"],
                "sharpe": metrics["sharpe"],
                "max_drawdown": metrics["max_drawdown"],
                "trades": len(signal_df)
            })
            print(f"  {year}: å¹´åŒ–={metrics['annual_return']*100:>7.1f}%, å¤æ™®={metrics['sharpe']:>5.2f}, å›æ’¤={metrics['max_drawdown']*100:>6.1f}%")
        except Exception as e:
            print(f"  {year}: [è·³è¿‡] {e}")
    
    if not results:
        return None
    
    df_results = pd.DataFrame(results)
    
    # ç»˜å›¾
    fig, ax = plt.subplots(figsize=(12, 6))
    
    colors = ['#2ecc71' if r > 0 else '#e74c3c' for r in df_results["annual_return"]]
    bars = ax.bar(df_results["year"].astype(str), df_results["annual_return"] * 100, color=colors, edgecolor='black')
    
    ax.axhline(0, color='black', linewidth=0.5)
    ax.axhline(df_results["annual_return"].mean() * 100, color='blue', linestyle='--', 
               label=f'å¹³å‡: {df_results["annual_return"].mean()*100:.1f}%')
    
    ax.set_xlabel("å¹´ä»½")
    ax.set_ylabel("å¹´åŒ–æ”¶ç›Šç‡ (%)")
    ax.set_title("åˆ†å¹´åº¦æ”¶ç›Šè¡¨ç°")
    ax.legend()
    ax.grid(True, alpha=0.3, axis='y')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, val in zip(bars, df_results["annual_return"] * 100):
        height = bar.get_height()
        ax.annotate(f'{val:.0f}%',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3),
                    textcoords="offset points",
                    ha='center', va='bottom', fontsize=9)
    
    plt.tight_layout()
    chart_path = os.path.join(report_dir, "yearly_returns.png")
    plt.savefig(chart_path, dpi=150)
    plt.close()
    logger.info(f"åˆ†å¹´åº¦æ”¶ç›Šå›¾å·²ä¿å­˜: {chart_path}")
    
    # åˆ†æ
    best_year = df_results.loc[df_results["annual_return"].idxmax()]
    worst_year = df_results.loc[df_results["annual_return"].idxmin()]
    
    print(f"\n  æœ€ä½³å¹´ä»½: {int(best_year['year'])} ({best_year['annual_return']*100:.1f}%)")
    print(f"  æœ€å·®å¹´ä»½: {int(worst_year['year'])} ({worst_year['annual_return']*100:.1f}%)")
    print(f"  å¹³å‡å¹´åŒ–: {df_results['annual_return'].mean()*100:.1f}%")
    print(f"  å¹´åŒ–æ ‡å‡†å·®: {df_results['annual_return'].std()*100:.1f}%")
    
    # æ£€æŸ¥æ˜¯å¦æ”¶ç›Šè¿‡äºé›†ä¸­
    if best_year['annual_return'] > df_results['annual_return'].sum() * 0.5:
        print(f"\n  âš ï¸  [è­¦å‘Š] æ”¶ç›Šè¿‡åº¦é›†ä¸­åœ¨ {int(best_year['year'])} å¹´")
        print(f"      è¯¥å¹´è´¡çŒ®äº†è¶…è¿‡50%çš„ç´¯è®¡æ”¶ç›Šï¼Œç­–ç•¥å¯èƒ½ä¸å¤Ÿç¨³å¥")
    
    return df_results


def check_feature_leakage(stock_df):
    """æ£€æŸ¥ç‰¹å¾æ˜¯å¦å­˜åœ¨æœªæ¥æ•°æ®æ³„éœ²"""
    print("\n" + "=" * 60)
    print("ğŸ” ç‰¹å¾æ³„éœ²æ£€æŸ¥ (Feature Leakage Check)")
    print("=" * 60)
    
    if stock_df is None:
        print("  [è·³è¿‡] æ— æ³•åŠ è½½è‚¡ç¥¨æ•°æ®")
        return
    
    # æ£€æŸ¥ç‰¹å¾åˆ—
    feat_cols = [c for c in stock_df.columns if c.startswith("feat_")]
    print(f"\n  ç‰¹å¾æ€»æ•°: {len(feat_cols)}")
    
    # æ£€æŸ¥ç‰¹å¾ä¸æ ‡ç­¾çš„ç›¸å…³æ€§
    if "label" in stock_df.columns:
        correlations = []
        for col in feat_cols:
            if stock_df[col].notna().sum() > 100:
                corr = stock_df[col].corr(stock_df["label"])
                correlations.append((col, abs(corr)))
        
        correlations.sort(key=lambda x: x[1], reverse=True)
        
        print(f"\n  ç‰¹å¾ä¸æ ‡ç­¾ç›¸å…³æ€§ Top 10:")
        print(f"  {'ç‰¹å¾å':<30} {'ç›¸å…³ç³»æ•°':>10}")
        print(f"  {'-'*40}")
        
        suspicious = []
        for col, corr in correlations[:10]:
            flag = " âš ï¸" if corr > 0.5 else ""
            print(f"  {col:<30} {corr:>10.4f}{flag}")
            if corr > 0.5:
                suspicious.append(col)
        
        if suspicious:
            print(f"\n  âš ï¸  [è­¦å‘Š] ä»¥ä¸‹ç‰¹å¾ä¸æ ‡ç­¾ç›¸å…³æ€§è¿‡é«˜ (>0.5):")
            for col in suspicious:
                print(f"      - {col}")
            print(f"      è¿™å¯èƒ½è¡¨ç¤ºå­˜åœ¨æœªæ¥æ•°æ®æ³„éœ²ï¼Œè¯·æ£€æŸ¥ç‰¹å¾è®¡ç®—é€»è¾‘")
        else:
            print(f"\n  âœ…  æœªå‘ç°æ˜æ˜¾çš„ç‰¹å¾æ³„éœ²è¿¹è±¡")


def generate_summary_report(results, report_dir):
    """ç”Ÿæˆç»¼åˆè¯Šæ–­æŠ¥å‘Š"""
    print("\n" + "=" * 60)
    print("ğŸ“‹ ç»¼åˆè¯Šæ–­æŠ¥å‘Š (Summary Report)")
    print("=" * 60)
    
    warnings = []
    positives = []
    
    # å¸‚å€¼åˆ†æç»“è®º
    if results.get("median_mcap"):
        if results["median_mcap"] < 50:
            warnings.append(f"é€‰è‚¡ç»„åˆä¸­ä½å¸‚å€¼ä»… {results['median_mcap']:.1f} äº¿ï¼Œåå‘å°ç›˜è‚¡")
        else:
            positives.append(f"é€‰è‚¡ç»„åˆä¸­ä½å¸‚å€¼ {results['median_mcap']:.1f} äº¿ï¼ŒæµåŠ¨æ€§å¯æ¥å—")
    
    # æˆæœ¬æ•æ„Ÿæ€§ç»“è®º
    if results.get("cost_df") is not None:
        high_cost = results["cost_df"][results["cost_df"]["cost_bps"] == 50]
        if not high_cost.empty:
            ret_at_50bps = high_cost.iloc[0]["annual_return"]
            if ret_at_50bps > 0.2:
                positives.append(f"0.5% æˆæœ¬ä¸‹ä»æœ‰ {ret_at_50bps*100:.1f}% å¹´åŒ–æ”¶ç›Š")
            elif ret_at_50bps > 0:
                warnings.append(f"0.5% æˆæœ¬ä¸‹æ”¶ç›Šé™è‡³ {ret_at_50bps*100:.1f}%ï¼Œç©ºé—´æœ‰é™")
            else:
                warnings.append(f"0.5% æˆæœ¬ä¸‹å·²äºæŸï¼Œç­–ç•¥å¯¹æˆæœ¬æå…¶æ•æ„Ÿ")
    
    # å¹´åº¦æ”¶ç›Šç»“è®º
    if results.get("yearly_df") is not None:
        yearly = results["yearly_df"]
        if yearly["annual_return"].std() > 0.5:
            warnings.append(f"å¹´åº¦æ”¶ç›Šæ³¢åŠ¨å¤§ (æ ‡å‡†å·® {yearly['annual_return'].std()*100:.1f}%)ï¼Œç¨³å®šæ€§å­˜ç–‘")
        neg_years = (yearly["annual_return"] < 0).sum()
        if neg_years > 0:
            warnings.append(f"æœ‰ {neg_years} å¹´å½•å¾—è´Ÿæ”¶ç›Š")
        else:
            positives.append("å†å²ä¸Šæœªå‡ºç°å¹´åº¦äºæŸ")
    
    # è¾“å‡ºç»“è®º
    if positives:
        print("\n  âœ… ç§¯æä¿¡å·:")
        for p in positives:
            print(f"     â€¢ {p}")
    
    if warnings:
        print("\n  âš ï¸ éœ€å…³æ³¨çš„é—®é¢˜:")
        for w in warnings:
            print(f"     â€¢ {w}")
    
    # æœ€ç»ˆå»ºè®®
    print("\n  ğŸ’¡ å»ºè®®:")
    if len(warnings) > len(positives):
        print("     æ¨¡å‹ç»“æœå­˜åœ¨å¤šä¸ªå¯ç–‘ç‚¹ï¼Œå»ºè®®è°¨æ…å¯¹å¾…å›æµ‹æ”¶ç›Š")
        print("     åœ¨å®ç›˜å‰åº”è¿›è¡Œå°èµ„é‡‘æµ‹è¯•ï¼ŒéªŒè¯å®é™…è¡¨ç°")
    else:
        print("     æ¨¡å‹é€šè¿‡äº†åŸºæœ¬éªŒè¯ï¼Œä½†ä»éœ€æ³¨æ„:")
        print("     1. å®ç›˜èµ„é‡‘é‡ä¸å®œè¿‡å¤§ (å»ºè®® <50ä¸‡)")
        print("     2. å…³æ³¨å°ç›˜è‚¡æµåŠ¨æ€§é£é™©")
        print("     3. å®šæœŸç›‘æ§ç­–ç•¥è¡¨ç°è¡°å‡")
    
    print("=" * 60)


def main():
    logger.info("=" * 60)
    logger.info("=== å›æµ‹ç»“æœæœ‰æ•ˆæ€§éªŒè¯ (Result Validity Checker) ===")
    logger.info("=" * 60)
    
    # 1. åŠ è½½æ•°æ®
    pred_df, version = get_latest_predictions()
    if pred_df is None:
        logger.error("æœªæ‰¾åˆ°é¢„æµ‹æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ run_walkforward.py")
        return
    
    pred_df["date"] = pd.to_datetime(pred_df["date"])
    logger.info(f"é¢„æµ‹æ•°æ®: {len(pred_df)} è¡Œ, æ—¥æœŸèŒƒå›´: {pred_df['date'].min()} ~ {pred_df['date'].max()}")
    
    stock_df = load_stock_data()
    
    # ç”Ÿæˆä¿¡å·
    strategy = TopKSignalStrategy()
    if not GLOBAL_CONFIG["strategy"].get("position_control", {}).get("enable", False):
        strategy.min_score = -999.0
    signal_df = strategy.generate(pred_df)
    
    # è¾“å‡ºç›®å½•
    report_dir = os.path.join(GLOBAL_CONFIG["paths"]["reports"], "validity_check")
    ensure_dir(report_dir)
    
    results = {}
    
    # 2. å¸‚å€¼åˆ†å¸ƒåˆ†æ
    results["median_mcap"] = analyze_market_cap_distribution(pred_df, signal_df, stock_df, report_dir)
    
    # 3. æˆæœ¬æ•æ„Ÿæ€§åˆ†æ
    results["cost_df"] = analyze_cost_sensitivity(pred_df, report_dir)
    
    # 4. åˆ†å¹´åº¦æ”¶ç›Šåˆ†æ
    results["yearly_df"] = analyze_yearly_returns(pred_df, report_dir)
    
    # 5. ç‰¹å¾æ³„éœ²æ£€æŸ¥
    check_feature_leakage(stock_df)
    
    # 6. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    generate_summary_report(results, report_dir)
    
    logger.info(f"\néªŒè¯å®Œæˆï¼æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_dir}")


if __name__ == "__main__":
    main()
