# scripts/analisis/return_attribution.py
# ============================================================================
# æ”¶ç›Šå½’å› åˆ†ææ¨¡å— - åŒºåˆ†æ¨¡å‹Alphaå’Œå¸‚åœºBetaè´¡çŒ®
# ============================================================================

import os
import sys
import pandas as pd
import numpy as np
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# è·¯å¾„é€‚é…
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from src.utils.config import GLOBAL_CONFIG
from src.utils.logger import get_logger
from src.utils.io import read_parquet

logger = get_logger()


def analyze_return_attribution(
    start_date: str = None,
    end_date: str = None,
    model_version: str = None
):
    """
    æ”¶ç›Šå½’å› åˆ†æï¼šåˆ†ç¦»ç­–ç•¥æ”¶ç›Šä¸­çš„Alphaå’ŒBetaæˆåˆ†
    
    å‚æ•°:
    - start_date: åˆ†æèµ·å§‹æ—¥æœŸ (æ ¼å¼: "YYYY-MM-DD")
    - end_date: åˆ†æç»“æŸæ—¥æœŸ
    - model_version: æŒ‡å®šæ¨¡å‹ç‰ˆæœ¬ï¼Œé»˜è®¤ä½¿ç”¨æœ€æ–°
    
    å½’å› å…¬å¼:
    ç­–ç•¥æ”¶ç›Š = Alpha (è¶…é¢æ”¶ç›Š) + Beta Ã— å¸‚åœºæ”¶ç›Š
    """
    
    # 1. ç¡®å®šæ¨¡å‹ç‰ˆæœ¬
    models_dir = GLOBAL_CONFIG["paths"]["models"]
    if model_version is None:
        subdirs = [d for d in os.listdir(models_dir) if os.path.isdir(os.path.join(models_dir, d))]
        subdirs.sort(reverse=True)
        model_version = subdirs[0] if subdirs else None
    
    if model_version is None:
        logger.error("æœªæ‰¾åˆ°æ¨¡å‹ç‰ˆæœ¬")
        return
    
    logger.info(f"=== æ”¶ç›Šå½’å› åˆ†æ (Model: {model_version}) ===")
    
    # 2. åŠ è½½ç­–ç•¥é¢„æµ‹æ•°æ®
    model_dir = os.path.join(models_dir, model_version)
    pred_path = os.path.join(model_dir, "predictions.parquet")
    
    if not os.path.exists(pred_path):
        logger.error(f"é¢„æµ‹æ–‡ä»¶ä¸å­˜åœ¨: {pred_path}")
        return
    
    pred_df = read_parquet(pred_path)
    pred_df["date"] = pd.to_datetime(pred_df["date"])
    
    # 3. åŠ è½½åŸºå‡†æŒ‡æ•°æ•°æ®
    idx_code = GLOBAL_CONFIG.get("preprocessing", {}).get("labels", {}).get("index_code", "000852.SH")
    idx_file = os.path.join(GLOBAL_CONFIG["paths"]["data_raw"], f"index_{idx_code.replace('.', '')}.parquet")
    
    if not os.path.exists(idx_file):
        logger.error(f"æŒ‡æ•°æ–‡ä»¶ä¸å­˜åœ¨: {idx_file}")
        return
    
    idx_df = read_parquet(idx_file)
    idx_df["date"] = pd.to_datetime(idx_df["date"])
    idx_df = idx_df.set_index("date").sort_index()
    
    # 4. ç¡®å®šåˆ†ææ—¶é—´èŒƒå›´
    pred_dates = pred_df["date"].unique()
    pred_min, pred_max = pred_dates.min(), pred_dates.max()
    
    if start_date:
        analysis_start = max(pd.to_datetime(start_date), pred_min)
    else:
        analysis_start = pred_min
    
    if end_date:
        analysis_end = min(pd.to_datetime(end_date), pred_max)
    else:
        analysis_end = pred_max
    
    logger.info(f"åˆ†æåŒºé—´: {analysis_start.strftime('%Y-%m-%d')} ~ {analysis_end.strftime('%Y-%m-%d')}")
    
    # 5. åŠ è½½ç­–ç•¥å‡€å€¼æ›²çº¿
    # é‡æ–°è¿è¡Œå›æµ‹ä»¥è·å–å®Œæ•´å‡€å€¼æ•°æ®
    from src.strategy.signal import TopKSignalStrategy
    from src.backtest.backtester import VectorBacktester
    
    # åŠ¨æ€èåˆé¢„æµ‹åˆ†æ•°
    dual_head_cfg = GLOBAL_CONFIG.get("model", {}).get("dual_head", {})
    has_reg = "pred_reg" in pred_df.columns
    has_cls = "pred_cls" in pred_df.columns
    
    if has_reg and has_cls:
        reg_weight = dual_head_cfg.get("regression", {}).get("weight", 0.6)
        cls_weight = dual_head_cfg.get("classification", {}).get("weight", 0.4)
        
        def min_max_normalize(arr):
            arr = np.array(arr)
            min_val, max_val = np.nanmin(arr), np.nanmax(arr)
            if max_val - min_val > 1e-8:
                return (arr - min_val) / (max_val - min_val)
            return np.zeros_like(arr)
        
        pred_reg_norm = min_max_normalize(pred_df["pred_reg"].values)
        pred_cls_norm = min_max_normalize(pred_df["pred_cls"].values)
        pred_df["pred_score"] = reg_weight * pred_reg_norm + cls_weight * pred_cls_norm
    
    # ç”Ÿæˆä¿¡å·å¹¶å›æµ‹
    strategy = TopKSignalStrategy()
    signal_df = strategy.generate(pred_df)
    
    backtester = VectorBacktester()
    out_dir = os.path.join(model_dir, "attribution_analysis")
    os.makedirs(out_dir, exist_ok=True)
    
    metrics = backtester.run(signal_df, output_dir=out_dir, start_date=str(analysis_start.date()), end_date=str(analysis_end.date()))
    
    if "equity_curve" not in metrics:
        logger.error("å›æµ‹å¤±è´¥ï¼Œæ— æ³•è·å–å‡€å€¼æ›²çº¿")
        return
    
    equity_curve = metrics["equity_curve"]
    
    # 6. è®¡ç®—åŸºå‡†æŒ‡æ•°æ”¶ç›Š
    idx_sub = idx_df.loc[analysis_start:analysis_end, "close"]
    benchmark_curve = idx_sub / idx_sub.iloc[0]
    
    # å¯¹é½æ—¥æœŸ
    common_dates = equity_curve.index.intersection(benchmark_curve.index)
    strategy_returns = equity_curve.loc[common_dates]
    benchmark_returns = benchmark_curve.loc[common_dates]
    
    # 7. æ ¸å¿ƒå½’å› è®¡ç®—
    # æ€»æ”¶ç›Š
    total_return = strategy_returns.iloc[-1] / strategy_returns.iloc[0] - 1
    benchmark_total = benchmark_returns.iloc[-1] / benchmark_returns.iloc[0] - 1
    
    # Alpha (è¶…é¢æ”¶ç›Š)
    alpha = total_return - benchmark_total
    
    # æ—¥æ”¶ç›Šç‡
    strategy_daily = strategy_returns.pct_change().dropna()
    benchmark_daily = benchmark_returns.pct_change().dropna()
    
    # Beta è®¡ç®— (ä½¿ç”¨å›å½’)
    common_idx = strategy_daily.index.intersection(benchmark_daily.index)
    strat_ret = strategy_daily.loc[common_idx].values
    bench_ret = benchmark_daily.loc[common_idx].values
    
    if len(common_idx) > 10:
        # ç®€å•çº¿æ€§å›å½’: strategy_ret = alpha + beta * benchmark_ret
        from scipy.stats import linregress
        beta, reg_alpha, r_value, p_value, std_err = linregress(bench_ret, strat_ret)
    else:
        beta = 1.0
        r_value = 0.0
        reg_alpha = 0.0
    
    # å¹´åŒ–å¤„ç†
    trading_days = len(common_dates)
    years = trading_days / 252
    
    ann_total_return = (1 + total_return) ** (1 / years) - 1 if years > 0 else 0
    ann_benchmark = (1 + benchmark_total) ** (1 / years) - 1 if years > 0 else 0
    ann_alpha = ann_total_return - beta * ann_benchmark
    
    # 8. æ‰“å°è¯¦ç»†å½’å› ç»“æœ
    print("\n" + "=" * 70)
    print("[æ”¶ç›Šå½’å› åˆ†ææŠ¥å‘Š]")
    print("=" * 70)
    print(f"åˆ†æåŒºé—´: {analysis_start.strftime('%Y-%m-%d')} ~ {analysis_end.strftime('%Y-%m-%d')}")
    print(f"äº¤æ˜“å¤©æ•°: {trading_days} å¤© ({years:.2f} å¹´)")
    print("-" * 70)
    
    print("\n[ç»å¯¹æ”¶ç›Šåˆ†è§£]")
    print(f"  ç­–ç•¥æ€»æ”¶ç›Š:     {total_return:>10.2%}")
    print(f"  åŸºå‡†æ€»æ”¶ç›Š:     {benchmark_total:>10.2%}  (ä¸­è¯1000)")
    print(f"  ----------------------")
    print(f"  è¶…é¢æ”¶ç›Š (Alpha):   {alpha:>10.2%}  = ç­–ç•¥ - åŸºå‡†")
    
    print("\n[é£é™©è°ƒæ•´åˆ†æ]")
    print(f"  ç­–ç•¥ Beta:      {beta:>10.2f}  (ç›¸å¯¹åŸºå‡†çš„æ•æ„Ÿåº¦)")
    print(f"  R-squared:      {r_value**2:>10.2%}  (æ”¶ç›Šç”±å¸‚åœºè§£é‡Šçš„æ¯”ä¾‹)")
    
    print("\n[å¹´åŒ–æŒ‡æ ‡]")
    print(f"  ç­–ç•¥å¹´åŒ–æ”¶ç›Š:   {ann_total_return:>10.2%}")
    print(f"  åŸºå‡†å¹´åŒ–æ”¶ç›Š:   {ann_benchmark:>10.2%}")
    print(f"  å¹´åŒ– Alpha:     {ann_alpha:>10.2%}")
    
    # æ”¶ç›Šæ¥æºå½’å› 
    beta_contribution = beta * benchmark_total  # å¸‚åœºæ•å£è´¡çŒ®
    alpha_contribution = total_return - beta_contribution  # çœŸæ­£çš„é€‰è‚¡èƒ½åŠ›
    
    print("\n[æ”¶ç›Šæ¥æºå½’å› ]")
    print(f"  å¸‚åœºæ•å£è´¡çŒ® (Beta x åŸºå‡†):  {beta_contribution:>10.2%}")
    print(f"  é€‰è‚¡èƒ½åŠ›è´¡çŒ® (Alpha):        {alpha_contribution:>10.2%}")
    
    # åˆ¤æ–­ç»“è®º
    print("\n" + "=" * 70)
    print("[ç»“è®º]")
    if alpha_contribution > 0.01:  # >1%
        print(f"   [OK] æ¨¡å‹ç¡®å®äº§ç”Ÿäº† {alpha_contribution:.2%} çš„è¶…é¢æ”¶ç›Š (Alpha)")
        print(f"   * æ‰£é™¤å¤§ç›˜æ¶¨å¹…åï¼Œæ¨¡å‹ä»è´¡çŒ®äº† {alpha:.2%} çš„è¶…é¢è¡¨ç°")
    elif alpha_contribution > -0.01:  # -1% ~ 1%
        print(f"   [WARN] æ¨¡å‹è¶…é¢æ”¶ç›Šæ¥è¿‘äºé›¶ ({alpha_contribution:.2%})")
        print(f"   * ç­–ç•¥æ”¶ç›Šä¸»è¦æ¥è‡ªå¸‚åœºæ•´ä½“ä¸Šæ¶¨ï¼Œè€Œéé€‰è‚¡èƒ½åŠ›")
    else:
        print(f"   [FAIL] æ¨¡å‹äº§ç”Ÿäº†è´Ÿ Alpha ({alpha_contribution:.2%})")
        print(f"   * ç­–ç•¥è·‘è¾“åŸºå‡†ï¼Œé€‰è‚¡èƒ½åŠ›æœ‰å¾…æå‡")
    print("=" * 70)
    
    # 9. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ï¼ˆç®€åŒ–ç‰ˆï¼Œ2è¡Œ1åˆ—ï¼‰
    fig = make_subplots(
        rows=2, cols=1,
        subplot_titles=[
            "å‡€å€¼æ›²çº¿å¯¹æ¯” (ç­–ç•¥ vs ä¸­è¯1000)",
            "è¶…é¢æ”¶ç›Šèµ°åŠ¿ (ç­–ç•¥ç›¸å¯¹åŸºå‡†)"
        ],
        row_heights=[0.5, 0.5],
        vertical_spacing=0.12
    )
    
    # å›¾1: å‡€å€¼æ›²çº¿å¯¹æ¯”
    fig.add_trace(
        go.Scatter(x=strategy_returns.index, y=strategy_returns.values, 
                   name="ç­–ç•¥å‡€å€¼", line=dict(color="red", width=2)),
        row=1, col=1
    )
    fig.add_trace(
        go.Scatter(x=benchmark_returns.index, y=benchmark_returns.values, 
                   name="åŸºå‡†å‡€å€¼ (ä¸­è¯1000)", line=dict(color="gray", width=2, dash="dash")),
        row=1, col=1
    )
    
    # å›¾2: è¶…é¢æ”¶ç›Šèµ°åŠ¿
    excess_curve = strategy_returns / benchmark_returns
    excess_values = excess_curve.values - 1
    
    # ä½¿ç”¨é¢œè‰²åŒºåˆ†æ­£è´Ÿè¶…é¢
    colors = ['green' if v >= 0 else 'red' for v in excess_values]
    fig.add_trace(
        go.Scatter(x=excess_curve.index, y=excess_values, 
                   name="è¶…é¢æ”¶ç›Šç‡", fill='tozeroy', 
                   fillcolor='rgba(0,128,0,0.3)',
                   line=dict(color='green', width=1.5)),
        row=2, col=1
    )
    fig.add_hline(y=0, line_dash="dash", line_color="gray", row=2, col=1)
    
    # æ·»åŠ æ”¶ç›Šå½’å› æ³¨é‡Š
    annotation_text = f"""
    <b>æ”¶ç›Šå½’å› :</b><br>
    å¸‚åœºæ•å£è´¡çŒ® (Î²Ã—åŸºå‡†): {beta_contribution:.2%}<br>
    é€‰è‚¡èƒ½åŠ›è´¡çŒ® (Î±): {alpha_contribution:.2%}
    """
    fig.add_annotation(
        x=0.98, y=0.02, xref="paper", yref="paper",
        text=annotation_text,
        showarrow=False,
        align="right",
        bgcolor="rgba(255,255,255,0.8)",
        bordercolor="gray",
        borderwidth=1,
        font=dict(size=11)
    )
    
    # æ›´æ–°å¸ƒå±€
    fig.update_layout(
        title=dict(
            text=f"æ”¶ç›Šå½’å› åˆ†æ | ç­–ç•¥æ”¶ç›Š: {total_return:.2%} | åŸºå‡†æ”¶ç›Š: {benchmark_total:.2%} | è¶…é¢æ”¶ç›Š(Î±): {alpha:.2%}",
            font=dict(size=14)
        ),
        height=600,
        showlegend=True,
        legend=dict(orientation="h", yanchor="bottom", y=-0.12, x=0.5, xanchor="center")
    )
    
    # æ›´æ–°åæ ‡è½´æ ‡é¢˜
    fig.update_xaxes(title_text="æ—¥æœŸ", row=1, col=1)
    fig.update_yaxes(title_text="å‡€å€¼", row=1, col=1)
    fig.update_xaxes(title_text="æ—¥æœŸ", row=2, col=1)
    fig.update_yaxes(title_text="è¶…é¢æ”¶ç›Šç‡", tickformat=".1%", row=2, col=1)
    
    # ä¿å­˜å›¾è¡¨
    output_path = os.path.join(out_dir, "return_attribution.png")
    fig.write_image(output_path, width=1200, height=600, scale=2)
    fig.write_html(os.path.join(out_dir, "return_attribution.html"))
    
    logger.info(f"å½’å› åˆ†æå›¾è¡¨å·²ä¿å­˜è‡³: {output_path}")
    
    # è¿”å›è¯¦ç»†ç»“æœ
    return {
        "total_return": total_return,
        "benchmark_return": benchmark_total,
        "alpha": alpha,
        "beta": beta,
        "r_squared": r_value ** 2,
        "ann_total_return": ann_total_return,
        "ann_benchmark": ann_benchmark,
        "ann_alpha": ann_alpha,
        "beta_contribution": beta_contribution,
        "alpha_contribution": alpha_contribution,
        "trading_days": trading_days
    }



def analyze_random_periods(
    model_version: str = None,
    samples: int = 200,
    duration_days: int = 35
):
    """
    éšæœºå‘¨æœŸåˆ†æï¼šç»Ÿè®¡ç­–ç•¥åœ¨éšæœºæŠ½å–çš„å›ºå®šæ—¶é•¿çª—å£ä¸‹çš„èƒœç‡
    
    å‚æ•°:
    - samples: æŠ½æ ·æ¬¡æ•°
    - duration_days: æ¯ä¸ªçª—å£çš„äº¤æ˜“æ—¥æ•°é‡ (35äº¤æ˜“æ—¥çº¦ç­‰äº1.5ä¸ªæœˆ)
    """
    logger.info(f"=== å¼€å§‹éšæœºå‘¨æœŸåˆ†æ (æŠ½æ ·: {samples}æ¬¡, çª—å£: {duration_days}å¤©) ===")
    
    # 1. å‡†å¤‡æ•°æ®
    models_dir = GLOBAL_CONFIG["paths"]["models"]
    if model_version is None:
        subdirs = [d for d in os.listdir(models_dir) if os.path.isdir(os.path.join(models_dir, d))]
        subdirs.sort(reverse=True)
        model_version = subdirs[0] if subdirs else None
        
    model_dir = os.path.join(models_dir, model_version)
    pred_path = os.path.join(model_dir, "predictions.parquet")
    idx_code = GLOBAL_CONFIG.get("preprocessing", {}).get("labels", {}).get("index_code", "000852.SH")
    idx_file = os.path.join(GLOBAL_CONFIG["paths"]["data_raw"], f"index_{idx_code.replace('.', '')}.parquet")
    
    pred_df = read_parquet(pred_path)
    pred_df["date"] = pd.to_datetime(pred_df["date"])
    
    idx_df = read_parquet(idx_file)
    idx_df["date"] = pd.to_datetime(idx_df["date"])
    idx_df = idx_df.set_index("date").sort_index()
    
    # 2. è¿è¡Œå…¨é‡å›æµ‹è·å–æ¯æ—¥å‡€å€¼
    from src.strategy.signal import TopKSignalStrategy
    from src.backtest.backtester import VectorBacktester
    
    # ç®€å•çš„èåˆé€»è¾‘ (å¦‚æœå·²å­˜åœ¨èåˆåˆ—åˆ™è·³è¿‡)
    if "pred_score" not in pred_df.columns:
        pred_df["pred_score"] = pred_df["pred_reg"] # ç®€åŒ–å‡è®¾
        
    strategy = TopKSignalStrategy()
    signal_df = strategy.generate(pred_df)
    backtester = VectorBacktester()
    
    # ä¸ºäº†é€Ÿåº¦ï¼Œä¸ç”»å›¾ï¼Œåªè·å–æ•°æ®
    # åˆ›å»ºä¸´æ—¶è¾“å‡ºç›®å½•
    import tempfile
    with tempfile.TemporaryDirectory() as tmp_dir:
        metrics = backtester.run(signal_df, output_dir=tmp_dir)
        
    equity_curve = metrics["equity_curve"]
    # ç¡®ä¿å’ŒæŒ‡æ•°æ—¥æœŸå¯¹é½
    common_dates = equity_curve.index.intersection(idx_df.index)
    strat_nav = equity_curve.loc[common_dates]
    bench_nav = idx_df.loc[common_dates, "close"]
    
    # 3. éšæœºæŠ½æ ·
    # æœ‰æ•ˆèµ·å§‹ç‚¹ï¼š0 åˆ° len - duration
    total_days = len(common_dates)
    if total_days < duration_days:
        logger.error("æ•°æ®é•¿åº¦ä¸è¶³")
        return
        
    valid_starts = np.arange(total_days - duration_days)
    # éšæœºé€‰æ‹©èµ·å§‹ç‚¹
    chosen_starts = np.random.choice(valid_starts, size=samples, replace=True)
    
    results = []
    
    for start_idx in tqdm(chosen_starts, desc="åˆ†æéšæœºçª—å£"):
        end_idx = start_idx + duration_days
        
        # çª—å£æœŸæ•°æ®
        s_start = strat_nav.iloc[start_idx]
        s_end = strat_nav.iloc[end_idx]
        b_start = bench_nav.iloc[start_idx]
        b_end = bench_nav.iloc[end_idx]
        
        strat_ret = s_end / s_start - 1
        bench_ret = b_end / b_start - 1
        alpha = strat_ret - bench_ret
        
        start_date = common_dates[start_idx]
        
        results.append({
            "start_date": start_date,
            "strat_ret": strat_ret,
            "bench_ret": bench_ret,
            "alpha": alpha,
            "win": alpha > 0
        })
        
    df_res = pd.DataFrame(results)
    
    # 4. ç»Ÿè®¡ç»“æœ
    win_rate = df_res["win"].mean()
    avg_alpha = df_res["alpha"].mean()
    median_alpha = df_res["alpha"].median()
    
    print("\n" + "="*60)
    print(f"ğŸ² éšæœºå‘¨æœŸåˆ†æç»“æœ (åŸºäºè¿‡å» {len(common_dates)} ä¸ªäº¤æ˜“æ—¥)")
    print("="*60)
    print(f"æŠ½æ ·å‚æ•°: {samples} æ¬¡æµ‹è¯•, æ¯æ¬¡æŒä»“ {duration_days} å¤© (çº¦1.5ä¸ªæœˆ)")
    print("-" * 60)
    print(f"ğŸ† èƒœç‡ (è·‘èµ¢åŸºå‡†):     {win_rate:>8.2%}")
    print(f"ğŸ’° å¹³å‡è¶…é¢æ”¶ç›Š (Mean): {avg_alpha:>8.2%}")
    print(f"ğŸ“Š ä¸­ä½æ•°è¶…é¢ (Median): {median_alpha:>8.2%}")
    print(f"ğŸ“ˆ æœ€å¥½è¡¨ç°:            {df_res['alpha'].max():>8.2%}")
    print(f"ğŸ“‰ æœ€å·®è¡¨ç°:            {df_res['alpha'].min():>8.2%}")
    print("="*60)
    
    # ä¿å­˜ç»Ÿè®¡ç»“æœåˆ° CSV
    summary_path = os.path.join(model_dir, "attribution_analysis", "random_analysis_summary.csv")
    df_res.to_csv(os.path.join(model_dir, "attribution_analysis", "random_analysis_details.csv"), index=False)
    
    summary_data = {
        "timestamp": [pd.Timestamp.now()],
        "samples": [samples],
        "duration_days": [duration_days],
        "win_rate": [win_rate],
        "avg_alpha": [avg_alpha],
        "median_alpha": [median_alpha],
        "max_alpha": [df_res['alpha'].max()],
        "min_alpha": [df_res['alpha'].min()]
    }
    pd.DataFrame(summary_data).to_csv(summary_path, index=False)
    logger.info(f"è¯¦ç»†ç»Ÿè®¡å·²ä¿å­˜: {summary_path}")

    # ç”Ÿæˆç»¼åˆåˆ†æå›¾è¡¨
    try:
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=[
                "è¶…é¢æ”¶ç›Š(Alpha)åˆ†å¸ƒç›´æ–¹å›¾", "ç­–ç•¥æ”¶ç›Š vs åŸºå‡†æ”¶ç›Šæ•£ç‚¹å›¾",
                "Alphaéšèµ·å§‹æ—¶é—´å˜åŒ–è¶‹åŠ¿", "ä¸åŒå¹´ä»½çš„å¹³å‡èƒœç‡heatmap(å¦‚æœ‰)"
            ],
            specs=[[{"type": "histogram"}, {"type": "scatter"}],
                   [{"type": "scatter"}, {"type": "table"}]]
        )
        
        # 1. Alpha åˆ†å¸ƒç›´æ–¹å›¾
        fig.add_trace(
            go.Histogram(x=df_res["alpha"], nbinsx=30, name="Alphaåˆ†å¸ƒ", marker_color='blue', opacity=0.7),
            row=1, col=1
        )
        fig.add_vline(x=0, line_dash="dash", line_color="red", annotation_text="0%", row=1, col=1)
        # fig.add_vline(x=-0.0607, line_dash="dash", line_color="green", annotation_text="å½“å‰å®ç›˜", row=1, col=1)

        # 2. ç­–ç•¥ vs åŸºå‡† æ•£ç‚¹å›¾
        fig.add_trace(
            go.Scatter(
                x=df_res["bench_ret"], 
                y=df_res["strat_ret"], 
                mode='markers',
                marker=dict(
                    color=df_res["alpha"], 
                    colorscale='RdBu', 
                    showscale=True,
                    colorbar=dict(title="Alpha", len=0.4, y=0.8)
                ),
                text=[f"æ—¶é—´: {d.date()}<br>Alpha: {a:.2%}" for d, a in zip(df_res["start_date"], df_res["alpha"])],
                name="æ ·æœ¬ç‚¹"
            ),
            row=1, col=2
        )
        # æ·»åŠ  y=x å‚è€ƒçº¿
        min_ret = min(df_res["bench_ret"].min(), df_res["strat_ret"].min())
        max_ret = max(df_res["bench_ret"].max(), df_res["strat_ret"].max())
        fig.add_trace(
            go.Scatter(x=[min_ret, max_ret], y=[min_ret, max_ret], mode='lines', line=dict(color='gray', dash='dash'), name="è·‘å¹³åŸºå‡†"),
            row=1, col=2
        )

        # 3. Alpha éšæ—¶é—´å˜åŒ–
        df_sorted = df_res.sort_values("start_date")
        fig.add_trace(
            go.Scatter(x=df_sorted["start_date"], y=df_sorted["alpha"], mode='lines', name="Alphaè¶‹åŠ¿", line=dict(width=1)),
            row=2, col=1
        )
        fig.add_hline(y=0, line_dash="dash", line_color="red", row=2, col=1)

        # 4. ç»Ÿè®¡è¡¨æ ¼
        fig.add_trace(
            go.Table(
                header=dict(values=["æŒ‡æ ‡", "æ•°å€¼"], fill_color='paleturquoise', align='left'),
                cells=dict(values=[
                    ["æŠ½æ ·æ¬¡æ•°", "èƒœç‡ (Win Rate)", "å¹³å‡ Alpha", "ä¸­ä½æ•° Alpha", "æœ€å¤§ Alpha", "æœ€å° Alpha"],
                    [
                        f"{samples}",
                        f"{win_rate:.2%}",
                        f"{avg_alpha:.2%}",
                        f"{median_alpha:.2%}",
                        f"{df_res['alpha'].max():.2%}",
                        f"{df_res['alpha'].min():.2%}"
                    ]
                ], fill_color='lavender', align='left')
            ),
            row=2, col=2
        )

        fig.update_layout(
            title_text=f"éšæœºå‘¨æœŸåˆ†ææŠ¥å‘Š (çª—å£={duration_days}äº¤æ˜“æ—¥, æ ·æœ¬={samples})",
            height=900,
            showlegend=False
        )
        
        # åæ ‡è½´æ ‡ç­¾
        fig.update_xaxes(title_text="Alpha", tickformat=".1%", row=1, col=1)
        fig.update_yaxes(title_text="é¢‘æ¬¡", row=1, col=1)
        
        fig.update_xaxes(title_text="åŸºå‡†æ”¶ç›Š", tickformat=".1%", row=1, col=2)
        fig.update_yaxes(title_text="ç­–ç•¥æ”¶ç›Š", tickformat=".1%", row=1, col=2)
        
        fig.update_xaxes(title_text="èµ·å§‹æ—¥æœŸ", row=2, col=1)
        fig.update_yaxes(title_text="Alpha", tickformat=".1%", row=2, col=1)

        out_dir = os.path.join(model_dir, "attribution_analysis")
        os.makedirs(out_dir, exist_ok=True)
        
        # ä¿å­˜ HTML (äº¤äº’å¼)
        out_path_html = os.path.join(out_dir, "random_analysis.html")
        fig.write_html(out_path_html)
        
        # ä¿å­˜ PNG (é™æ€)
        out_path_png = os.path.join(out_dir, "random_analysis.png")
        fig.write_image(out_path_png, scale=2)
        
        logger.info(f"éšæœºåˆ†æå›¾è¡¨å·²ä¿å­˜:\n  HTML: {out_path_html}\n  PNG:  {out_path_png}")
    except Exception as e:
        logger.error(f"ç”Ÿæˆéšæœºåˆ†æå›¾è¡¨å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    import argparse
    from tqdm import tqdm
    
    parser = argparse.ArgumentParser(description="æ”¶ç›Šå½’å› åˆ†æå·¥å…·")
    parser.add_argument("--mode", type=str, default="all", choices=["all", "real", "random"], help="åˆ†ææ¨¡å¼: all(å…¨éƒ¨), real(ä»…å®ç›˜), random(ä»…éšæœºéªŒè¯)")
    parser.add_argument("--start_date", type=str, default="2025-11-27", help="å®ç›˜åˆ†æå¼€å§‹æ—¥æœŸ (YYYY-MM-DD)")
    parser.add_argument("--samples", type=int, default=500, help="éšæœºéªŒè¯æŠ½æ ·æ¬¡æ•°")
    parser.add_argument("--duration", type=int, default=35, help="éšæœºéªŒè¯æŒä»“å¤©æ•°")
    
    args = parser.parse_args()
    
    # 1. åˆ†æå½“å‰å®ç›˜å‘¨æœŸ
    if args.mode in ["all", "real"]:
        print(f"\n>>> åˆ†æ 1: å½“å‰å®ç›˜å‘¨æœŸ ({args.start_date} ~ ä»Š)")
        analyze_return_attribution(
            start_date=args.start_date,
            end_date=None 
        )
    
    # 2. éšæœºå‘¨æœŸéªŒè¯
    if args.mode in ["all", "random"]:
        print("\n>>> åˆ†æ 2: å†å²éšæœºå‘¨æœŸéªŒè¯ (éªŒè¯ç­–ç•¥ç¨³å¥æ€§)")
        analyze_random_periods(
            samples=args.samples,
            duration_days=args.duration
        )
