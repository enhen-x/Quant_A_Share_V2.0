# scripts/update_data.py

import os
import sys
import argparse
import datetime
import pandas as pd
import subprocess
from tqdm import tqdm

# è·¯å¾„é€‚é…
current_dir = os.path.dirname(os.path.abspath(__file__))
# ä»å½“å‰æ–‡ä»¶ä½ç½® (scripts/date_landing) è¿”å›ä¸¤çº§åˆ°é¡¹ç›®æ ¹ç›®å½•
project_root = os.path.dirname(os.path.dirname(current_dir))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from src.data_source.datahub import DataHub
from src.utils.config import GLOBAL_CONFIG
from src.utils.io import read_parquet, save_parquet, ensure_dir
from src.utils.logger import get_logger
import glob

logger = get_logger()

class DataUpdater:
    def __init__(self):
        self.config = GLOBAL_CONFIG
        self.paths = self.config["paths"]
        self.datahub = DataHub()
        
        # ä»Šå¤©çš„æ—¥æœŸ
        self.today = datetime.datetime.now().strftime("%Y-%m-%d")
        
        # åŠ è½½æœ¬åœ°äº¤æ˜“æ—¥å† (ç”¨äºåˆ¤æ–­æ˜¯å¦éœ€è¦æ›´æ–°)
        self.calendar_path = os.path.join(self.paths["data_meta"], "trade_calendar.parquet")
        self.trade_dates = []
        self._load_local_calendar()

    def _load_local_calendar(self):
        """åŠ è½½æœ¬åœ°æ—¥å†ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆå§‹åŒ–ä¸ºç©º"""
        if os.path.exists(self.calendar_path):
            df = read_parquet(self.calendar_path)
            self.trade_dates = pd.to_datetime(df["date"]).dt.date.tolist()
            self.trade_dates.sort()
        else:
            self.trade_dates = []

    def get_last_date(self, df: pd.DataFrame) -> str:
        """è·å– DataFrame ä¸­çš„æœ€åæ—¥æœŸ"""
        if df is None or df.empty or "date" not in df.columns:
            return None
        return df["date"].max().strftime("%Y-%m-%d")

    def get_next_date(self, date_str: str) -> str:
        """ç»™å®šæ—¥æœŸï¼Œè¿”å›ä¸‹ä¸€å¤©"""
        if not date_str:
            return self.config["data"]["start_date"]
        
        dt = datetime.datetime.strptime(date_str, "%Y-%m-%d")
        next_dt = dt + datetime.timedelta(days=1)
        return next_dt.strftime("%Y-%m-%d")

    # ==========================================
    # 1. æ›´æ–°äº¤æ˜“æ—¥å†
    # ==========================================
    def update_calendar(self):
        logger.info(">>> æ­¥éª¤ 1/3: æ£€æŸ¥å¹¶æ›´æ–°äº¤æ˜“æ—¥å†...")
        
        try:
            # è·å–èŒƒå›´ï¼šä»é…ç½®å¼€å§‹æ—¥æœŸ åˆ° æœªæ¥ä¸€å¹´
            start_date = self.config["data"]["start_date"]
            future_date = (datetime.datetime.now() + datetime.timedelta(days=365)).strftime("%Y-%m-%d")
            
            df_cal = self.datahub.get_trade_calendar(start_date, future_date)
            
            if not df_cal.empty:
                save_parquet(df_cal, self.calendar_path)
                # åˆ·æ–°å†…å­˜ä¸­çš„æ—¥å†
                self._load_local_calendar()
                logger.info(f"äº¤æ˜“æ—¥å†å·²æ›´æ–°ï¼Œæœ€æ–°æ—¥æœŸè¦†ç›–è‡³: {self.get_last_date(df_cal)}")
            else:
                logger.warning("äº¤æ˜“æ—¥å†æ¥å£æœªè¿”å›æ•°æ®ï¼Œè·³è¿‡æ›´æ–°ã€‚")
        except Exception as e:
            logger.error(f"æ›´æ–°äº¤æ˜“æ—¥å†å¤±è´¥: {e}")

    # ==========================================
    # 2. æ›´æ–°æŒ‡æ•°
    # ==========================================
    def update_index(self):
        index_code = self.config["preprocessing"]["labels"]["index_code"]
        logger.info(f">>> æ­¥éª¤ 2/3: æ›´æ–°åŸºå‡†æŒ‡æ•° ({index_code})...")
        
        file_name = f"index_{index_code.replace('.', '')}.parquet"
        file_path = os.path.join(self.paths["data_raw"], file_name)
        
        df_local = pd.DataFrame()
        start_fetch_date = self.config["data"]["start_date"]
        
        # 1. è¯»å–æœ¬åœ°
        if os.path.exists(file_path):
            df_local = read_parquet(file_path)
            last_date = self.get_last_date(df_local)
            if last_date:
                # å¦‚æœæœ¬åœ°æœ€æ–°æ—¥æœŸ >= ä»Šå¤©ï¼Œè¯´æ˜ä¸ç”¨æ›´æ–°
                if last_date >= self.today:
                    logger.info(f"æŒ‡æ•° {index_code} å·²æ˜¯æœ€æ–° ({last_date})ï¼Œæ— éœ€æ›´æ–°ã€‚")
                    return
                start_fetch_date = self.get_next_date(last_date)
        
        # 2. ä¸‹è½½å¢é‡
        logger.info(f"æ­£åœ¨ä¸‹è½½æŒ‡æ•°å¢é‡æ•°æ®: {start_fetch_date} -> {self.today}")
        df_new = self.datahub.fetch_index_price(index_code, start_date=start_fetch_date, end_date=self.today) 
        
        if not df_new.empty:
            df_new["date"] = pd.to_datetime(df_new["date"])
            
            # 3. åˆå¹¶
            if not df_local.empty:
                df_final = pd.concat([df_local, df_new], axis=0)
                df_final = df_final.drop_duplicates(subset=["date"]).sort_values("date")
            else:
                df_final = df_new
            
            save_parquet(df_final, file_path)
            logger.info(f"æŒ‡æ•°æ›´æ–°å®Œæˆï¼Œæ–°å¢ {len(df_new)} æ¡è®°å½•ã€‚")
        else:
            logger.info("æœªå‘ç°æ–°çš„æŒ‡æ•°äº¤æ˜“æ•°æ®æˆ–æ•°æ®ä¸‹è½½å¤±è´¥ã€‚")

    # ==========================================
    # 3. æ›´æ–°ä¸ªè‚¡
    # ==========================================
    def update_stocks(self):
        logger.info(">>> æ­¥éª¤ 3/3: å¢é‡æ›´æ–°ä¸ªè‚¡æ•°æ®...")
        
        meta_path = os.path.join(self.paths["data_meta"], "all_stocks_meta.parquet")
        if not os.path.exists(meta_path):
            logger.error("å…ƒæ•°æ®ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ init_stock_pool.py")
            return
            
        raw_dir = self.paths["data_raw"]
        # ä»…æ›´æ–° data/raw ä¸‹å·²æœ‰çš„æ–‡ä»¶
        existing_files = [f for f in os.listdir(raw_dir) if f.endswith(".parquet") and f[0].isdigit()]
        
        if not existing_files:
            logger.warning("data/raw ä¸‹æ²¡æœ‰ä»»ä½•è‚¡ç¥¨æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ download_data.py è¿›è¡Œé¦–æ¬¡ä¸‹è½½ã€‚")
            return
            
        update_count = 0
        skip_count = 0
        
        # è·å–æœ€æ–°çš„å¸‚åœºäº¤æ˜“æ—¥
        if self.trade_dates:
            market_last_date = self.trade_dates[-1] 
        else:
            market_last_date = datetime.date.today()

        pbar = tqdm(existing_files, desc="Updating Stocks")
        
        for file_name in pbar:
            symbol = file_name.replace(".parquet", "")
            file_path = os.path.join(raw_dir, file_name)
            
            try:
                # 1. è¯»å–æœ¬åœ°æœ€åä¸€è¡Œ
                df_local = read_parquet(file_path)
                last_date_str = self.get_last_date(df_local)
                
                if not last_date_str:
                    start_date = self.config["data"]["start_date"]
                else:
                    last_date = datetime.datetime.strptime(last_date_str, "%Y-%m-%d").date()
                    
                    # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯æœ€æ–°
                    if last_date >= market_last_date:
                        skip_count += 1
                        continue
                        
                    start_date = self.get_next_date(last_date_str)

                # ä¸ºäº†é˜²æ­¢ start_date > end_date æŠ¥é”™
                if start_date > self.today:
                    skip_count += 1
                    continue
                    
                df_new = self.datahub.fetch_price(symbol, start_date=start_date, end_date=self.today)
                
                if not df_new.empty:
                    # åˆå¹¶ä¸å»é‡
                    df_final = pd.concat([df_local, df_new], axis=0)
                    df_final = df_final.drop_duplicates(subset=["date"], keep="last")
                    df_final = df_final.sort_values("date").reset_index(drop=True)
                    
                    save_parquet(df_final, file_path)
                    update_count += 1
                else:
                    skip_count += 1
                    
                pbar.set_postfix({"Upd": update_count, "Skip": skip_count})
                
            except Exception as e:
                logger.error(f"æ›´æ–° {symbol} å¤±è´¥: {e}")
        
        logger.info(f"æ›´æ–°å®Œæˆã€‚å·²æ›´æ–°: {update_count}, è·³è¿‡(æ— éœ€æ›´æ–°/åœç‰Œ): {skip_count}")

def verify_data_freshness(step_name, data_dir, file_pattern="*.parquet", single_file=None):
    """
    éªŒè¯æ•°æ®ç›®å½•æˆ–å•ä¸ªæ–‡ä»¶ä¸­çš„æœ€æ–°æ—¥æœŸ
    :param step_name: æ­¥éª¤åç§°
    :param data_dir: æ•°æ®ç›®å½• (ç›¸å¯¹äº project_root)
    :param file_pattern: æ–‡ä»¶åŒ¹é…æ¨¡å¼
    :param single_file: å¦‚æœæŒ‡å®šï¼Œåªæ£€æŸ¥è¯¥å•æ–‡ä»¶ (ç›¸å¯¹äº project_root)
    """
    logger.info(f"\nğŸ“… [{step_name}] æ•°æ®æ–°é²œåº¦æ£€æŸ¥:")
    
    try:
        if single_file:
            # æ£€æŸ¥å•ä¸ªæ–‡ä»¶
            file_path = os.path.join(project_root, single_file)
            if not os.path.exists(file_path):
                logger.warning(f"   æ–‡ä»¶ä¸å­˜åœ¨: {single_file}")
                return
            df = read_parquet(file_path)
            if df is not None and not df.empty and "date" in df.columns:
                df["date"] = pd.to_datetime(df["date"])
                latest = df["date"].max()
                earliest = df["date"].min()
                n_dates = df["date"].nunique()
                logger.info(f"   ğŸ“„ {os.path.basename(single_file)}")
                logger.info(f"      æ—¥æœŸèŒƒå›´: {earliest.strftime('%Y-%m-%d')} ~ {latest.strftime('%Y-%m-%d')} ({n_dates} ä¸ªäº¤æ˜“æ—¥)")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ label åˆ—ï¼Œç»Ÿè®¡å…¶ NaN æƒ…å†µ
                if "label" in df.columns:
                    label_valid = df["label"].notna().sum()
                    label_nan = df["label"].isna().sum()
                    label_latest = df[df["label"].notna()]["date"].max() if label_valid > 0 else None
                    logger.info(f"      æ ‡ç­¾(label): æœ‰æ•ˆ={label_valid:,}, NaN={label_nan:,}")
                    if label_latest:
                        logger.info(f"      æ ‡ç­¾æœ€æ–°æœ‰æ•ˆæ—¥æœŸ: {label_latest.strftime('%Y-%m-%d')}")
                    else:
                        logger.warning(f"      âš ï¸ æ ‡ç­¾å…¨éƒ¨ä¸º NaN!")
                
                # æ£€æŸ¥ feat_ åˆ—æƒ…å†µ
                feat_cols = [c for c in df.columns if c.startswith("feat_")]
                if feat_cols:
                    feat_latest = df.dropna(subset=feat_cols[:3])["date"].max() if not df.dropna(subset=feat_cols[:3]).empty else None
                    if feat_latest:
                        logger.info(f"      ç‰¹å¾æœ€æ–°æœ‰æ•ˆæ—¥æœŸ: {feat_latest.strftime('%Y-%m-%d')} ({len(feat_cols)} ä¸ªç‰¹å¾åˆ—)")
            else:
                logger.warning(f"   æ–‡ä»¶ä¸ºç©ºæˆ–ç¼ºå°‘ date åˆ—: {single_file}")
            return

        # æ£€æŸ¥ç›®å½•ä¸‹çš„æ–‡ä»¶
        dir_path = os.path.join(project_root, data_dir)
        if not os.path.exists(dir_path):
            logger.warning(f"   ç›®å½•ä¸å­˜åœ¨: {data_dir}")
            return
        
        files = [f for f in os.listdir(dir_path) if f.endswith(".parquet") and f[0].isdigit()]
        if not files:
            logger.warning(f"   ç›®å½•ä¸‹æ²¡æœ‰è‚¡ç¥¨æ•°æ®æ–‡ä»¶: {data_dir}")
            return
        
        # éšæœºæŠ½æ ·å‡ åªè‚¡ç¥¨æ£€æŸ¥
        import random
        sample_files = random.sample(files, min(5, len(files)))
        latest_dates = []
        
        for f in sample_files:
            fp = os.path.join(dir_path, f)
            df = read_parquet(fp)
            if df is not None and not df.empty and "date" in df.columns:
                df["date"] = pd.to_datetime(df["date"])
                latest_dates.append((f.replace(".parquet", ""), df["date"].max()))
        
        if latest_dates:
            overall_max = max(d for _, d in latest_dates)
            overall_min = min(d for _, d in latest_dates)
            logger.info(f"   å…± {len(files)} åªè‚¡ç¥¨, æŠ½æ · {len(sample_files)} åª:")
            for sym, dt in latest_dates:
                logger.info(f"      {sym}: æœ€æ–°æ—¥æœŸ {dt.strftime('%Y-%m-%d')}")
            logger.info(f"   ğŸ“Š æŠ½æ ·æœ€æ–°æ—¥æœŸèŒƒå›´: {overall_min.strftime('%Y-%m-%d')} ~ {overall_max.strftime('%Y-%m-%d')}")
        else:
            logger.warning(f"   æŠ½æ ·æ–‡ä»¶å‡æ— æœ‰æ•ˆæ—¥æœŸæ•°æ®")
    except Exception as e:
        logger.error(f"   æ•°æ®æ–°é²œåº¦æ£€æŸ¥å¤±è´¥: {e}")


def run_external_script(script_rel_path, step_name):
    """
    è°ƒç”¨å¤–éƒ¨ Python è„šæœ¬
    :param script_rel_path: ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•çš„è„šæœ¬è·¯å¾„ (å¦‚ scripts/analisis/clean_and_check.py)
    :param step_name: æ­¥éª¤åç§°
    """
    script_path = os.path.join(project_root, script_rel_path)
    
    logger.info("\n" + "="*60)
    logger.info(f"ğŸš€ æ­£åœ¨å¯åŠ¨: {step_name} ...")
    logger.info(f"   è„šæœ¬è·¯å¾„: {script_path}")
    logger.info("="*60)
    
    if not os.path.exists(script_path):
        logger.error(f"âŒ æ‰¾ä¸åˆ°è„šæœ¬æ–‡ä»¶: {script_path}")
        return False
        
    try:
        # ä½¿ç”¨å½“å‰ Python è§£é‡Šå™¨æ‰§è¡Œ
        cmd = [sys.executable, script_path]
        # cwd è®¾ç½®ä¸º project_root ç¡®ä¿è„šæœ¬å†…éƒ¨ç›¸å¯¹è·¯å¾„é€»è¾‘æ­£å¸¸
        result = subprocess.run(cmd, cwd=project_root)
        
        if result.returncode == 0:
            logger.info(f"âœ… {step_name} æ‰§è¡ŒæˆåŠŸã€‚")
            return True
        else:
            logger.error(f"âŒ {step_name} æ‰§è¡Œå¤±è´¥ï¼Œè¿”å›ç : {result.returncode}")
            return False
    except Exception as e:
        logger.error(f"âŒ {step_name} å‘ç”Ÿå¼‚å¸¸: {e}")
        return False

def main():
    parser = argparse.ArgumentParser(description="å¢é‡æ›´æ–°æœ¬åœ°æ•°æ®å¹¶è¿è¡Œå…¨æµç¨‹")
    parser.parse_args()
    
    # === 1. æ›´æ–°æ•°æ® (Download) ===
    try:
        updater = DataUpdater()
        updater.update_calendar()
        updater.update_index()
        updater.update_stocks()
    except Exception as e:
        logger.error(f"æ•°æ®æ›´æ–°é˜¶æ®µå‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        return
    
    # âœ… æ­¥éª¤1å®Œæˆ - éªŒè¯åŸå§‹æ•°æ®æ–°é²œåº¦
    verify_data_freshness("æ­¥éª¤1: æ•°æ®ä¸‹è½½", GLOBAL_CONFIG["paths"]["data_raw"])

    # === 2. æ¸…æ´—æ•°æ® (Clean) ===
    # è„šæœ¬: scripts/analisis/clean_and_check.py
    if not run_external_script(os.path.join("scripts", "analisis", "clean_and_check.py"), "æ•°æ®æ¸…æ´— (Clean)"):
        logger.warning("æµç¨‹ä¸­æ–­ï¼šæ•°æ®æ¸…æ´—å¤±è´¥ã€‚")
        return
    
    # âœ… æ­¥éª¤2å®Œæˆ - éªŒè¯æ¸…æ´—åæ•°æ®æ–°é²œåº¦
    verify_data_freshness("æ­¥éª¤2: æ•°æ®æ¸…æ´—", GLOBAL_CONFIG["paths"]["data_cleaned"])

    # === 3. æ„å»ºç‰¹å¾ (Feature Engineering) ===
    # è„šæœ¬: scripts/feature_create/rebuild_features.py
    if not run_external_script(os.path.join("scripts", "feature_create", "rebuild_features.py"), "ç‰¹å¾å·¥ç¨‹ (Features)"):
        logger.warning("æµç¨‹ä¸­æ–­ï¼šç‰¹å¾æ„å»ºå¤±è´¥ã€‚")
        return
    
    # âœ… æ­¥éª¤3å®Œæˆ - éªŒè¯ç‰¹å¾æ•°æ®æ–°é²œåº¦ï¼ˆå«æ ‡ç­¾æ£€æŸ¥ï¼‰
    concat_file = GLOBAL_CONFIG.get("preprocessing", {}).get("batch", {}).get("concat_file", "all_stocks.parquet")
    verify_data_freshness("æ­¥éª¤3: ç‰¹å¾å·¥ç¨‹", None, 
                          single_file=os.path.join(GLOBAL_CONFIG["paths"]["data_processed"], concat_file))

    # === 4. æ¯æ—¥æ¨è (Recommendation) ===
    # è„šæœ¬: scripts/back_test/run_recommendation.py
    # æ¨èæœ€è¿‘ N ä¸ªäº¤æ˜“æ—¥ç”± config/main.yaml -> strategy.recommend_history_days æ§åˆ¶
    if not run_external_script(os.path.join("scripts", "back_test", "run_recommendation.py"), "ç­–ç•¥æ¨è (Recommendation)"):
        logger.warning("æµç¨‹ä¸­æ–­ï¼šæ¨èç”Ÿæˆå¤±è´¥ã€‚")
        return
    
    # âœ… æ­¥éª¤4å®Œæˆ - éªŒè¯æ¨èç»“æœ
    picks_dir = os.path.join(GLOBAL_CONFIG["paths"]["reports"], "daily_picks")
    picks_abs = os.path.join(project_root, picks_dir)
    if os.path.exists(picks_abs):
        csv_files = sorted(glob.glob(os.path.join(picks_abs, "picks_*.csv")))
        if csv_files:
            latest_pick = csv_files[-1]
            logger.info(f"\nğŸ“… [æ­¥éª¤4: ç­–ç•¥æ¨è] æ•°æ®æ–°é²œåº¦æ£€æŸ¥:")
            logger.info(f"   ğŸ“„ æœ€æ–°æ¨èæ–‡ä»¶: {os.path.basename(latest_pick)}")
            try:
                df_pick = pd.read_csv(latest_pick)
                logger.info(f"   æ¨èè‚¡ç¥¨æ•°: {len(df_pick)}")
                if "symbol" in df_pick.columns:
                    logger.info(f"   æ¨èåˆ—è¡¨: {', '.join(df_pick['symbol'].tolist())}")
            except Exception as e:
                logger.warning(f"   è¯»å–æ¨èæ–‡ä»¶å¤±è´¥: {e}")
    
    logger.info("\n" + "="*60)
    logger.info("ğŸ‰ğŸ‰ğŸ‰ æ¯æ—¥å…¨æµç¨‹ä»»åŠ¡é¡ºåˆ©å®Œæˆï¼è¯·æŸ¥çœ‹ reports ç›®å½•ä¸‹çš„æ¨èç»“æœã€‚")
    logger.info("="*60)

if __name__ == "__main__":
    main()
